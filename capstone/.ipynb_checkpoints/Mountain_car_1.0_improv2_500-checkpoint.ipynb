{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Improvement by decreasing gamma - 500 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.2)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 14:13:51,201] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.2\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 162 steps, reward Average: -197.28571428571428, up to now: minReward: -162.0, minAverage: -197.28571428571428\n",
      "Episode 14 done after 157 steps, reward Average: -194.6, up to now: minReward: -157.0, minAverage: -194.6\n",
      "Episode 15 done after 200 steps, reward Average: -194.9375, up to now: minReward: -157.0, minAverage: -194.6\n",
      "Episode 16 done after 147 steps, reward Average: -192.11764705882354, up to now: minReward: -147.0, minAverage: -192.11764705882354\n",
      "Episode 17 done after 154 steps, reward Average: -190.0, up to now: minReward: -147.0, minAverage: -190.0\n",
      "Episode 18 done after 162 steps, reward Average: -188.52631578947367, up to now: minReward: -147.0, minAverage: -188.52631578947367\n",
      "Episode 19 done after 149 steps, reward Average: -186.55, up to now: minReward: -147.0, minAverage: -186.55\n",
      "Episode 20 done after 147 steps, reward Average: -184.66666666666666, up to now: minReward: -147.0, minAverage: -184.66666666666666\n",
      "Episode 21 done after 148 steps, reward Average: -183.0, up to now: minReward: -147.0, minAverage: -183.0\n",
      "Episode 22 done after 146 steps, reward Average: -181.3913043478261, up to now: minReward: -146.0, minAverage: -181.3913043478261\n",
      "Episode 23 done after 141 steps, reward Average: -179.70833333333334, up to now: minReward: -141.0, minAverage: -179.70833333333334\n",
      "Episode 24 done after 139 steps, reward Average: -178.08, up to now: minReward: -139.0, minAverage: -178.08\n",
      "Episode 25 done after 142 steps, reward Average: -176.69230769230768, up to now: minReward: -139.0, minAverage: -176.69230769230768\n",
      "Episode 26 done after 139 steps, reward Average: -175.2962962962963, up to now: minReward: -139.0, minAverage: -175.2962962962963\n",
      "Episode 27 done after 138 steps, reward Average: -173.96428571428572, up to now: minReward: -138.0, minAverage: -173.96428571428572\n",
      "Episode 28 done after 137 steps, reward Average: -172.68965517241378, up to now: minReward: -137.0, minAverage: -172.68965517241378\n",
      "Episode 29 done after 138 steps, reward Average: -171.53333333333333, up to now: minReward: -137.0, minAverage: -171.53333333333333\n",
      "Episode 30 done after 135 steps, reward Average: -170.3548387096774, up to now: minReward: -135.0, minAverage: -170.3548387096774\n",
      "Episode 31 done after 137 steps, reward Average: -169.3125, up to now: minReward: -135.0, minAverage: -169.3125\n",
      "Episode 32 done after 135 steps, reward Average: -168.27272727272728, up to now: minReward: -135.0, minAverage: -168.27272727272728\n",
      "Episode 33 done after 138 steps, reward Average: -167.38235294117646, up to now: minReward: -135.0, minAverage: -167.38235294117646\n",
      "Episode 34 done after 137 steps, reward Average: -166.5142857142857, up to now: minReward: -135.0, minAverage: -166.5142857142857\n",
      "Episode 35 done after 133 steps, reward Average: -165.58333333333334, up to now: minReward: -133.0, minAverage: -165.58333333333334\n",
      "Episode 36 done after 132 steps, reward Average: -164.67567567567568, up to now: minReward: -132.0, minAverage: -164.67567567567568\n",
      "Episode 37 done after 137 steps, reward Average: -163.94736842105263, up to now: minReward: -132.0, minAverage: -163.94736842105263\n",
      "Episode 38 done after 121 steps, reward Average: -162.84615384615384, up to now: minReward: -121.0, minAverage: -162.84615384615384\n",
      "Episode 39 done after 117 steps, reward Average: -161.7, up to now: minReward: -117.0, minAverage: -161.7\n",
      "Episode 40 done after 113 steps, reward Average: -160.5121951219512, up to now: minReward: -113.0, minAverage: -160.5121951219512\n",
      "Episode 41 done after 137 steps, reward Average: -159.95238095238096, up to now: minReward: -113.0, minAverage: -159.95238095238096\n",
      "Episode 42 done after 135 steps, reward Average: -159.37209302325581, up to now: minReward: -113.0, minAverage: -159.37209302325581\n",
      "Episode 43 done after 114 steps, reward Average: -158.3409090909091, up to now: minReward: -113.0, minAverage: -158.3409090909091\n",
      "Episode 44 done after 133 steps, reward Average: -157.77777777777777, up to now: minReward: -113.0, minAverage: -157.77777777777777\n",
      "Episode 45 done after 116 steps, reward Average: -156.8695652173913, up to now: minReward: -113.0, minAverage: -156.8695652173913\n",
      "Episode 46 done after 133 steps, reward Average: -156.36170212765958, up to now: minReward: -113.0, minAverage: -156.36170212765958\n",
      "Episode 47 done after 123 steps, reward Average: -155.66666666666666, up to now: minReward: -113.0, minAverage: -155.66666666666666\n",
      "Episode 48 done after 137 steps, reward Average: -155.28571428571428, up to now: minReward: -113.0, minAverage: -155.28571428571428\n",
      "Episode 49 done after 122 steps, reward Average: -154.62, up to now: minReward: -113.0, minAverage: -154.62\n",
      "Episode 50 done after 134 steps, reward Average: -154.2156862745098, up to now: minReward: -113.0, minAverage: -154.2156862745098\n",
      "Episode 51 done after 136 steps, reward Average: -153.8653846153846, up to now: minReward: -113.0, minAverage: -153.8653846153846\n",
      "Episode 52 done after 135 steps, reward Average: -153.50943396226415, up to now: minReward: -113.0, minAverage: -153.50943396226415\n",
      "Episode 53 done after 135 steps, reward Average: -153.16666666666666, up to now: minReward: -113.0, minAverage: -153.16666666666666\n",
      "Episode 54 done after 136 steps, reward Average: -152.85454545454544, up to now: minReward: -113.0, minAverage: -152.85454545454544\n",
      "Episode 55 done after 135 steps, reward Average: -152.53571428571428, up to now: minReward: -113.0, minAverage: -152.53571428571428\n",
      "Episode 56 done after 122 steps, reward Average: -152.0, up to now: minReward: -113.0, minAverage: -152.0\n",
      "Episode 57 done after 122 steps, reward Average: -151.48275862068965, up to now: minReward: -113.0, minAverage: -151.48275862068965\n",
      "Episode 58 done after 113 steps, reward Average: -150.83050847457628, up to now: minReward: -113.0, minAverage: -150.83050847457628\n",
      "Episode 59 done after 134 steps, reward Average: -150.55, up to now: minReward: -113.0, minAverage: -150.55\n",
      "Episode 60 done after 135 steps, reward Average: -150.29508196721312, up to now: minReward: -113.0, minAverage: -150.29508196721312\n",
      "Episode 61 done after 122 steps, reward Average: -149.83870967741936, up to now: minReward: -113.0, minAverage: -149.83870967741936\n",
      "Episode 62 done after 118 steps, reward Average: -149.33333333333334, up to now: minReward: -113.0, minAverage: -149.33333333333334\n",
      "Episode 63 done after 137 steps, reward Average: -149.140625, up to now: minReward: -113.0, minAverage: -149.140625\n",
      "Episode 64 done after 134 steps, reward Average: -148.90769230769232, up to now: minReward: -113.0, minAverage: -148.90769230769232\n",
      "Episode 65 done after 114 steps, reward Average: -148.37878787878788, up to now: minReward: -113.0, minAverage: -148.37878787878788\n",
      "Episode 66 done after 114 steps, reward Average: -147.86567164179104, up to now: minReward: -113.0, minAverage: -147.86567164179104\n",
      "Episode 67 done after 135 steps, reward Average: -147.6764705882353, up to now: minReward: -113.0, minAverage: -147.6764705882353\n",
      "Episode 68 done after 122 steps, reward Average: -147.30434782608697, up to now: minReward: -113.0, minAverage: -147.30434782608697\n",
      "Episode 69 done after 114 steps, reward Average: -146.82857142857142, up to now: minReward: -113.0, minAverage: -146.82857142857142\n",
      "Episode 70 done after 117 steps, reward Average: -146.40845070422534, up to now: minReward: -113.0, minAverage: -146.40845070422534\n",
      "Episode 71 done after 119 steps, reward Average: -146.02777777777777, up to now: minReward: -113.0, minAverage: -146.02777777777777\n",
      "Episode 72 done after 135 steps, reward Average: -145.87671232876713, up to now: minReward: -113.0, minAverage: -145.87671232876713\n",
      "Episode 73 done after 133 steps, reward Average: -145.7027027027027, up to now: minReward: -113.0, minAverage: -145.7027027027027\n",
      "Episode 74 done after 134 steps, reward Average: -145.54666666666665, up to now: minReward: -113.0, minAverage: -145.54666666666665\n",
      "Episode 75 done after 138 steps, reward Average: -145.44736842105263, up to now: minReward: -113.0, minAverage: -145.44736842105263\n",
      "Episode 76 done after 114 steps, reward Average: -145.03896103896105, up to now: minReward: -113.0, minAverage: -145.03896103896105\n",
      "Episode 77 done after 114 steps, reward Average: -144.64102564102564, up to now: minReward: -113.0, minAverage: -144.64102564102564\n",
      "Episode 78 done after 139 steps, reward Average: -144.56962025316454, up to now: minReward: -113.0, minAverage: -144.56962025316454\n",
      "Episode 79 done after 115 steps, reward Average: -144.2, up to now: minReward: -113.0, minAverage: -144.2\n",
      "Episode 80 done after 115 steps, reward Average: -143.8395061728395, up to now: minReward: -113.0, minAverage: -143.8395061728395\n",
      "Episode 81 done after 135 steps, reward Average: -143.73170731707316, up to now: minReward: -113.0, minAverage: -143.73170731707316\n",
      "Episode 82 done after 114 steps, reward Average: -143.3734939759036, up to now: minReward: -113.0, minAverage: -143.3734939759036\n",
      "Episode 83 done after 116 steps, reward Average: -143.04761904761904, up to now: minReward: -113.0, minAverage: -143.04761904761904\n",
      "Episode 84 done after 138 steps, reward Average: -142.98823529411766, up to now: minReward: -113.0, minAverage: -142.98823529411766\n",
      "Episode 85 done after 138 steps, reward Average: -142.93023255813952, up to now: minReward: -113.0, minAverage: -142.93023255813952\n",
      "Episode 86 done after 139 steps, reward Average: -142.88505747126436, up to now: minReward: -113.0, minAverage: -142.88505747126436\n",
      "Episode 87 done after 138 steps, reward Average: -142.82954545454547, up to now: minReward: -113.0, minAverage: -142.82954545454547\n",
      "Episode 88 done after 139 steps, reward Average: -142.7865168539326, up to now: minReward: -113.0, minAverage: -142.7865168539326\n",
      "Episode 89 done after 136 steps, reward Average: -142.7111111111111, up to now: minReward: -113.0, minAverage: -142.7111111111111\n",
      "Episode 90 done after 116 steps, reward Average: -142.41758241758242, up to now: minReward: -113.0, minAverage: -142.41758241758242\n",
      "Episode 91 done after 117 steps, reward Average: -142.1413043478261, up to now: minReward: -113.0, minAverage: -142.1413043478261\n",
      "Episode 92 done after 135 steps, reward Average: -142.06451612903226, up to now: minReward: -113.0, minAverage: -142.06451612903226\n",
      "Episode 93 done after 115 steps, reward Average: -141.77659574468086, up to now: minReward: -113.0, minAverage: -141.77659574468086\n",
      "Episode 94 done after 116 steps, reward Average: -141.50526315789475, up to now: minReward: -113.0, minAverage: -141.50526315789475\n",
      "Episode 95 done after 122 steps, reward Average: -141.30208333333334, up to now: minReward: -113.0, minAverage: -141.30208333333334\n",
      "Episode 96 done after 121 steps, reward Average: -141.09278350515464, up to now: minReward: -113.0, minAverage: -141.09278350515464\n",
      "Episode 97 done after 137 steps, reward Average: -141.05102040816325, up to now: minReward: -113.0, minAverage: -141.05102040816325\n",
      "Episode 98 done after 137 steps, reward Average: -141.010101010101, up to now: minReward: -113.0, minAverage: -141.010101010101\n",
      "Episode 99 done after 114 steps, reward Average: -140.74, up to now: minReward: -113.0, minAverage: -140.74\n",
      "Episode 100 done after 135 steps, reward Average: -140.09, up to now: minReward: -113.0, minAverage: -140.09\n",
      "Episode 101 done after 114 steps, reward Average: -139.23, up to now: minReward: -113.0, minAverage: -139.23\n",
      "Episode 102 done after 114 steps, reward Average: -138.37, up to now: minReward: -113.0, minAverage: -138.37\n",
      "Episode 103 done after 136 steps, reward Average: -137.73, up to now: minReward: -113.0, minAverage: -137.73\n",
      "Episode 104 done after 114 steps, reward Average: -136.87, up to now: minReward: -113.0, minAverage: -136.87\n",
      "Episode 105 done after 139 steps, reward Average: -136.26, up to now: minReward: -113.0, minAverage: -136.26\n",
      "Episode 106 done after 114 steps, reward Average: -135.4, up to now: minReward: -113.0, minAverage: -135.4\n",
      "Episode 107 done after 133 steps, reward Average: -134.73, up to now: minReward: -113.0, minAverage: -134.73\n",
      "Episode 108 done after 115 steps, reward Average: -133.88, up to now: minReward: -113.0, minAverage: -133.88\n",
      "Episode 109 done after 115 steps, reward Average: -133.03, up to now: minReward: -113.0, minAverage: -133.03\n",
      "Episode 110 done after 138 steps, reward Average: -132.41, up to now: minReward: -113.0, minAverage: -132.41\n",
      "Episode 111 done after 114 steps, reward Average: -131.55, up to now: minReward: -113.0, minAverage: -131.55\n",
      "Episode 112 done after 114 steps, reward Average: -130.69, up to now: minReward: -113.0, minAverage: -130.69\n",
      "Episode 113 done after 135 steps, reward Average: -130.42, up to now: minReward: -113.0, minAverage: -130.42\n",
      "Episode 114 done after 114 steps, reward Average: -129.99, up to now: minReward: -113.0, minAverage: -129.99\n",
      "Episode 115 done after 138 steps, reward Average: -129.37, up to now: minReward: -113.0, minAverage: -129.37\n",
      "Episode 116 done after 138 steps, reward Average: -129.28, up to now: minReward: -113.0, minAverage: -129.28\n",
      "Episode 117 done after 135 steps, reward Average: -129.09, up to now: minReward: -113.0, minAverage: -129.09\n",
      "Episode 118 done after 135 steps, reward Average: -128.82, up to now: minReward: -113.0, minAverage: -128.82\n",
      "Episode 119 done after 138 steps, reward Average: -128.71, up to now: minReward: -113.0, minAverage: -128.71\n",
      "Episode 120 done after 117 steps, reward Average: -128.41, up to now: minReward: -113.0, minAverage: -128.41\n",
      "Episode 121 done after 138 steps, reward Average: -128.31, up to now: minReward: -113.0, minAverage: -128.31\n",
      "Episode 122 done after 135 steps, reward Average: -128.2, up to now: minReward: -113.0, minAverage: -128.2\n",
      "Episode 123 done after 137 steps, reward Average: -128.16, up to now: minReward: -113.0, minAverage: -128.16\n",
      "Episode 124 done after 134 steps, reward Average: -128.11, up to now: minReward: -113.0, minAverage: -128.11\n",
      "Episode 125 done after 138 steps, reward Average: -128.07, up to now: minReward: -113.0, minAverage: -128.07\n",
      "Episode 126 done after 115 steps, reward Average: -127.83, up to now: minReward: -113.0, minAverage: -127.83\n",
      "Episode 127 done after 121 steps, reward Average: -127.66, up to now: minReward: -113.0, minAverage: -127.66\n",
      "Episode 128 done after 113 steps, reward Average: -127.42, up to now: minReward: -113.0, minAverage: -127.42\n",
      "Episode 129 done after 137 steps, reward Average: -127.41, up to now: minReward: -113.0, minAverage: -127.41\n",
      "Episode 130 done after 137 steps, reward Average: -127.43, up to now: minReward: -113.0, minAverage: -127.41\n",
      "Episode 131 done after 115 steps, reward Average: -127.21, up to now: minReward: -113.0, minAverage: -127.21\n",
      "Episode 132 done after 135 steps, reward Average: -127.21, up to now: minReward: -113.0, minAverage: -127.21\n",
      "Episode 133 done after 114 steps, reward Average: -126.97, up to now: minReward: -113.0, minAverage: -126.97\n",
      "Episode 134 done after 114 steps, reward Average: -126.74, up to now: minReward: -113.0, minAverage: -126.74\n",
      "Episode 135 done after 134 steps, reward Average: -126.75, up to now: minReward: -113.0, minAverage: -126.74\n",
      "Episode 136 done after 135 steps, reward Average: -126.78, up to now: minReward: -113.0, minAverage: -126.74\n",
      "Episode 137 done after 118 steps, reward Average: -126.59, up to now: minReward: -113.0, minAverage: -126.59\n",
      "Episode 138 done after 114 steps, reward Average: -126.52, up to now: minReward: -113.0, minAverage: -126.52\n",
      "Episode 139 done after 138 steps, reward Average: -126.73, up to now: minReward: -113.0, minAverage: -126.52\n",
      "Episode 140 done after 114 steps, reward Average: -126.74, up to now: minReward: -113.0, minAverage: -126.52\n",
      "Episode 141 done after 121 steps, reward Average: -126.58, up to now: minReward: -113.0, minAverage: -126.52\n",
      "Episode 142 done after 135 steps, reward Average: -126.58, up to now: minReward: -113.0, minAverage: -126.52\n",
      "Episode 143 done after 114 steps, reward Average: -126.58, up to now: minReward: -113.0, minAverage: -126.52\n",
      "Episode 144 done after 114 steps, reward Average: -126.39, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 145 done after 137 steps, reward Average: -126.6, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 146 done after 137 steps, reward Average: -126.64, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 147 done after 135 steps, reward Average: -126.76, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 148 done after 138 steps, reward Average: -126.77, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 149 done after 115 steps, reward Average: -126.7, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 150 done after 138 steps, reward Average: -126.74, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 151 done after 135 steps, reward Average: -126.73, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 152 done after 119 steps, reward Average: -126.57, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 153 done after 135 steps, reward Average: -126.57, up to now: minReward: -113.0, minAverage: -126.39\n",
      "Episode 154 done after 113 steps, reward Average: -126.34, up to now: minReward: -113.0, minAverage: -126.34\n",
      "Episode 155 done after 114 steps, reward Average: -126.13, up to now: minReward: -113.0, minAverage: -126.13\n",
      "Episode 156 done after 114 steps, reward Average: -126.05, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 157 done after 137 steps, reward Average: -126.2, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 158 done after 138 steps, reward Average: -126.45, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 159 done after 137 steps, reward Average: -126.48, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 160 done after 135 steps, reward Average: -126.48, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 161 done after 138 steps, reward Average: -126.64, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 162 done after 135 steps, reward Average: -126.81, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 163 done after 114 steps, reward Average: -126.58, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 164 done after 116 steps, reward Average: -126.4, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 165 done after 116 steps, reward Average: -126.42, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 166 done after 116 steps, reward Average: -126.44, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 167 done after 134 steps, reward Average: -126.43, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 168 done after 138 steps, reward Average: -126.59, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 169 done after 135 steps, reward Average: -126.8, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 170 done after 134 steps, reward Average: -126.97, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 171 done after 121 steps, reward Average: -126.99, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 172 done after 117 steps, reward Average: -126.81, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 173 done after 137 steps, reward Average: -126.85, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 174 done after 114 steps, reward Average: -126.65, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 175 done after 122 steps, reward Average: -126.49, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 176 done after 116 steps, reward Average: -126.51, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 177 done after 123 steps, reward Average: -126.6, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 178 done after 134 steps, reward Average: -126.55, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 179 done after 113 steps, reward Average: -126.53, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 180 done after 117 steps, reward Average: -126.55, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 181 done after 115 steps, reward Average: -126.35, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 182 done after 134 steps, reward Average: -126.55, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 183 done after 134 steps, reward Average: -126.73, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 184 done after 115 steps, reward Average: -126.5, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 185 done after 138 steps, reward Average: -126.5, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 186 done after 135 steps, reward Average: -126.46, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 187 done after 116 steps, reward Average: -126.24, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 188 done after 135 steps, reward Average: -126.2, up to now: minReward: -113.0, minAverage: -126.05\n",
      "Episode 189 done after 115 steps, reward Average: -125.99, up to now: minReward: -113.0, minAverage: -125.99\n",
      "Episode 190 done after 114 steps, reward Average: -125.97, up to now: minReward: -113.0, minAverage: -125.97\n",
      "Episode 191 done after 137 steps, reward Average: -126.17, up to now: minReward: -113.0, minAverage: -125.97\n",
      "Episode 192 done after 134 steps, reward Average: -126.16, up to now: minReward: -113.0, minAverage: -125.97\n",
      "Episode 193 done after 116 steps, reward Average: -126.17, up to now: minReward: -113.0, minAverage: -125.97\n",
      "Episode 194 done after 115 steps, reward Average: -126.16, up to now: minReward: -113.0, minAverage: -125.97\n",
      "Episode 195 done after 121 steps, reward Average: -126.15, up to now: minReward: -113.0, minAverage: -125.97\n",
      "Episode 196 done after 114 steps, reward Average: -126.08, up to now: minReward: -113.0, minAverage: -125.97\n",
      "Episode 197 done after 138 steps, reward Average: -126.09, up to now: minReward: -113.0, minAverage: -125.97\n",
      "Episode 198 done after 122 steps, reward Average: -125.94, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 199 done after 135 steps, reward Average: -126.15, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 200 done after 114 steps, reward Average: -125.94, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 201 done after 114 steps, reward Average: -125.94, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 202 done after 135 steps, reward Average: -126.15, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 203 done after 135 steps, reward Average: -126.14, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 204 done after 135 steps, reward Average: -126.35, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 205 done after 135 steps, reward Average: -126.31, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 206 done after 117 steps, reward Average: -126.34, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 207 done after 135 steps, reward Average: -126.36, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 208 done after 114 steps, reward Average: -126.35, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 209 done after 120 steps, reward Average: -126.4, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 210 done after 137 steps, reward Average: -126.39, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 211 done after 114 steps, reward Average: -126.39, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 212 done after 114 steps, reward Average: -126.39, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 213 done after 135 steps, reward Average: -126.39, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 214 done after 117 steps, reward Average: -126.42, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 215 done after 137 steps, reward Average: -126.41, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 216 done after 117 steps, reward Average: -126.2, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 217 done after 118 steps, reward Average: -126.03, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 218 done after 135 steps, reward Average: -126.03, up to now: minReward: -113.0, minAverage: -125.94\n",
      "Episode 219 done after 119 steps, reward Average: -125.84, up to now: minReward: -113.0, minAverage: -125.84\n",
      "Episode 220 done after 116 steps, reward Average: -125.83, up to now: minReward: -113.0, minAverage: -125.83\n",
      "Episode 221 done after 114 steps, reward Average: -125.59, up to now: minReward: -113.0, minAverage: -125.59\n",
      "Episode 222 done after 137 steps, reward Average: -125.61, up to now: minReward: -113.0, minAverage: -125.59\n",
      "Episode 223 done after 114 steps, reward Average: -125.38, up to now: minReward: -113.0, minAverage: -125.38\n",
      "Episode 224 done after 135 steps, reward Average: -125.39, up to now: minReward: -113.0, minAverage: -125.38\n",
      "Episode 225 done after 119 steps, reward Average: -125.2, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 226 done after 138 steps, reward Average: -125.43, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 227 done after 134 steps, reward Average: -125.56, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 228 done after 137 steps, reward Average: -125.8, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 229 done after 135 steps, reward Average: -125.78, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 230 done after 116 steps, reward Average: -125.57, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 231 done after 113 steps, reward Average: -125.55, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 232 done after 137 steps, reward Average: -125.57, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 233 done after 116 steps, reward Average: -125.59, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 234 done after 114 steps, reward Average: -125.59, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 235 done after 119 steps, reward Average: -125.44, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 236 done after 118 steps, reward Average: -125.27, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 237 done after 119 steps, reward Average: -125.28, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 238 done after 139 steps, reward Average: -125.53, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 239 done after 115 steps, reward Average: -125.3, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 240 done after 119 steps, reward Average: -125.35, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 241 done after 138 steps, reward Average: -125.52, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 242 done after 114 steps, reward Average: -125.31, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 243 done after 138 steps, reward Average: -125.55, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 244 done after 115 steps, reward Average: -125.56, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 245 done after 120 steps, reward Average: -125.39, up to now: minReward: -113.0, minAverage: -125.2\n",
      "Episode 246 done after 114 steps, reward Average: -125.16, up to now: minReward: -113.0, minAverage: -125.16\n",
      "Episode 247 done after 114 steps, reward Average: -124.95, up to now: minReward: -113.0, minAverage: -124.95\n",
      "Episode 248 done after 135 steps, reward Average: -124.92, up to now: minReward: -113.0, minAverage: -124.92\n",
      "Episode 249 done after 137 steps, reward Average: -125.14, up to now: minReward: -113.0, minAverage: -124.92\n",
      "Episode 250 done after 114 steps, reward Average: -124.9, up to now: minReward: -113.0, minAverage: -124.9\n",
      "Episode 251 done after 118 steps, reward Average: -124.73, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 252 done after 135 steps, reward Average: -124.89, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 253 done after 134 steps, reward Average: -124.88, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 254 done after 138 steps, reward Average: -125.13, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 255 done after 115 steps, reward Average: -125.14, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 256 done after 114 steps, reward Average: -125.14, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 257 done after 121 steps, reward Average: -124.98, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 258 done after 117 steps, reward Average: -124.77, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 259 done after 138 steps, reward Average: -124.78, up to now: minReward: -113.0, minAverage: -124.73\n",
      "Episode 260 done after 119 steps, reward Average: -124.62, up to now: minReward: -113.0, minAverage: -124.62\n",
      "Episode 261 done after 138 steps, reward Average: -124.62, up to now: minReward: -113.0, minAverage: -124.62\n",
      "Episode 262 done after 114 steps, reward Average: -124.41, up to now: minReward: -113.0, minAverage: -124.41\n",
      "Episode 263 done after 113 steps, reward Average: -124.4, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 264 done after 137 steps, reward Average: -124.61, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 265 done after 135 steps, reward Average: -124.8, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 266 done after 135 steps, reward Average: -124.99, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 267 done after 114 steps, reward Average: -124.79, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 268 done after 122 steps, reward Average: -124.63, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 269 done after 138 steps, reward Average: -124.66, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 270 done after 138 steps, reward Average: -124.7, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 271 done after 134 steps, reward Average: -124.83, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 272 done after 137 steps, reward Average: -125.03, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 273 done after 114 steps, reward Average: -124.8, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 274 done after 137 steps, reward Average: -125.03, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 275 done after 114 steps, reward Average: -124.95, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 276 done after 135 steps, reward Average: -125.14, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 277 done after 135 steps, reward Average: -125.26, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 278 done after 113 steps, reward Average: -125.05, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 279 done after 135 steps, reward Average: -125.27, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 280 done after 113 steps, reward Average: -125.23, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 281 done after 139 steps, reward Average: -125.47, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 282 done after 137 steps, reward Average: -125.5, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 283 done after 113 steps, reward Average: -125.29, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 284 done after 113 steps, reward Average: -125.27, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 285 done after 134 steps, reward Average: -125.23, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 286 done after 135 steps, reward Average: -125.23, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 287 done after 135 steps, reward Average: -125.42, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 288 done after 116 steps, reward Average: -125.23, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 289 done after 134 steps, reward Average: -125.42, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 290 done after 122 steps, reward Average: -125.5, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 291 done after 122 steps, reward Average: -125.35, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 292 done after 114 steps, reward Average: -125.15, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 293 done after 113 steps, reward Average: -125.12, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 294 done after 135 steps, reward Average: -125.32, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 295 done after 135 steps, reward Average: -125.46, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 296 done after 117 steps, reward Average: -125.49, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 297 done after 137 steps, reward Average: -125.48, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 298 done after 114 steps, reward Average: -125.4, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 299 done after 138 steps, reward Average: -125.43, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 300 done after 137 steps, reward Average: -125.66, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 301 done after 137 steps, reward Average: -125.89, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 302 done after 114 steps, reward Average: -125.68, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 303 done after 137 steps, reward Average: -125.7, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 304 done after 134 steps, reward Average: -125.69, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 305 done after 115 steps, reward Average: -125.49, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 306 done after 114 steps, reward Average: -125.46, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 307 done after 137 steps, reward Average: -125.48, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 308 done after 113 steps, reward Average: -125.47, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 309 done after 134 steps, reward Average: -125.61, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 310 done after 136 steps, reward Average: -125.6, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 311 done after 114 steps, reward Average: -125.6, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 312 done after 114 steps, reward Average: -125.6, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 313 done after 137 steps, reward Average: -125.62, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 314 done after 133 steps, reward Average: -125.78, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 315 done after 123 steps, reward Average: -125.64, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 316 done after 114 steps, reward Average: -125.61, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 317 done after 133 steps, reward Average: -125.76, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 318 done after 116 steps, reward Average: -125.57, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 319 done after 114 steps, reward Average: -125.52, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 320 done after 138 steps, reward Average: -125.74, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 321 done after 114 steps, reward Average: -125.74, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 322 done after 114 steps, reward Average: -125.51, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 323 done after 139 steps, reward Average: -125.76, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 324 done after 138 steps, reward Average: -125.79, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 325 done after 115 steps, reward Average: -125.75, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 326 done after 114 steps, reward Average: -125.51, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 327 done after 135 steps, reward Average: -125.52, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 328 done after 137 steps, reward Average: -125.52, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 329 done after 133 steps, reward Average: -125.5, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 330 done after 115 steps, reward Average: -125.49, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 331 done after 133 steps, reward Average: -125.69, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 332 done after 134 steps, reward Average: -125.66, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 333 done after 134 steps, reward Average: -125.84, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 334 done after 117 steps, reward Average: -125.87, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 335 done after 135 steps, reward Average: -126.03, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 336 done after 117 steps, reward Average: -126.02, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 337 done after 137 steps, reward Average: -126.2, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 338 done after 137 steps, reward Average: -126.18, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 339 done after 113 steps, reward Average: -126.16, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 340 done after 135 steps, reward Average: -126.32, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 341 done after 138 steps, reward Average: -126.32, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 342 done after 135 steps, reward Average: -126.53, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 343 done after 134 steps, reward Average: -126.49, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 344 done after 135 steps, reward Average: -126.69, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 345 done after 135 steps, reward Average: -126.84, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 346 done after 136 steps, reward Average: -127.06, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 347 done after 133 steps, reward Average: -127.25, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 348 done after 113 steps, reward Average: -127.03, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 349 done after 137 steps, reward Average: -127.03, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 350 done after 135 steps, reward Average: -127.24, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 351 done after 115 steps, reward Average: -127.21, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 352 done after 138 steps, reward Average: -127.24, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 353 done after 118 steps, reward Average: -127.08, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 354 done after 114 steps, reward Average: -126.84, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 355 done after 137 steps, reward Average: -127.06, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 356 done after 135 steps, reward Average: -127.27, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 357 done after 138 steps, reward Average: -127.44, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 358 done after 138 steps, reward Average: -127.65, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 359 done after 138 steps, reward Average: -127.65, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 360 done after 135 steps, reward Average: -127.81, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 361 done after 115 steps, reward Average: -127.58, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 362 done after 115 steps, reward Average: -127.59, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 363 done after 138 steps, reward Average: -127.84, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 364 done after 114 steps, reward Average: -127.61, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 365 done after 114 steps, reward Average: -127.4, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 366 done after 114 steps, reward Average: -127.19, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 367 done after 117 steps, reward Average: -127.22, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 368 done after 133 steps, reward Average: -127.33, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 369 done after 138 steps, reward Average: -127.33, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 370 done after 117 steps, reward Average: -127.12, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 371 done after 135 steps, reward Average: -127.13, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 372 done after 122 steps, reward Average: -126.98, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 373 done after 116 steps, reward Average: -127.0, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 374 done after 114 steps, reward Average: -126.77, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 375 done after 134 steps, reward Average: -126.97, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 376 done after 135 steps, reward Average: -126.97, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 377 done after 137 steps, reward Average: -126.99, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 378 done after 114 steps, reward Average: -127.0, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 379 done after 122 steps, reward Average: -126.87, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 380 done after 116 steps, reward Average: -126.9, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 381 done after 120 steps, reward Average: -126.71, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 382 done after 137 steps, reward Average: -126.71, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 383 done after 138 steps, reward Average: -126.96, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 384 done after 116 steps, reward Average: -126.99, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 385 done after 114 steps, reward Average: -126.79, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 386 done after 137 steps, reward Average: -126.81, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 387 done after 137 steps, reward Average: -126.83, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 388 done after 117 steps, reward Average: -126.84, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 389 done after 137 steps, reward Average: -126.87, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 390 done after 115 steps, reward Average: -126.8, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 391 done after 138 steps, reward Average: -126.96, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 392 done after 114 steps, reward Average: -126.96, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 393 done after 114 steps, reward Average: -126.97, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 394 done after 113 steps, reward Average: -126.75, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 395 done after 137 steps, reward Average: -126.77, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 396 done after 113 steps, reward Average: -126.73, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 397 done after 117 steps, reward Average: -126.53, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 398 done after 116 steps, reward Average: -126.55, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 399 done after 138 steps, reward Average: -126.55, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 400 done after 118 steps, reward Average: -126.36, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 401 done after 114 steps, reward Average: -126.13, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 402 done after 137 steps, reward Average: -126.36, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 403 done after 115 steps, reward Average: -126.14, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 404 done after 116 steps, reward Average: -125.96, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 405 done after 138 steps, reward Average: -126.19, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 406 done after 114 steps, reward Average: -126.19, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 407 done after 122 steps, reward Average: -126.04, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 408 done after 119 steps, reward Average: -126.1, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 409 done after 118 steps, reward Average: -125.94, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 410 done after 114 steps, reward Average: -125.72, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 411 done after 137 steps, reward Average: -125.95, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 412 done after 115 steps, reward Average: -125.96, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 413 done after 145 steps, reward Average: -126.04, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 414 done after 133 steps, reward Average: -126.04, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 415 done after 113 steps, reward Average: -125.94, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 416 done after 133 steps, reward Average: -126.13, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 417 done after 117 steps, reward Average: -125.97, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 418 done after 117 steps, reward Average: -125.98, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 419 done after 135 steps, reward Average: -126.19, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 420 done after 135 steps, reward Average: -126.16, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 421 done after 138 steps, reward Average: -126.4, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 422 done after 119 steps, reward Average: -126.45, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 423 done after 121 steps, reward Average: -126.27, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 424 done after 134 steps, reward Average: -126.23, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 425 done after 134 steps, reward Average: -126.42, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 426 done after 133 steps, reward Average: -126.61, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 427 done after 138 steps, reward Average: -126.64, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 428 done after 134 steps, reward Average: -126.61, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 429 done after 135 steps, reward Average: -126.63, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 430 done after 114 steps, reward Average: -126.62, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 431 done after 134 steps, reward Average: -126.63, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 432 done after 138 steps, reward Average: -126.67, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 433 done after 139 steps, reward Average: -126.72, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 434 done after 122 steps, reward Average: -126.77, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 435 done after 135 steps, reward Average: -126.77, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 436 done after 135 steps, reward Average: -126.95, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 437 done after 114 steps, reward Average: -126.72, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 438 done after 133 steps, reward Average: -126.68, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 439 done after 135 steps, reward Average: -126.9, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 440 done after 135 steps, reward Average: -126.9, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 441 done after 135 steps, reward Average: -126.87, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 442 done after 138 steps, reward Average: -126.9, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 443 done after 137 steps, reward Average: -126.93, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 444 done after 135 steps, reward Average: -126.93, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 445 done after 115 steps, reward Average: -126.73, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 446 done after 137 steps, reward Average: -126.74, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 447 done after 114 steps, reward Average: -126.55, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 448 done after 137 steps, reward Average: -126.79, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 449 done after 135 steps, reward Average: -126.77, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 450 done after 139 steps, reward Average: -126.81, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 451 done after 135 steps, reward Average: -127.01, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 452 done after 114 steps, reward Average: -126.77, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 453 done after 135 steps, reward Average: -126.94, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 454 done after 135 steps, reward Average: -127.15, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 455 done after 134 steps, reward Average: -127.12, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 456 done after 113 steps, reward Average: -126.9, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 457 done after 115 steps, reward Average: -126.67, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 458 done after 134 steps, reward Average: -126.63, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 459 done after 117 steps, reward Average: -126.42, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 460 done after 133 steps, reward Average: -126.4, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 461 done after 113 steps, reward Average: -126.38, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 462 done after 115 steps, reward Average: -126.38, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 463 done after 139 steps, reward Average: -126.39, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 464 done after 136 steps, reward Average: -126.61, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 465 done after 136 steps, reward Average: -126.83, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 466 done after 114 steps, reward Average: -126.83, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 467 done after 138 steps, reward Average: -127.04, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 468 done after 114 steps, reward Average: -126.85, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 469 done after 122 steps, reward Average: -126.69, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 470 done after 138 steps, reward Average: -126.9, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 471 done after 115 steps, reward Average: -126.7, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 472 done after 138 steps, reward Average: -126.86, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 473 done after 139 steps, reward Average: -127.09, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 474 done after 136 steps, reward Average: -127.31, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 475 done after 117 steps, reward Average: -127.14, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 476 done after 139 steps, reward Average: -127.18, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 477 done after 137 steps, reward Average: -127.18, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 478 done after 114 steps, reward Average: -127.18, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 479 done after 117 steps, reward Average: -127.13, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 480 done after 113 steps, reward Average: -127.1, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 481 done after 115 steps, reward Average: -127.05, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 482 done after 114 steps, reward Average: -126.82, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 483 done after 114 steps, reward Average: -126.58, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 484 done after 114 steps, reward Average: -126.56, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 485 done after 135 steps, reward Average: -126.77, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 486 done after 117 steps, reward Average: -126.57, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 487 done after 115 steps, reward Average: -126.35, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 488 done after 133 steps, reward Average: -126.51, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 489 done after 133 steps, reward Average: -126.47, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 490 done after 138 steps, reward Average: -126.7, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 491 done after 115 steps, reward Average: -126.47, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 492 done after 113 steps, reward Average: -126.46, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 493 done after 114 steps, reward Average: -126.46, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 494 done after 138 steps, reward Average: -126.71, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 495 done after 137 steps, reward Average: -126.71, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 496 done after 139 steps, reward Average: -126.97, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 497 done after 114 steps, reward Average: -126.94, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 498 done after 135 steps, reward Average: -127.13, up to now: minReward: -113.0, minAverage: -124.4\n",
      "Episode 499 done after 135 steps, reward Average: -127.1, up to now: minReward: -113.0, minAverage: -124.4\n",
      "final result: \n",
      "486 times arrived in 500 episodes, first time in episode 13\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFkNoRTwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23636b13128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) +  (R +  Q(S, A)  Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
