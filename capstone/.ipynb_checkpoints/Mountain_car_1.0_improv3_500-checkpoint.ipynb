{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Improvement by decreasing gamma - 500 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.2)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 15:42:06,724] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.2\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 157 steps, reward Average: -197.3125, up to now: minReward: -157.0, minAverage: -197.3125\n",
      "Episode 16 done after 155 steps, reward Average: -194.8235294117647, up to now: minReward: -155.0, minAverage: -194.8235294117647\n",
      "Episode 17 done after 155 steps, reward Average: -192.61111111111111, up to now: minReward: -155.0, minAverage: -192.61111111111111\n",
      "Episode 18 done after 153 steps, reward Average: -190.52631578947367, up to now: minReward: -153.0, minAverage: -190.52631578947367\n",
      "Episode 19 done after 151 steps, reward Average: -188.55, up to now: minReward: -151.0, minAverage: -188.55\n",
      "Episode 20 done after 145 steps, reward Average: -186.47619047619048, up to now: minReward: -145.0, minAverage: -186.47619047619048\n",
      "Episode 21 done after 148 steps, reward Average: -184.72727272727272, up to now: minReward: -145.0, minAverage: -184.72727272727272\n",
      "Episode 22 done after 144 steps, reward Average: -182.95652173913044, up to now: minReward: -144.0, minAverage: -182.95652173913044\n",
      "Episode 23 done after 152 steps, reward Average: -181.66666666666666, up to now: minReward: -144.0, minAverage: -181.66666666666666\n",
      "Episode 24 done after 175 steps, reward Average: -181.4, up to now: minReward: -144.0, minAverage: -181.4\n",
      "Episode 25 done after 141 steps, reward Average: -179.84615384615384, up to now: minReward: -141.0, minAverage: -179.84615384615384\n",
      "Episode 26 done after 144 steps, reward Average: -178.5185185185185, up to now: minReward: -141.0, minAverage: -178.5185185185185\n",
      "Episode 27 done after 139 steps, reward Average: -177.10714285714286, up to now: minReward: -139.0, minAverage: -177.10714285714286\n",
      "Episode 28 done after 141 steps, reward Average: -175.86206896551724, up to now: minReward: -139.0, minAverage: -175.86206896551724\n",
      "Episode 29 done after 138 steps, reward Average: -174.6, up to now: minReward: -138.0, minAverage: -174.6\n",
      "Episode 30 done after 140 steps, reward Average: -173.48387096774192, up to now: minReward: -138.0, minAverage: -173.48387096774192\n",
      "Episode 31 done after 144 steps, reward Average: -172.5625, up to now: minReward: -138.0, minAverage: -172.5625\n",
      "Episode 32 done after 136 steps, reward Average: -171.45454545454547, up to now: minReward: -136.0, minAverage: -171.45454545454547\n",
      "Episode 33 done after 139 steps, reward Average: -170.5, up to now: minReward: -136.0, minAverage: -170.5\n",
      "Episode 34 done after 140 steps, reward Average: -169.62857142857143, up to now: minReward: -136.0, minAverage: -169.62857142857143\n",
      "Episode 35 done after 138 steps, reward Average: -168.75, up to now: minReward: -136.0, minAverage: -168.75\n",
      "Episode 36 done after 137 steps, reward Average: -167.8918918918919, up to now: minReward: -136.0, minAverage: -167.8918918918919\n",
      "Episode 37 done after 141 steps, reward Average: -167.18421052631578, up to now: minReward: -136.0, minAverage: -167.18421052631578\n",
      "Episode 38 done after 137 steps, reward Average: -166.4102564102564, up to now: minReward: -136.0, minAverage: -166.4102564102564\n",
      "Episode 39 done after 136 steps, reward Average: -165.65, up to now: minReward: -136.0, minAverage: -165.65\n",
      "Episode 40 done after 135 steps, reward Average: -164.90243902439025, up to now: minReward: -135.0, minAverage: -164.90243902439025\n",
      "Episode 41 done after 141 steps, reward Average: -164.33333333333334, up to now: minReward: -135.0, minAverage: -164.33333333333334\n",
      "Episode 42 done after 135 steps, reward Average: -163.65116279069767, up to now: minReward: -135.0, minAverage: -163.65116279069767\n",
      "Episode 43 done after 135 steps, reward Average: -163.0, up to now: minReward: -135.0, minAverage: -163.0\n",
      "Episode 44 done after 135 steps, reward Average: -162.37777777777777, up to now: minReward: -135.0, minAverage: -162.37777777777777\n",
      "Episode 45 done after 134 steps, reward Average: -161.7608695652174, up to now: minReward: -134.0, minAverage: -161.7608695652174\n",
      "Episode 46 done after 135 steps, reward Average: -161.19148936170214, up to now: minReward: -134.0, minAverage: -161.19148936170214\n",
      "Episode 47 done after 135 steps, reward Average: -160.64583333333334, up to now: minReward: -134.0, minAverage: -160.64583333333334\n",
      "Episode 48 done after 137 steps, reward Average: -160.16326530612244, up to now: minReward: -134.0, minAverage: -160.16326530612244\n",
      "Episode 49 done after 138 steps, reward Average: -159.72, up to now: minReward: -134.0, minAverage: -159.72\n",
      "Episode 50 done after 141 steps, reward Average: -159.35294117647058, up to now: minReward: -134.0, minAverage: -159.35294117647058\n",
      "Episode 51 done after 137 steps, reward Average: -158.92307692307693, up to now: minReward: -134.0, minAverage: -158.92307692307693\n",
      "Episode 52 done after 135 steps, reward Average: -158.47169811320754, up to now: minReward: -134.0, minAverage: -158.47169811320754\n",
      "Episode 53 done after 139 steps, reward Average: -158.11111111111111, up to now: minReward: -134.0, minAverage: -158.11111111111111\n",
      "Episode 54 done after 137 steps, reward Average: -157.72727272727272, up to now: minReward: -134.0, minAverage: -157.72727272727272\n",
      "Episode 55 done after 137 steps, reward Average: -157.35714285714286, up to now: minReward: -134.0, minAverage: -157.35714285714286\n",
      "Episode 56 done after 140 steps, reward Average: -157.05263157894737, up to now: minReward: -134.0, minAverage: -157.05263157894737\n",
      "Episode 57 done after 136 steps, reward Average: -156.68965517241378, up to now: minReward: -134.0, minAverage: -156.68965517241378\n",
      "Episode 58 done after 135 steps, reward Average: -156.32203389830508, up to now: minReward: -134.0, minAverage: -156.32203389830508\n",
      "Episode 59 done after 135 steps, reward Average: -155.96666666666667, up to now: minReward: -134.0, minAverage: -155.96666666666667\n",
      "Episode 60 done after 139 steps, reward Average: -155.68852459016392, up to now: minReward: -134.0, minAverage: -155.68852459016392\n",
      "Episode 61 done after 135 steps, reward Average: -155.3548387096774, up to now: minReward: -134.0, minAverage: -155.3548387096774\n",
      "Episode 62 done after 137 steps, reward Average: -155.06349206349208, up to now: minReward: -134.0, minAverage: -155.06349206349208\n",
      "Episode 63 done after 135 steps, reward Average: -154.75, up to now: minReward: -134.0, minAverage: -154.75\n",
      "Episode 64 done after 135 steps, reward Average: -154.44615384615383, up to now: minReward: -134.0, minAverage: -154.44615384615383\n",
      "Episode 65 done after 141 steps, reward Average: -154.24242424242425, up to now: minReward: -134.0, minAverage: -154.24242424242425\n",
      "Episode 66 done after 135 steps, reward Average: -153.955223880597, up to now: minReward: -134.0, minAverage: -153.955223880597\n",
      "Episode 67 done after 135 steps, reward Average: -153.6764705882353, up to now: minReward: -134.0, minAverage: -153.6764705882353\n",
      "Episode 68 done after 141 steps, reward Average: -153.4927536231884, up to now: minReward: -134.0, minAverage: -153.4927536231884\n",
      "Episode 69 done after 135 steps, reward Average: -153.22857142857143, up to now: minReward: -134.0, minAverage: -153.22857142857143\n",
      "Episode 70 done after 135 steps, reward Average: -152.9718309859155, up to now: minReward: -134.0, minAverage: -152.9718309859155\n",
      "Episode 71 done after 137 steps, reward Average: -152.75, up to now: minReward: -134.0, minAverage: -152.75\n",
      "Episode 72 done after 135 steps, reward Average: -152.5068493150685, up to now: minReward: -134.0, minAverage: -152.5068493150685\n",
      "Episode 73 done after 137 steps, reward Average: -152.2972972972973, up to now: minReward: -134.0, minAverage: -152.2972972972973\n",
      "Episode 74 done after 136 steps, reward Average: -152.08, up to now: minReward: -134.0, minAverage: -152.08\n",
      "Episode 75 done after 138 steps, reward Average: -151.89473684210526, up to now: minReward: -134.0, minAverage: -151.89473684210526\n",
      "Episode 76 done after 137 steps, reward Average: -151.7012987012987, up to now: minReward: -134.0, minAverage: -151.7012987012987\n",
      "Episode 77 done after 140 steps, reward Average: -151.55128205128204, up to now: minReward: -134.0, minAverage: -151.55128205128204\n",
      "Episode 78 done after 137 steps, reward Average: -151.36708860759492, up to now: minReward: -134.0, minAverage: -151.36708860759492\n",
      "Episode 79 done after 137 steps, reward Average: -151.1875, up to now: minReward: -134.0, minAverage: -151.1875\n",
      "Episode 80 done after 137 steps, reward Average: -151.01234567901236, up to now: minReward: -134.0, minAverage: -151.01234567901236\n",
      "Episode 81 done after 141 steps, reward Average: -150.890243902439, up to now: minReward: -134.0, minAverage: -150.890243902439\n",
      "Episode 82 done after 135 steps, reward Average: -150.6987951807229, up to now: minReward: -134.0, minAverage: -150.6987951807229\n",
      "Episode 83 done after 136 steps, reward Average: -150.52380952380952, up to now: minReward: -134.0, minAverage: -150.52380952380952\n",
      "Episode 84 done after 135 steps, reward Average: -150.34117647058824, up to now: minReward: -134.0, minAverage: -150.34117647058824\n",
      "Episode 85 done after 136 steps, reward Average: -150.17441860465115, up to now: minReward: -134.0, minAverage: -150.17441860465115\n",
      "Episode 86 done after 137 steps, reward Average: -150.02298850574712, up to now: minReward: -134.0, minAverage: -150.02298850574712\n",
      "Episode 87 done after 136 steps, reward Average: -149.86363636363637, up to now: minReward: -134.0, minAverage: -149.86363636363637\n",
      "Episode 88 done after 136 steps, reward Average: -149.7078651685393, up to now: minReward: -134.0, minAverage: -149.7078651685393\n",
      "Episode 89 done after 142 steps, reward Average: -149.62222222222223, up to now: minReward: -134.0, minAverage: -149.62222222222223\n",
      "Episode 90 done after 137 steps, reward Average: -149.4835164835165, up to now: minReward: -134.0, minAverage: -149.4835164835165\n",
      "Episode 91 done after 136 steps, reward Average: -149.33695652173913, up to now: minReward: -134.0, minAverage: -149.33695652173913\n",
      "Episode 92 done after 140 steps, reward Average: -149.23655913978496, up to now: minReward: -134.0, minAverage: -149.23655913978496\n",
      "Episode 93 done after 135 steps, reward Average: -149.08510638297872, up to now: minReward: -134.0, minAverage: -149.08510638297872\n",
      "Episode 94 done after 135 steps, reward Average: -148.93684210526317, up to now: minReward: -134.0, minAverage: -148.93684210526317\n",
      "Episode 95 done after 135 steps, reward Average: -148.79166666666666, up to now: minReward: -134.0, minAverage: -148.79166666666666\n",
      "Episode 96 done after 135 steps, reward Average: -148.64948453608247, up to now: minReward: -134.0, minAverage: -148.64948453608247\n",
      "Episode 97 done after 135 steps, reward Average: -148.51020408163265, up to now: minReward: -134.0, minAverage: -148.51020408163265\n",
      "Episode 98 done after 135 steps, reward Average: -148.37373737373738, up to now: minReward: -134.0, minAverage: -148.37373737373738\n",
      "Episode 99 done after 135 steps, reward Average: -148.24, up to now: minReward: -134.0, minAverage: -148.24\n",
      "Episode 100 done after 138 steps, reward Average: -147.62, up to now: minReward: -134.0, minAverage: -147.62\n",
      "Episode 101 done after 137 steps, reward Average: -146.99, up to now: minReward: -134.0, minAverage: -146.99\n",
      "Episode 102 done after 137 steps, reward Average: -146.36, up to now: minReward: -134.0, minAverage: -146.36\n",
      "Episode 103 done after 137 steps, reward Average: -145.73, up to now: minReward: -134.0, minAverage: -145.73\n",
      "Episode 104 done after 135 steps, reward Average: -145.08, up to now: minReward: -134.0, minAverage: -145.08\n",
      "Episode 105 done after 135 steps, reward Average: -144.43, up to now: minReward: -134.0, minAverage: -144.43\n",
      "Episode 106 done after 135 steps, reward Average: -143.78, up to now: minReward: -134.0, minAverage: -143.78\n",
      "Episode 107 done after 135 steps, reward Average: -143.13, up to now: minReward: -134.0, minAverage: -143.13\n",
      "Episode 108 done after 135 steps, reward Average: -142.48, up to now: minReward: -134.0, minAverage: -142.48\n",
      "Episode 109 done after 153 steps, reward Average: -142.01, up to now: minReward: -134.0, minAverage: -142.01\n",
      "Episode 110 done after 135 steps, reward Average: -141.36, up to now: minReward: -134.0, minAverage: -141.36\n",
      "Episode 111 done after 135 steps, reward Average: -140.71, up to now: minReward: -134.0, minAverage: -140.71\n",
      "Episode 112 done after 135 steps, reward Average: -140.06, up to now: minReward: -134.0, minAverage: -140.06\n",
      "Episode 113 done after 136 steps, reward Average: -139.42, up to now: minReward: -134.0, minAverage: -139.42\n",
      "Episode 114 done after 135 steps, reward Average: -138.77, up to now: minReward: -134.0, minAverage: -138.77\n",
      "Episode 115 done after 136 steps, reward Average: -138.56, up to now: minReward: -134.0, minAverage: -138.56\n",
      "Episode 116 done after 135 steps, reward Average: -138.36, up to now: minReward: -134.0, minAverage: -138.36\n",
      "Episode 117 done after 136 steps, reward Average: -138.17, up to now: minReward: -134.0, minAverage: -138.17\n",
      "Episode 118 done after 137 steps, reward Average: -138.01, up to now: minReward: -134.0, minAverage: -138.01\n",
      "Episode 119 done after 135 steps, reward Average: -137.85, up to now: minReward: -134.0, minAverage: -137.85\n",
      "Episode 120 done after 140 steps, reward Average: -137.8, up to now: minReward: -134.0, minAverage: -137.8\n",
      "Episode 121 done after 138 steps, reward Average: -137.7, up to now: minReward: -134.0, minAverage: -137.7\n",
      "Episode 122 done after 135 steps, reward Average: -137.61, up to now: minReward: -134.0, minAverage: -137.61\n",
      "Episode 123 done after 135 steps, reward Average: -137.44, up to now: minReward: -134.0, minAverage: -137.44\n",
      "Episode 124 done after 135 steps, reward Average: -137.04, up to now: minReward: -134.0, minAverage: -137.04\n",
      "Episode 125 done after 137 steps, reward Average: -137.0, up to now: minReward: -134.0, minAverage: -137.0\n",
      "Episode 126 done after 135 steps, reward Average: -136.91, up to now: minReward: -134.0, minAverage: -136.91\n",
      "Episode 127 done after 139 steps, reward Average: -136.91, up to now: minReward: -134.0, minAverage: -136.91\n",
      "Episode 128 done after 135 steps, reward Average: -136.85, up to now: minReward: -134.0, minAverage: -136.85\n",
      "Episode 129 done after 135 steps, reward Average: -136.82, up to now: minReward: -134.0, minAverage: -136.82\n",
      "Episode 130 done after 135 steps, reward Average: -136.77, up to now: minReward: -134.0, minAverage: -136.77\n",
      "Episode 131 done after 135 steps, reward Average: -136.68, up to now: minReward: -134.0, minAverage: -136.68\n",
      "Episode 132 done after 137 steps, reward Average: -136.69, up to now: minReward: -134.0, minAverage: -136.68\n",
      "Episode 133 done after 135 steps, reward Average: -136.65, up to now: minReward: -134.0, minAverage: -136.65\n",
      "Episode 134 done after 142 steps, reward Average: -136.67, up to now: minReward: -134.0, minAverage: -136.65\n",
      "Episode 135 done after 135 steps, reward Average: -136.64, up to now: minReward: -134.0, minAverage: -136.64\n",
      "Episode 136 done after 135 steps, reward Average: -136.62, up to now: minReward: -134.0, minAverage: -136.62\n",
      "Episode 137 done after 134 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.55\n",
      "Episode 138 done after 135 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 139 done after 138 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 140 done after 135 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 141 done after 139 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 142 done after 139 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 143 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 144 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 145 done after 135 steps, reward Average: -136.58, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 146 done after 136 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 147 done after 135 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 148 done after 136 steps, reward Average: -136.58, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 149 done after 136 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.53\n",
      "Episode 150 done after 136 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.51\n",
      "Episode 151 done after 137 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.51\n",
      "Episode 152 done after 140 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.51\n",
      "Episode 153 done after 134 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.51\n",
      "Episode 154 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.49\n",
      "Episode 155 done after 134 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 156 done after 140 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 157 done after 140 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 158 done after 138 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 159 done after 135 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 160 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 161 done after 136 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 162 done after 138 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 163 done after 137 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 164 done after 140 steps, reward Average: -136.58, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 165 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 166 done after 137 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 167 done after 135 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 168 done after 136 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 169 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 170 done after 139 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 171 done after 137 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 172 done after 136 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 173 done after 134 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 174 done after 136 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 175 done after 136 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 176 done after 135 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.46\n",
      "Episode 177 done after 138 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.45\n",
      "Episode 178 done after 135 steps, reward Average: -136.43, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 179 done after 141 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 180 done after 137 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 181 done after 138 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 182 done after 137 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 183 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 184 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 185 done after 139 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 186 done after 139 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 187 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 188 done after 135 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 189 done after 137 steps, reward Average: -136.43, up to now: minReward: -134.0, minAverage: -136.43\n",
      "Episode 190 done after 136 steps, reward Average: -136.42, up to now: minReward: -134.0, minAverage: -136.42\n",
      "Episode 191 done after 137 steps, reward Average: -136.43, up to now: minReward: -134.0, minAverage: -136.42\n",
      "Episode 192 done after 135 steps, reward Average: -136.38, up to now: minReward: -134.0, minAverage: -136.38\n",
      "Episode 193 done after 135 steps, reward Average: -136.38, up to now: minReward: -134.0, minAverage: -136.38\n",
      "Episode 194 done after 135 steps, reward Average: -136.38, up to now: minReward: -134.0, minAverage: -136.38\n",
      "Episode 195 done after 135 steps, reward Average: -136.38, up to now: minReward: -134.0, minAverage: -136.38\n",
      "Episode 196 done after 134 steps, reward Average: -136.37, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 197 done after 141 steps, reward Average: -136.43, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 198 done after 135 steps, reward Average: -136.43, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 199 done after 136 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 200 done after 136 steps, reward Average: -136.42, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 201 done after 137 steps, reward Average: -136.42, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 202 done after 136 steps, reward Average: -136.41, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 203 done after 142 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 204 done after 135 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 205 done after 135 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 206 done after 135 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 207 done after 138 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 208 done after 137 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 209 done after 140 steps, reward Average: -136.38, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 210 done after 141 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 211 done after 135 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 212 done after 137 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 213 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 214 done after 136 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 215 done after 137 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 216 done after 138 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 217 done after 137 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 218 done after 136 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 219 done after 137 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 220 done after 135 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 221 done after 136 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 222 done after 137 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 223 done after 135 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 224 done after 134 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 225 done after 140 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 226 done after 138 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 227 done after 135 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 228 done after 135 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 229 done after 136 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 230 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 231 done after 136 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 232 done after 136 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 233 done after 136 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 234 done after 135 steps, reward Average: -136.43, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 235 done after 139 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 236 done after 135 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 237 done after 135 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 238 done after 135 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 239 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 240 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 241 done after 135 steps, reward Average: -136.41, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 242 done after 138 steps, reward Average: -136.4, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 243 done after 141 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 244 done after 139 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 245 done after 135 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 246 done after 138 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 247 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 248 done after 140 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 249 done after 139 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 250 done after 137 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 251 done after 135 steps, reward Average: -136.58, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 252 done after 138 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 253 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 254 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 255 done after 136 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 256 done after 136 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 257 done after 140 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 258 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 259 done after 140 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 260 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 261 done after 135 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 262 done after 137 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 263 done after 136 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 264 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 265 done after 136 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 266 done after 141 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 267 done after 135 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 268 done after 141 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 269 done after 138 steps, reward Average: -136.62, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 270 done after 137 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 271 done after 138 steps, reward Average: -136.61, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 272 done after 138 steps, reward Average: -136.63, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 273 done after 135 steps, reward Average: -136.64, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 274 done after 138 steps, reward Average: -136.66, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 275 done after 135 steps, reward Average: -136.65, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 276 done after 135 steps, reward Average: -136.65, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 277 done after 135 steps, reward Average: -136.62, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 278 done after 136 steps, reward Average: -136.63, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 279 done after 139 steps, reward Average: -136.61, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 280 done after 135 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 281 done after 137 steps, reward Average: -136.58, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 282 done after 136 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 283 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 284 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 285 done after 135 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 286 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 287 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 288 done after 135 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 289 done after 135 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 290 done after 135 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 291 done after 135 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 292 done after 135 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 293 done after 135 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 294 done after 140 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 295 done after 136 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 296 done after 135 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 297 done after 138 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 298 done after 135 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 299 done after 135 steps, reward Average: -136.47, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 300 done after 141 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 301 done after 137 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 302 done after 135 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 303 done after 135 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 304 done after 140 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 305 done after 141 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 306 done after 140 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 307 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 308 done after 135 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 309 done after 135 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 310 done after 136 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 311 done after 136 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 312 done after 135 steps, reward Average: -136.44, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 313 done after 140 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 314 done after 135 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 315 done after 137 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 316 done after 138 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 317 done after 138 steps, reward Average: -136.49, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 318 done after 141 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 319 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 320 done after 137 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 321 done after 137 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 322 done after 135 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 323 done after 137 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 324 done after 135 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 325 done after 138 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 326 done after 136 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 327 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 328 done after 136 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 329 done after 138 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 330 done after 135 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 331 done after 141 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 332 done after 135 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 333 done after 137 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 334 done after 138 steps, reward Average: -136.63, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 335 done after 136 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 336 done after 135 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 337 done after 135 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 338 done after 135 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 339 done after 137 steps, reward Average: -136.62, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 340 done after 136 steps, reward Average: -136.63, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 341 done after 135 steps, reward Average: -136.63, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 342 done after 138 steps, reward Average: -136.63, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 343 done after 135 steps, reward Average: -136.57, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 344 done after 138 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 345 done after 135 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 346 done after 135 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 347 done after 135 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 348 done after 141 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 349 done after 138 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 350 done after 138 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 351 done after 135 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 352 done after 136 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 353 done after 137 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 354 done after 136 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 355 done after 135 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 356 done after 140 steps, reward Average: -136.58, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 357 done after 141 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 358 done after 136 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 359 done after 138 steps, reward Average: -136.58, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 360 done after 138 steps, reward Average: -136.61, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 361 done after 135 steps, reward Average: -136.61, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 362 done after 135 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 363 done after 138 steps, reward Average: -136.61, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 364 done after 136 steps, reward Average: -136.62, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 365 done after 135 steps, reward Average: -136.61, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 366 done after 135 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 367 done after 141 steps, reward Average: -136.61, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 368 done after 135 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 369 done after 138 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 370 done after 135 steps, reward Average: -136.53, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 371 done after 135 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 372 done after 136 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 373 done after 135 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 374 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 375 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 376 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 377 done after 135 steps, reward Average: -136.45, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 378 done after 142 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 379 done after 134 steps, reward Average: -136.46, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 380 done after 137 steps, reward Average: -136.48, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 381 done after 140 steps, reward Average: -136.51, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 382 done after 135 steps, reward Average: -136.5, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 383 done after 137 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 384 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 385 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 386 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 387 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 388 done after 135 steps, reward Average: -136.52, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 389 done after 137 steps, reward Average: -136.54, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 390 done after 137 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 391 done after 135 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 392 done after 135 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 393 done after 140 steps, reward Average: -136.61, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 394 done after 135 steps, reward Average: -136.56, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 395 done after 135 steps, reward Average: -136.55, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 396 done after 140 steps, reward Average: -136.6, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 397 done after 146 steps, reward Average: -136.68, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 398 done after 135 steps, reward Average: -136.68, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 399 done after 135 steps, reward Average: -136.68, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 400 done after 139 steps, reward Average: -136.66, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 401 done after 141 steps, reward Average: -136.7, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 402 done after 135 steps, reward Average: -136.7, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 403 done after 138 steps, reward Average: -136.73, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 404 done after 135 steps, reward Average: -136.68, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 405 done after 137 steps, reward Average: -136.64, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 406 done after 135 steps, reward Average: -136.59, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 407 done after 141 steps, reward Average: -136.65, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 408 done after 136 steps, reward Average: -136.66, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 409 done after 136 steps, reward Average: -136.67, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 410 done after 136 steps, reward Average: -136.67, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 411 done after 136 steps, reward Average: -136.67, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 412 done after 136 steps, reward Average: -136.68, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 413 done after 138 steps, reward Average: -136.66, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 414 done after 137 steps, reward Average: -136.68, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 415 done after 141 steps, reward Average: -136.72, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 416 done after 138 steps, reward Average: -136.72, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 417 done after 136 steps, reward Average: -136.7, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 418 done after 138 steps, reward Average: -136.67, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 419 done after 141 steps, reward Average: -136.73, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 420 done after 137 steps, reward Average: -136.73, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 421 done after 141 steps, reward Average: -136.77, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 422 done after 135 steps, reward Average: -136.77, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 423 done after 136 steps, reward Average: -136.76, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 424 done after 136 steps, reward Average: -136.77, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 425 done after 139 steps, reward Average: -136.78, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 426 done after 137 steps, reward Average: -136.79, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 427 done after 141 steps, reward Average: -136.85, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 428 done after 140 steps, reward Average: -136.89, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 429 done after 141 steps, reward Average: -136.92, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 430 done after 136 steps, reward Average: -136.93, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 431 done after 135 steps, reward Average: -136.87, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 432 done after 135 steps, reward Average: -136.87, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 433 done after 137 steps, reward Average: -136.87, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 434 done after 140 steps, reward Average: -136.89, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 435 done after 140 steps, reward Average: -136.93, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 436 done after 135 steps, reward Average: -136.93, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 437 done after 140 steps, reward Average: -136.98, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 438 done after 135 steps, reward Average: -136.98, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 439 done after 136 steps, reward Average: -136.97, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 440 done after 140 steps, reward Average: -137.01, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 441 done after 135 steps, reward Average: -137.01, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 442 done after 135 steps, reward Average: -136.98, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 443 done after 137 steps, reward Average: -137.0, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 444 done after 135 steps, reward Average: -136.97, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 445 done after 136 steps, reward Average: -136.98, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 446 done after 135 steps, reward Average: -136.98, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 447 done after 135 steps, reward Average: -136.98, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 448 done after 138 steps, reward Average: -136.95, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 449 done after 136 steps, reward Average: -136.93, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 450 done after 136 steps, reward Average: -136.91, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 451 done after 135 steps, reward Average: -136.91, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 452 done after 136 steps, reward Average: -136.91, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 453 done after 136 steps, reward Average: -136.9, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 454 done after 135 steps, reward Average: -136.89, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 455 done after 141 steps, reward Average: -136.95, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 456 done after 137 steps, reward Average: -136.92, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 457 done after 141 steps, reward Average: -136.92, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 458 done after 140 steps, reward Average: -136.96, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 459 done after 135 steps, reward Average: -136.93, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 460 done after 135 steps, reward Average: -136.9, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 461 done after 135 steps, reward Average: -136.9, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 462 done after 135 steps, reward Average: -136.9, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 463 done after 145 steps, reward Average: -136.97, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 464 done after 135 steps, reward Average: -136.96, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 465 done after 136 steps, reward Average: -136.97, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 466 done after 136 steps, reward Average: -136.98, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 467 done after 136 steps, reward Average: -136.93, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 468 done after 139 steps, reward Average: -136.97, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 469 done after 140 steps, reward Average: -136.99, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 470 done after 135 steps, reward Average: -136.99, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 471 done after 137 steps, reward Average: -137.01, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 472 done after 137 steps, reward Average: -137.02, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 473 done after 135 steps, reward Average: -137.02, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 474 done after 134 steps, reward Average: -137.01, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 475 done after 135 steps, reward Average: -137.01, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 476 done after 135 steps, reward Average: -137.01, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 477 done after 141 steps, reward Average: -137.07, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 478 done after 137 steps, reward Average: -137.02, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 479 done after 141 steps, reward Average: -137.09, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 480 done after 138 steps, reward Average: -137.1, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 481 done after 136 steps, reward Average: -137.06, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 482 done after 136 steps, reward Average: -137.07, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 483 done after 137 steps, reward Average: -137.07, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 484 done after 140 steps, reward Average: -137.12, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 485 done after 140 steps, reward Average: -137.17, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 486 done after 135 steps, reward Average: -137.17, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 487 done after 136 steps, reward Average: -137.18, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 488 done after 136 steps, reward Average: -137.19, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 489 done after 139 steps, reward Average: -137.21, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 490 done after 138 steps, reward Average: -137.22, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 491 done after 137 steps, reward Average: -137.24, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 492 done after 135 steps, reward Average: -137.24, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 493 done after 135 steps, reward Average: -137.19, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 494 done after 135 steps, reward Average: -137.19, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 495 done after 135 steps, reward Average: -137.19, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 496 done after 135 steps, reward Average: -137.14, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 497 done after 135 steps, reward Average: -137.03, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 498 done after 138 steps, reward Average: -137.06, up to now: minReward: -134.0, minAverage: -136.37\n",
      "Episode 499 done after 138 steps, reward Average: -137.09, up to now: minReward: -134.0, minAverage: -136.37\n",
      "final result: \n",
      "485 times arrived in 500 episodes, first time in episode 15\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2557bb3a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
