{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Try QLearning - 500 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.2)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 20:30:21,176] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.2\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=False\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 195 steps, reward Average: -199.6153846153846, up to now: minReward: -195.0, minAverage: -199.6153846153846\n",
      "Episode 13 done after 200 steps, reward Average: -199.64285714285714, up to now: minReward: -195.0, minAverage: -199.6153846153846\n",
      "Episode 14 done after 200 steps, reward Average: -199.66666666666666, up to now: minReward: -195.0, minAverage: -199.6153846153846\n",
      "Episode 15 done after 200 steps, reward Average: -199.6875, up to now: minReward: -195.0, minAverage: -199.6153846153846\n",
      "Episode 16 done after 200 steps, reward Average: -199.7058823529412, up to now: minReward: -195.0, minAverage: -199.6153846153846\n",
      "Episode 17 done after 200 steps, reward Average: -199.72222222222223, up to now: minReward: -195.0, minAverage: -199.6153846153846\n",
      "Episode 18 done after 121 steps, reward Average: -195.57894736842104, up to now: minReward: -121.0, minAverage: -195.57894736842104\n",
      "Episode 19 done after 111 steps, reward Average: -191.35, up to now: minReward: -111.0, minAverage: -191.35\n",
      "Episode 20 done after 113 steps, reward Average: -187.61904761904762, up to now: minReward: -111.0, minAverage: -187.61904761904762\n",
      "Episode 21 done after 107 steps, reward Average: -183.95454545454547, up to now: minReward: -107.0, minAverage: -183.95454545454547\n",
      "Episode 22 done after 111 steps, reward Average: -180.7826086956522, up to now: minReward: -107.0, minAverage: -180.7826086956522\n",
      "Episode 23 done after 112 steps, reward Average: -177.91666666666666, up to now: minReward: -107.0, minAverage: -177.91666666666666\n",
      "Episode 24 done after 110 steps, reward Average: -175.2, up to now: minReward: -107.0, minAverage: -175.2\n",
      "Episode 25 done after 108 steps, reward Average: -172.6153846153846, up to now: minReward: -107.0, minAverage: -172.6153846153846\n",
      "Episode 26 done after 105 steps, reward Average: -170.11111111111111, up to now: minReward: -105.0, minAverage: -170.11111111111111\n",
      "Episode 27 done after 110 steps, reward Average: -167.96428571428572, up to now: minReward: -105.0, minAverage: -167.96428571428572\n",
      "Episode 28 done after 119 steps, reward Average: -166.27586206896552, up to now: minReward: -105.0, minAverage: -166.27586206896552\n",
      "Episode 29 done after 106 steps, reward Average: -164.26666666666668, up to now: minReward: -105.0, minAverage: -164.26666666666668\n",
      "Episode 30 done after 105 steps, reward Average: -162.3548387096774, up to now: minReward: -105.0, minAverage: -162.3548387096774\n",
      "Episode 31 done after 105 steps, reward Average: -160.5625, up to now: minReward: -105.0, minAverage: -160.5625\n",
      "Episode 32 done after 105 steps, reward Average: -158.87878787878788, up to now: minReward: -105.0, minAverage: -158.87878787878788\n",
      "Episode 33 done after 105 steps, reward Average: -157.2941176470588, up to now: minReward: -105.0, minAverage: -157.2941176470588\n",
      "Episode 34 done after 119 steps, reward Average: -156.2, up to now: minReward: -105.0, minAverage: -156.2\n",
      "Episode 35 done after 105 steps, reward Average: -154.77777777777777, up to now: minReward: -105.0, minAverage: -154.77777777777777\n",
      "Episode 36 done after 104 steps, reward Average: -153.40540540540542, up to now: minReward: -104.0, minAverage: -153.40540540540542\n",
      "Episode 37 done after 103 steps, reward Average: -152.07894736842104, up to now: minReward: -103.0, minAverage: -152.07894736842104\n",
      "Episode 38 done after 103 steps, reward Average: -150.82051282051282, up to now: minReward: -103.0, minAverage: -150.82051282051282\n",
      "Episode 39 done after 104 steps, reward Average: -149.65, up to now: minReward: -103.0, minAverage: -149.65\n",
      "Episode 40 done after 104 steps, reward Average: -148.53658536585365, up to now: minReward: -103.0, minAverage: -148.53658536585365\n",
      "Episode 41 done after 104 steps, reward Average: -147.47619047619048, up to now: minReward: -103.0, minAverage: -147.47619047619048\n",
      "Episode 42 done after 109 steps, reward Average: -146.58139534883722, up to now: minReward: -103.0, minAverage: -146.58139534883722\n",
      "Episode 43 done after 104 steps, reward Average: -145.61363636363637, up to now: minReward: -103.0, minAverage: -145.61363636363637\n",
      "Episode 44 done after 105 steps, reward Average: -144.7111111111111, up to now: minReward: -103.0, minAverage: -144.7111111111111\n",
      "Episode 45 done after 104 steps, reward Average: -143.82608695652175, up to now: minReward: -103.0, minAverage: -143.82608695652175\n",
      "Episode 46 done after 105 steps, reward Average: -143.0, up to now: minReward: -103.0, minAverage: -143.0\n",
      "Episode 47 done after 104 steps, reward Average: -142.1875, up to now: minReward: -103.0, minAverage: -142.1875\n",
      "Episode 48 done after 118 steps, reward Average: -141.69387755102042, up to now: minReward: -103.0, minAverage: -141.69387755102042\n",
      "Episode 49 done after 105 steps, reward Average: -140.96, up to now: minReward: -103.0, minAverage: -140.96\n",
      "Episode 50 done after 119 steps, reward Average: -140.52941176470588, up to now: minReward: -103.0, minAverage: -140.52941176470588\n",
      "Episode 51 done after 105 steps, reward Average: -139.84615384615384, up to now: minReward: -103.0, minAverage: -139.84615384615384\n",
      "Episode 52 done after 105 steps, reward Average: -139.18867924528303, up to now: minReward: -103.0, minAverage: -139.18867924528303\n",
      "Episode 53 done after 118 steps, reward Average: -138.7962962962963, up to now: minReward: -103.0, minAverage: -138.7962962962963\n",
      "Episode 54 done after 104 steps, reward Average: -138.16363636363636, up to now: minReward: -103.0, minAverage: -138.16363636363636\n",
      "Episode 55 done after 118 steps, reward Average: -137.80357142857142, up to now: minReward: -103.0, minAverage: -137.80357142857142\n",
      "Episode 56 done after 104 steps, reward Average: -137.21052631578948, up to now: minReward: -103.0, minAverage: -137.21052631578948\n",
      "Episode 57 done after 104 steps, reward Average: -136.63793103448276, up to now: minReward: -103.0, minAverage: -136.63793103448276\n",
      "Episode 58 done after 105 steps, reward Average: -136.10169491525423, up to now: minReward: -103.0, minAverage: -136.10169491525423\n",
      "Episode 59 done after 105 steps, reward Average: -135.58333333333334, up to now: minReward: -103.0, minAverage: -135.58333333333334\n",
      "Episode 60 done after 103 steps, reward Average: -135.04918032786884, up to now: minReward: -103.0, minAverage: -135.04918032786884\n",
      "Episode 61 done after 105 steps, reward Average: -134.56451612903226, up to now: minReward: -103.0, minAverage: -134.56451612903226\n",
      "Episode 62 done after 110 steps, reward Average: -134.17460317460316, up to now: minReward: -103.0, minAverage: -134.17460317460316\n",
      "Episode 63 done after 104 steps, reward Average: -133.703125, up to now: minReward: -103.0, minAverage: -133.703125\n",
      "Episode 64 done after 104 steps, reward Average: -133.24615384615385, up to now: minReward: -103.0, minAverage: -133.24615384615385\n",
      "Episode 65 done after 104 steps, reward Average: -132.8030303030303, up to now: minReward: -103.0, minAverage: -132.8030303030303\n",
      "Episode 66 done after 105 steps, reward Average: -132.38805970149255, up to now: minReward: -103.0, minAverage: -132.38805970149255\n",
      "Episode 67 done after 104 steps, reward Average: -131.97058823529412, up to now: minReward: -103.0, minAverage: -131.97058823529412\n",
      "Episode 68 done after 105 steps, reward Average: -131.57971014492753, up to now: minReward: -103.0, minAverage: -131.57971014492753\n",
      "Episode 69 done after 104 steps, reward Average: -131.18571428571428, up to now: minReward: -103.0, minAverage: -131.18571428571428\n",
      "Episode 70 done after 105 steps, reward Average: -130.81690140845072, up to now: minReward: -103.0, minAverage: -130.81690140845072\n",
      "Episode 71 done after 104 steps, reward Average: -130.44444444444446, up to now: minReward: -103.0, minAverage: -130.44444444444446\n",
      "Episode 72 done after 105 steps, reward Average: -130.0958904109589, up to now: minReward: -103.0, minAverage: -130.0958904109589\n",
      "Episode 73 done after 104 steps, reward Average: -129.74324324324326, up to now: minReward: -103.0, minAverage: -129.74324324324326\n",
      "Episode 74 done after 107 steps, reward Average: -129.44, up to now: minReward: -103.0, minAverage: -129.44\n",
      "Episode 75 done after 105 steps, reward Average: -129.1184210526316, up to now: minReward: -103.0, minAverage: -129.1184210526316\n",
      "Episode 76 done after 104 steps, reward Average: -128.7922077922078, up to now: minReward: -103.0, minAverage: -128.7922077922078\n",
      "Episode 77 done after 111 steps, reward Average: -128.56410256410257, up to now: minReward: -103.0, minAverage: -128.56410256410257\n",
      "Episode 78 done after 104 steps, reward Average: -128.25316455696202, up to now: minReward: -103.0, minAverage: -128.25316455696202\n",
      "Episode 79 done after 105 steps, reward Average: -127.9625, up to now: minReward: -103.0, minAverage: -127.9625\n",
      "Episode 80 done after 104 steps, reward Average: -127.66666666666667, up to now: minReward: -103.0, minAverage: -127.66666666666667\n",
      "Episode 81 done after 104 steps, reward Average: -127.3780487804878, up to now: minReward: -103.0, minAverage: -127.3780487804878\n",
      "Episode 82 done after 104 steps, reward Average: -127.09638554216868, up to now: minReward: -103.0, minAverage: -127.09638554216868\n",
      "Episode 83 done after 105 steps, reward Average: -126.83333333333333, up to now: minReward: -103.0, minAverage: -126.83333333333333\n",
      "Episode 84 done after 104 steps, reward Average: -126.56470588235294, up to now: minReward: -103.0, minAverage: -126.56470588235294\n",
      "Episode 85 done after 104 steps, reward Average: -126.30232558139535, up to now: minReward: -103.0, minAverage: -126.30232558139535\n",
      "Episode 86 done after 104 steps, reward Average: -126.04597701149426, up to now: minReward: -103.0, minAverage: -126.04597701149426\n",
      "Episode 87 done after 106 steps, reward Average: -125.81818181818181, up to now: minReward: -103.0, minAverage: -125.81818181818181\n",
      "Episode 88 done after 104 steps, reward Average: -125.57303370786516, up to now: minReward: -103.0, minAverage: -125.57303370786516\n",
      "Episode 89 done after 104 steps, reward Average: -125.33333333333333, up to now: minReward: -103.0, minAverage: -125.33333333333333\n",
      "Episode 90 done after 104 steps, reward Average: -125.0989010989011, up to now: minReward: -103.0, minAverage: -125.0989010989011\n",
      "Episode 91 done after 102 steps, reward Average: -124.84782608695652, up to now: minReward: -102.0, minAverage: -124.84782608695652\n",
      "Episode 92 done after 104 steps, reward Average: -124.6236559139785, up to now: minReward: -102.0, minAverage: -124.6236559139785\n",
      "Episode 93 done after 104 steps, reward Average: -124.40425531914893, up to now: minReward: -102.0, minAverage: -124.40425531914893\n",
      "Episode 94 done after 103 steps, reward Average: -124.17894736842105, up to now: minReward: -102.0, minAverage: -124.17894736842105\n",
      "Episode 95 done after 104 steps, reward Average: -123.96875, up to now: minReward: -102.0, minAverage: -123.96875\n",
      "Episode 96 done after 104 steps, reward Average: -123.76288659793815, up to now: minReward: -102.0, minAverage: -123.76288659793815\n",
      "Episode 97 done after 104 steps, reward Average: -123.56122448979592, up to now: minReward: -102.0, minAverage: -123.56122448979592\n",
      "Episode 98 done after 104 steps, reward Average: -123.36363636363636, up to now: minReward: -102.0, minAverage: -123.36363636363636\n",
      "Episode 99 done after 104 steps, reward Average: -123.17, up to now: minReward: -102.0, minAverage: -123.17\n",
      "Episode 100 done after 104 steps, reward Average: -122.21, up to now: minReward: -102.0, minAverage: -122.21\n",
      "Episode 101 done after 103 steps, reward Average: -121.24, up to now: minReward: -102.0, minAverage: -121.24\n",
      "Episode 102 done after 104 steps, reward Average: -120.28, up to now: minReward: -102.0, minAverage: -120.28\n",
      "Episode 103 done after 104 steps, reward Average: -119.32, up to now: minReward: -102.0, minAverage: -119.32\n",
      "Episode 104 done after 104 steps, reward Average: -118.36, up to now: minReward: -102.0, minAverage: -118.36\n",
      "Episode 105 done after 104 steps, reward Average: -117.4, up to now: minReward: -102.0, minAverage: -117.4\n",
      "Episode 106 done after 103 steps, reward Average: -116.43, up to now: minReward: -102.0, minAverage: -116.43\n",
      "Episode 107 done after 104 steps, reward Average: -115.47, up to now: minReward: -102.0, minAverage: -115.47\n",
      "Episode 108 done after 104 steps, reward Average: -114.51, up to now: minReward: -102.0, minAverage: -114.51\n",
      "Episode 109 done after 104 steps, reward Average: -113.55, up to now: minReward: -102.0, minAverage: -113.55\n",
      "Episode 110 done after 104 steps, reward Average: -112.59, up to now: minReward: -102.0, minAverage: -112.59\n",
      "Episode 111 done after 104 steps, reward Average: -111.63, up to now: minReward: -102.0, minAverage: -111.63\n",
      "Episode 112 done after 104 steps, reward Average: -110.72, up to now: minReward: -102.0, minAverage: -110.72\n",
      "Episode 113 done after 104 steps, reward Average: -109.76, up to now: minReward: -102.0, minAverage: -109.76\n",
      "Episode 114 done after 103 steps, reward Average: -108.79, up to now: minReward: -102.0, minAverage: -108.79\n",
      "Episode 115 done after 103 steps, reward Average: -107.82, up to now: minReward: -102.0, minAverage: -107.82\n",
      "Episode 116 done after 103 steps, reward Average: -106.85, up to now: minReward: -102.0, minAverage: -106.85\n",
      "Episode 117 done after 103 steps, reward Average: -105.88, up to now: minReward: -102.0, minAverage: -105.88\n",
      "Episode 118 done after 103 steps, reward Average: -105.7, up to now: minReward: -102.0, minAverage: -105.7\n",
      "Episode 119 done after 104 steps, reward Average: -105.63, up to now: minReward: -102.0, minAverage: -105.63\n",
      "Episode 120 done after 104 steps, reward Average: -105.54, up to now: minReward: -102.0, minAverage: -105.54\n",
      "Episode 121 done after 104 steps, reward Average: -105.51, up to now: minReward: -102.0, minAverage: -105.51\n",
      "Episode 122 done after 104 steps, reward Average: -105.44, up to now: minReward: -102.0, minAverage: -105.44\n",
      "Episode 123 done after 103 steps, reward Average: -105.35, up to now: minReward: -102.0, minAverage: -105.35\n",
      "Episode 124 done after 104 steps, reward Average: -105.29, up to now: minReward: -102.0, minAverage: -105.29\n",
      "Episode 125 done after 104 steps, reward Average: -105.25, up to now: minReward: -102.0, minAverage: -105.25\n",
      "Episode 126 done after 103 steps, reward Average: -105.23, up to now: minReward: -102.0, minAverage: -105.23\n",
      "Episode 127 done after 104 steps, reward Average: -105.17, up to now: minReward: -102.0, minAverage: -105.17\n",
      "Episode 128 done after 104 steps, reward Average: -105.02, up to now: minReward: -102.0, minAverage: -105.02\n",
      "Episode 129 done after 103 steps, reward Average: -104.99, up to now: minReward: -102.0, minAverage: -104.99\n",
      "Episode 130 done after 104 steps, reward Average: -104.98, up to now: minReward: -102.0, minAverage: -104.98\n",
      "Episode 131 done after 103 steps, reward Average: -104.96, up to now: minReward: -102.0, minAverage: -104.96\n",
      "Episode 132 done after 104 steps, reward Average: -104.95, up to now: minReward: -102.0, minAverage: -104.95\n",
      "Episode 133 done after 103 steps, reward Average: -104.93, up to now: minReward: -102.0, minAverage: -104.93\n",
      "Episode 134 done after 103 steps, reward Average: -104.77, up to now: minReward: -102.0, minAverage: -104.77\n",
      "Episode 135 done after 104 steps, reward Average: -104.76, up to now: minReward: -102.0, minAverage: -104.76\n",
      "Episode 136 done after 103 steps, reward Average: -104.75, up to now: minReward: -102.0, minAverage: -104.75\n",
      "Episode 137 done after 104 steps, reward Average: -104.76, up to now: minReward: -102.0, minAverage: -104.75\n",
      "Episode 138 done after 103 steps, reward Average: -104.76, up to now: minReward: -102.0, minAverage: -104.75\n",
      "Episode 139 done after 104 steps, reward Average: -104.76, up to now: minReward: -102.0, minAverage: -104.75\n",
      "Episode 140 done after 103 steps, reward Average: -104.75, up to now: minReward: -102.0, minAverage: -104.75\n",
      "Episode 141 done after 104 steps, reward Average: -104.75, up to now: minReward: -102.0, minAverage: -104.75\n",
      "Episode 142 done after 104 steps, reward Average: -104.7, up to now: minReward: -102.0, minAverage: -104.7\n",
      "Episode 143 done after 104 steps, reward Average: -104.7, up to now: minReward: -102.0, minAverage: -104.7\n",
      "Episode 144 done after 103 steps, reward Average: -104.68, up to now: minReward: -102.0, minAverage: -104.68\n",
      "Episode 145 done after 104 steps, reward Average: -104.68, up to now: minReward: -102.0, minAverage: -104.68\n",
      "Episode 146 done after 103 steps, reward Average: -104.66, up to now: minReward: -102.0, minAverage: -104.66\n",
      "Episode 147 done after 104 steps, reward Average: -104.66, up to now: minReward: -102.0, minAverage: -104.66\n",
      "Episode 148 done after 103 steps, reward Average: -104.51, up to now: minReward: -102.0, minAverage: -104.51\n",
      "Episode 149 done after 104 steps, reward Average: -104.5, up to now: minReward: -102.0, minAverage: -104.5\n",
      "Episode 150 done after 103 steps, reward Average: -104.34, up to now: minReward: -102.0, minAverage: -104.34\n",
      "Episode 151 done after 104 steps, reward Average: -104.33, up to now: minReward: -102.0, minAverage: -104.33\n",
      "Episode 152 done after 104 steps, reward Average: -104.32, up to now: minReward: -102.0, minAverage: -104.32\n",
      "Episode 153 done after 103 steps, reward Average: -104.17, up to now: minReward: -102.0, minAverage: -104.17\n",
      "Episode 154 done after 103 steps, reward Average: -104.16, up to now: minReward: -102.0, minAverage: -104.16\n",
      "Episode 155 done after 104 steps, reward Average: -104.02, up to now: minReward: -102.0, minAverage: -104.02\n",
      "Episode 156 done after 103 steps, reward Average: -104.01, up to now: minReward: -102.0, minAverage: -104.01\n",
      "Episode 157 done after 104 steps, reward Average: -104.01, up to now: minReward: -102.0, minAverage: -104.01\n",
      "Episode 158 done after 104 steps, reward Average: -104.0, up to now: minReward: -102.0, minAverage: -104.0\n",
      "Episode 159 done after 104 steps, reward Average: -103.99, up to now: minReward: -102.0, minAverage: -103.99\n",
      "Episode 160 done after 104 steps, reward Average: -104.0, up to now: minReward: -102.0, minAverage: -103.99\n",
      "Episode 161 done after 104 steps, reward Average: -103.99, up to now: minReward: -102.0, minAverage: -103.99\n",
      "Episode 162 done after 103 steps, reward Average: -103.92, up to now: minReward: -102.0, minAverage: -103.92\n",
      "Episode 163 done after 103 steps, reward Average: -103.91, up to now: minReward: -102.0, minAverage: -103.91\n",
      "Episode 164 done after 104 steps, reward Average: -103.91, up to now: minReward: -102.0, minAverage: -103.91\n",
      "Episode 165 done after 104 steps, reward Average: -103.91, up to now: minReward: -102.0, minAverage: -103.91\n",
      "Episode 166 done after 104 steps, reward Average: -103.9, up to now: minReward: -102.0, minAverage: -103.9\n",
      "Episode 167 done after 104 steps, reward Average: -103.9, up to now: minReward: -102.0, minAverage: -103.9\n",
      "Episode 168 done after 103 steps, reward Average: -103.88, up to now: minReward: -102.0, minAverage: -103.88\n",
      "Episode 169 done after 104 steps, reward Average: -103.88, up to now: minReward: -102.0, minAverage: -103.88\n",
      "Episode 170 done after 104 steps, reward Average: -103.87, up to now: minReward: -102.0, minAverage: -103.87\n",
      "Episode 171 done after 103 steps, reward Average: -103.86, up to now: minReward: -102.0, minAverage: -103.86\n",
      "Episode 172 done after 105 steps, reward Average: -103.86, up to now: minReward: -102.0, minAverage: -103.86\n",
      "Episode 173 done after 104 steps, reward Average: -103.86, up to now: minReward: -102.0, minAverage: -103.86\n",
      "Episode 174 done after 104 steps, reward Average: -103.83, up to now: minReward: -102.0, minAverage: -103.83\n",
      "Episode 175 done after 104 steps, reward Average: -103.82, up to now: minReward: -102.0, minAverage: -103.82\n",
      "Episode 176 done after 104 steps, reward Average: -103.82, up to now: minReward: -102.0, minAverage: -103.82\n",
      "Episode 177 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.75\n",
      "Episode 178 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.75\n",
      "Episode 179 done after 104 steps, reward Average: -103.74, up to now: minReward: -102.0, minAverage: -103.74\n",
      "Episode 180 done after 105 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.74\n",
      "Episode 181 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.74\n",
      "Episode 182 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.74\n",
      "Episode 183 done after 103 steps, reward Average: -103.73, up to now: minReward: -102.0, minAverage: -103.73\n",
      "Episode 184 done after 103 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.72\n",
      "Episode 185 done after 103 steps, reward Average: -103.71, up to now: minReward: -102.0, minAverage: -103.71\n",
      "Episode 186 done after 104 steps, reward Average: -103.71, up to now: minReward: -102.0, minAverage: -103.71\n",
      "Episode 187 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 188 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 189 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 190 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 191 done after 104 steps, reward Average: -103.71, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 192 done after 103 steps, reward Average: -103.7, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 193 done after 103 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 194 done after 104 steps, reward Average: -103.7, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 195 done after 103 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 196 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 197 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 198 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.69\n",
      "Episode 199 done after 103 steps, reward Average: -103.68, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 200 done after 104 steps, reward Average: -103.68, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 201 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 202 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 203 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 204 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 205 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 206 done after 104 steps, reward Average: -103.7, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 207 done after 104 steps, reward Average: -103.7, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 208 done after 103 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 209 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 210 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 211 done after 103 steps, reward Average: -103.68, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 212 done after 104 steps, reward Average: -103.68, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 213 done after 104 steps, reward Average: -103.68, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 214 done after 104 steps, reward Average: -103.69, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 215 done after 104 steps, reward Average: -103.7, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 216 done after 103 steps, reward Average: -103.7, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 217 done after 104 steps, reward Average: -103.71, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 218 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 219 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 220 done after 103 steps, reward Average: -103.71, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 221 done after 104 steps, reward Average: -103.71, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 222 done after 104 steps, reward Average: -103.71, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 223 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 224 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 225 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 226 done after 103 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 227 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 228 done after 103 steps, reward Average: -103.71, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 229 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 230 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 231 done after 103 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 232 done after 104 steps, reward Average: -103.72, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 233 done after 104 steps, reward Average: -103.73, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 234 done after 104 steps, reward Average: -103.74, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 235 done after 104 steps, reward Average: -103.74, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 236 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 237 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 238 done after 103 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 239 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 240 done after 104 steps, reward Average: -103.76, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 241 done after 103 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 242 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 243 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 244 done after 103 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 245 done after 103 steps, reward Average: -103.74, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 246 done after 104 steps, reward Average: -103.75, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 247 done after 105 steps, reward Average: -103.76, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 248 done after 103 steps, reward Average: -103.76, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 249 done after 104 steps, reward Average: -103.76, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 250 done after 105 steps, reward Average: -103.78, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 251 done after 104 steps, reward Average: -103.78, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 252 done after 103 steps, reward Average: -103.77, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 253 done after 104 steps, reward Average: -103.78, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 254 done after 104 steps, reward Average: -103.79, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 255 done after 104 steps, reward Average: -103.79, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 256 done after 103 steps, reward Average: -103.79, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 257 done after 104 steps, reward Average: -103.79, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 258 done after 103 steps, reward Average: -103.78, up to now: minReward: -102.0, minAverage: -103.68\n",
      "Episode 259 done after 96 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 260 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 261 done after 103 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 262 done after 103 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 263 done after 105 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 264 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 265 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 266 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 267 done after 103 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 268 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 269 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 270 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 271 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 272 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 273 done after 103 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 274 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 275 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 276 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 277 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 278 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 279 done after 103 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 280 done after 104 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 281 done after 104 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 282 done after 104 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 283 done after 104 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 284 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 285 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 286 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 287 done after 103 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 288 done after 103 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 289 done after 103 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 290 done after 104 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 291 done after 104 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 292 done after 104 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 293 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 294 done after 103 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 295 done after 103 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 296 done after 104 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 297 done after 104 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 298 done after 103 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 299 done after 105 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 300 done after 104 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 301 done after 103 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 302 done after 103 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 303 done after 104 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.68\n",
      "Episode 304 done after 103 steps, reward Average: -103.67, up to now: minReward: -96.0, minAverage: -103.67\n",
      "Episode 305 done after 103 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.66\n",
      "Episode 306 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.66\n",
      "Episode 307 done after 103 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.65\n",
      "Episode 308 done after 103 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.65\n",
      "Episode 309 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.65\n",
      "Episode 310 done after 103 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.64\n",
      "Episode 311 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.64\n",
      "Episode 312 done after 103 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.64\n",
      "Episode 313 done after 104 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.64\n",
      "Episode 314 done after 103 steps, reward Average: -103.63, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 315 done after 104 steps, reward Average: -103.63, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 316 done after 105 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 317 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 318 done after 103 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 319 done after 104 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 320 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 321 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 322 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 323 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 324 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 325 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 326 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 327 done after 103 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 328 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 329 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 330 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 331 done after 104 steps, reward Average: -103.67, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 332 done after 103 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 333 done after 105 steps, reward Average: -103.67, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 334 done after 103 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 335 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 336 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 337 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 338 done after 103 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 339 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 340 done after 103 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 341 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 342 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 343 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 344 done after 103 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 345 done after 103 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 346 done after 103 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 347 done after 104 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 348 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 349 done after 103 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.63\n",
      "Episode 350 done after 103 steps, reward Average: -103.62, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 351 done after 104 steps, reward Average: -103.62, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 352 done after 104 steps, reward Average: -103.63, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 353 done after 105 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 354 done after 104 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 355 done after 104 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 356 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 357 done after 103 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 358 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 359 done after 103 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 360 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 361 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 362 done after 104 steps, reward Average: -103.74, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 363 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 364 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 365 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 366 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 367 done after 103 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 368 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 369 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 370 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 371 done after 103 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 372 done after 103 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 373 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 374 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 375 done after 103 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 376 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 377 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 378 done after 103 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 379 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 380 done after 106 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 381 done after 102 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 382 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 383 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 384 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 385 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 386 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 387 done after 103 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 388 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 389 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 390 done after 103 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 391 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 392 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 393 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 394 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 395 done after 103 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 396 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 397 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 398 done after 104 steps, reward Average: -103.74, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 399 done after 103 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 400 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 401 done after 103 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 402 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 403 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 404 done after 103 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 405 done after 104 steps, reward Average: -103.74, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 406 done after 104 steps, reward Average: -103.74, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 407 done after 102 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 408 done after 103 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 409 done after 102 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 410 done after 103 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 411 done after 104 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 412 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 413 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 414 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 415 done after 104 steps, reward Average: -103.73, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 416 done after 104 steps, reward Average: -103.72, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 417 done after 103 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 418 done after 103 steps, reward Average: -103.71, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 419 done after 103 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 420 done after 103 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 421 done after 104 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 422 done after 104 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 423 done after 104 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 424 done after 103 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 425 done after 102 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 426 done after 103 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 427 done after 103 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 428 done after 103 steps, reward Average: -103.64, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 429 done after 103 steps, reward Average: -103.63, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 430 done after 103 steps, reward Average: -103.62, up to now: minReward: -96.0, minAverage: -103.62\n",
      "Episode 431 done after 103 steps, reward Average: -103.61, up to now: minReward: -96.0, minAverage: -103.61\n",
      "Episode 432 done after 103 steps, reward Average: -103.61, up to now: minReward: -96.0, minAverage: -103.61\n",
      "Episode 433 done after 103 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.59\n",
      "Episode 434 done after 103 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.59\n",
      "Episode 435 done after 104 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.59\n",
      "Episode 436 done after 104 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.59\n",
      "Episode 437 done after 103 steps, reward Average: -103.58, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 438 done after 104 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 439 done after 104 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 440 done after 103 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 441 done after 104 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 442 done after 104 steps, reward Average: -103.59, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 443 done after 103 steps, reward Average: -103.58, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 444 done after 103 steps, reward Average: -103.58, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 445 done after 103 steps, reward Average: -103.58, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 446 done after 103 steps, reward Average: -103.58, up to now: minReward: -96.0, minAverage: -103.58\n",
      "Episode 447 done after 103 steps, reward Average: -103.57, up to now: minReward: -96.0, minAverage: -103.57\n",
      "Episode 448 done after 103 steps, reward Average: -103.56, up to now: minReward: -96.0, minAverage: -103.56\n",
      "Episode 449 done after 103 steps, reward Average: -103.56, up to now: minReward: -96.0, minAverage: -103.56\n",
      "Episode 450 done after 104 steps, reward Average: -103.57, up to now: minReward: -96.0, minAverage: -103.56\n",
      "Episode 451 done after 103 steps, reward Average: -103.56, up to now: minReward: -96.0, minAverage: -103.56\n",
      "Episode 452 done after 98 steps, reward Average: -103.5, up to now: minReward: -96.0, minAverage: -103.5\n",
      "Episode 453 done after 98 steps, reward Average: -103.43, up to now: minReward: -96.0, minAverage: -103.43\n",
      "Episode 454 done after 104 steps, reward Average: -103.43, up to now: minReward: -96.0, minAverage: -103.43\n",
      "Episode 455 done after 103 steps, reward Average: -103.42, up to now: minReward: -96.0, minAverage: -103.42\n",
      "Episode 456 done after 103 steps, reward Average: -103.41, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 457 done after 104 steps, reward Average: -103.42, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 458 done after 104 steps, reward Average: -103.42, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 459 done after 104 steps, reward Average: -103.43, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 460 done after 104 steps, reward Average: -103.43, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 461 done after 104 steps, reward Average: -103.43, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 462 done after 103 steps, reward Average: -103.42, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 463 done after 117 steps, reward Average: -103.55, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 464 done after 111 steps, reward Average: -103.62, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 465 done after 104 steps, reward Average: -103.62, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 466 done after 104 steps, reward Average: -103.62, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 467 done after 107 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 468 done after 104 steps, reward Average: -103.66, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 469 done after 103 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 470 done after 104 steps, reward Average: -103.65, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 471 done after 105 steps, reward Average: -103.67, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 472 done after 104 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 473 done after 104 steps, reward Average: -103.68, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 474 done after 106 steps, reward Average: -103.7, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 475 done after 102 steps, reward Average: -103.69, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 476 done after 118 steps, reward Average: -103.83, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 477 done after 112 steps, reward Average: -103.91, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 478 done after 105 steps, reward Average: -103.93, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 479 done after 105 steps, reward Average: -103.94, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 480 done after 104 steps, reward Average: -103.92, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 481 done after 103 steps, reward Average: -103.93, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 482 done after 103 steps, reward Average: -103.92, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 483 done after 103 steps, reward Average: -103.91, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 484 done after 104 steps, reward Average: -103.91, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 485 done after 103 steps, reward Average: -103.9, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 486 done after 104 steps, reward Average: -103.9, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 487 done after 103 steps, reward Average: -103.9, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 488 done after 103 steps, reward Average: -103.89, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 489 done after 104 steps, reward Average: -103.89, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 490 done after 105 steps, reward Average: -103.91, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 491 done after 104 steps, reward Average: -103.91, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 492 done after 105 steps, reward Average: -103.92, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 493 done after 105 steps, reward Average: -103.93, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 494 done after 104 steps, reward Average: -103.93, up to now: minReward: -96.0, minAverage: -103.41\n",
      "Episode 495 done after 89 steps, reward Average: -103.79, up to now: minReward: -89.0, minAverage: -103.41\n",
      "Episode 496 done after 104 steps, reward Average: -103.79, up to now: minReward: -89.0, minAverage: -103.41\n",
      "Episode 497 done after 105 steps, reward Average: -103.8, up to now: minReward: -89.0, minAverage: -103.41\n",
      "Episode 498 done after 105 steps, reward Average: -103.81, up to now: minReward: -89.0, minAverage: -103.41\n",
      "Episode 499 done after 104 steps, reward Average: -103.82, up to now: minReward: -89.0, minAverage: -103.41\n",
      "final result: \n",
      "483 times arrived in 500 episodes, first time in episode 12\n",
      "problem solved?:True\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c3c5b2240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
