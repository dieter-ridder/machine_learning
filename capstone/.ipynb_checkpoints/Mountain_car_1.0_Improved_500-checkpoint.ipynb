{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling 888, Alpha was further tuned, 500 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 25:\n",
    "            return 0.65\n",
    "        elif episode < 35:\n",
    "            return 0.6\n",
    "        elif episode < 45:\n",
    "            return 0.55\n",
    "        elif episode < 55:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.4\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 13:51:49,648] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 198 steps, reward Average: -199.875, up to now: minReward: -198.0, minAverage: -199.875\n",
      "Episode 16 done after 122 steps, reward Average: -195.2941176470588, up to now: minReward: -122.0, minAverage: -195.2941176470588\n",
      "Episode 17 done after 200 steps, reward Average: -195.55555555555554, up to now: minReward: -122.0, minAverage: -195.2941176470588\n",
      "Episode 18 done after 200 steps, reward Average: -195.78947368421052, up to now: minReward: -122.0, minAverage: -195.2941176470588\n",
      "Episode 19 done after 111 steps, reward Average: -191.55, up to now: minReward: -111.0, minAverage: -191.55\n",
      "Episode 20 done after 114 steps, reward Average: -187.85714285714286, up to now: minReward: -111.0, minAverage: -187.85714285714286\n",
      "Episode 21 done after 116 steps, reward Average: -184.5909090909091, up to now: minReward: -111.0, minAverage: -184.5909090909091\n",
      "Episode 22 done after 200 steps, reward Average: -185.2608695652174, up to now: minReward: -111.0, minAverage: -184.5909090909091\n",
      "Episode 23 done after 119 steps, reward Average: -182.5, up to now: minReward: -111.0, minAverage: -182.5\n",
      "Episode 24 done after 114 steps, reward Average: -179.76, up to now: minReward: -111.0, minAverage: -179.76\n",
      "Episode 25 done after 111 steps, reward Average: -177.1153846153846, up to now: minReward: -111.0, minAverage: -177.1153846153846\n",
      "Episode 26 done after 110 steps, reward Average: -174.62962962962962, up to now: minReward: -110.0, minAverage: -174.62962962962962\n",
      "Episode 27 done after 119 steps, reward Average: -172.64285714285714, up to now: minReward: -110.0, minAverage: -172.64285714285714\n",
      "Episode 28 done after 110 steps, reward Average: -170.48275862068965, up to now: minReward: -110.0, minAverage: -170.48275862068965\n",
      "Episode 29 done after 111 steps, reward Average: -168.5, up to now: minReward: -110.0, minAverage: -168.5\n",
      "Episode 30 done after 108 steps, reward Average: -166.5483870967742, up to now: minReward: -108.0, minAverage: -166.5483870967742\n",
      "Episode 31 done after 200 steps, reward Average: -167.59375, up to now: minReward: -108.0, minAverage: -166.5483870967742\n",
      "Episode 32 done after 108 steps, reward Average: -165.78787878787878, up to now: minReward: -108.0, minAverage: -165.78787878787878\n",
      "Episode 33 done after 106 steps, reward Average: -164.02941176470588, up to now: minReward: -106.0, minAverage: -164.02941176470588\n",
      "Episode 34 done after 124 steps, reward Average: -162.88571428571427, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 35 done after 200 steps, reward Average: -163.91666666666666, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 36 done after 132 steps, reward Average: -163.05405405405406, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 37 done after 200 steps, reward Average: -164.02631578947367, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 38 done after 200 steps, reward Average: -164.94871794871796, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 39 done after 200 steps, reward Average: -165.825, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 40 done after 126 steps, reward Average: -164.85365853658536, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 41 done after 200 steps, reward Average: -165.6904761904762, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 42 done after 117 steps, reward Average: -164.5581395348837, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 43 done after 126 steps, reward Average: -163.6818181818182, up to now: minReward: -106.0, minAverage: -162.88571428571427\n",
      "Episode 44 done after 118 steps, reward Average: -162.66666666666666, up to now: minReward: -106.0, minAverage: -162.66666666666666\n",
      "Episode 45 done after 136 steps, reward Average: -162.08695652173913, up to now: minReward: -106.0, minAverage: -162.08695652173913\n",
      "Episode 46 done after 200 steps, reward Average: -162.89361702127658, up to now: minReward: -106.0, minAverage: -162.08695652173913\n",
      "Episode 47 done after 127 steps, reward Average: -162.14583333333334, up to now: minReward: -106.0, minAverage: -162.08695652173913\n",
      "Episode 48 done after 118 steps, reward Average: -161.24489795918367, up to now: minReward: -106.0, minAverage: -161.24489795918367\n",
      "Episode 49 done after 200 steps, reward Average: -162.02, up to now: minReward: -106.0, minAverage: -161.24489795918367\n",
      "Episode 50 done after 200 steps, reward Average: -162.76470588235293, up to now: minReward: -106.0, minAverage: -161.24489795918367\n",
      "Episode 51 done after 117 steps, reward Average: -161.8846153846154, up to now: minReward: -106.0, minAverage: -161.24489795918367\n",
      "Episode 52 done after 119 steps, reward Average: -161.0754716981132, up to now: minReward: -106.0, minAverage: -161.0754716981132\n",
      "Episode 53 done after 122 steps, reward Average: -160.35185185185185, up to now: minReward: -106.0, minAverage: -160.35185185185185\n",
      "Episode 54 done after 129 steps, reward Average: -159.78181818181818, up to now: minReward: -106.0, minAverage: -159.78181818181818\n",
      "Episode 55 done after 123 steps, reward Average: -159.125, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 56 done after 200 steps, reward Average: -159.8421052631579, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 57 done after 200 steps, reward Average: -160.5344827586207, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 58 done after 200 steps, reward Average: -161.20338983050848, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 59 done after 200 steps, reward Average: -161.85, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 60 done after 200 steps, reward Average: -162.47540983606558, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 61 done after 109 steps, reward Average: -161.61290322580646, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 62 done after 110 steps, reward Average: -160.79365079365078, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 63 done after 126 steps, reward Average: -160.25, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 64 done after 115 steps, reward Average: -159.55384615384617, up to now: minReward: -106.0, minAverage: -159.125\n",
      "Episode 65 done after 117 steps, reward Average: -158.9090909090909, up to now: minReward: -106.0, minAverage: -158.9090909090909\n",
      "Episode 66 done after 124 steps, reward Average: -158.38805970149255, up to now: minReward: -106.0, minAverage: -158.38805970149255\n",
      "Episode 67 done after 121 steps, reward Average: -157.83823529411765, up to now: minReward: -106.0, minAverage: -157.83823529411765\n",
      "Episode 68 done after 120 steps, reward Average: -157.28985507246378, up to now: minReward: -106.0, minAverage: -157.28985507246378\n",
      "Episode 69 done after 115 steps, reward Average: -156.68571428571428, up to now: minReward: -106.0, minAverage: -156.68571428571428\n",
      "Episode 70 done after 200 steps, reward Average: -157.29577464788733, up to now: minReward: -106.0, minAverage: -156.68571428571428\n",
      "Episode 71 done after 115 steps, reward Average: -156.70833333333334, up to now: minReward: -106.0, minAverage: -156.68571428571428\n",
      "Episode 72 done after 108 steps, reward Average: -156.04109589041096, up to now: minReward: -106.0, minAverage: -156.04109589041096\n",
      "Episode 73 done after 124 steps, reward Average: -155.6081081081081, up to now: minReward: -106.0, minAverage: -155.6081081081081\n",
      "Episode 74 done after 120 steps, reward Average: -155.13333333333333, up to now: minReward: -106.0, minAverage: -155.13333333333333\n",
      "Episode 75 done after 118 steps, reward Average: -154.64473684210526, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 76 done after 200 steps, reward Average: -155.23376623376623, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 77 done after 200 steps, reward Average: -155.80769230769232, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 78 done after 200 steps, reward Average: -156.36708860759492, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 79 done after 121 steps, reward Average: -155.925, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 80 done after 118 steps, reward Average: -155.45679012345678, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 81 done after 122 steps, reward Average: -155.0487804878049, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 82 done after 200 steps, reward Average: -155.59036144578315, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 83 done after 129 steps, reward Average: -155.27380952380952, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 84 done after 200 steps, reward Average: -155.8, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 85 done after 121 steps, reward Average: -155.3953488372093, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 86 done after 118 steps, reward Average: -154.9655172413793, up to now: minReward: -106.0, minAverage: -154.64473684210526\n",
      "Episode 87 done after 117 steps, reward Average: -154.5340909090909, up to now: minReward: -106.0, minAverage: -154.5340909090909\n",
      "Episode 88 done after 117 steps, reward Average: -154.1123595505618, up to now: minReward: -106.0, minAverage: -154.1123595505618\n",
      "Episode 89 done after 115 steps, reward Average: -153.67777777777778, up to now: minReward: -106.0, minAverage: -153.67777777777778\n",
      "Episode 90 done after 119 steps, reward Average: -153.2967032967033, up to now: minReward: -106.0, minAverage: -153.2967032967033\n",
      "Episode 91 done after 120 steps, reward Average: -152.93478260869566, up to now: minReward: -106.0, minAverage: -152.93478260869566\n",
      "Episode 92 done after 115 steps, reward Average: -152.5268817204301, up to now: minReward: -106.0, minAverage: -152.5268817204301\n",
      "Episode 93 done after 119 steps, reward Average: -152.17021276595744, up to now: minReward: -106.0, minAverage: -152.17021276595744\n",
      "Episode 94 done after 200 steps, reward Average: -152.67368421052632, up to now: minReward: -106.0, minAverage: -152.17021276595744\n",
      "Episode 95 done after 117 steps, reward Average: -152.30208333333334, up to now: minReward: -106.0, minAverage: -152.17021276595744\n",
      "Episode 96 done after 200 steps, reward Average: -152.79381443298968, up to now: minReward: -106.0, minAverage: -152.17021276595744\n",
      "Episode 97 done after 117 steps, reward Average: -152.42857142857142, up to now: minReward: -106.0, minAverage: -152.17021276595744\n",
      "Episode 98 done after 117 steps, reward Average: -152.07070707070707, up to now: minReward: -106.0, minAverage: -152.07070707070707\n",
      "Episode 99 done after 126 steps, reward Average: -151.81, up to now: minReward: -106.0, minAverage: -151.81\n",
      "Episode 100 done after 200 steps, reward Average: -151.81, up to now: minReward: -106.0, minAverage: -151.81\n",
      "Episode 101 done after 116 steps, reward Average: -150.97, up to now: minReward: -106.0, minAverage: -150.97\n",
      "Episode 102 done after 200 steps, reward Average: -150.97, up to now: minReward: -106.0, minAverage: -150.97\n",
      "Episode 103 done after 111 steps, reward Average: -150.08, up to now: minReward: -106.0, minAverage: -150.08\n",
      "Episode 104 done after 200 steps, reward Average: -150.08, up to now: minReward: -106.0, minAverage: -150.08\n",
      "Episode 105 done after 200 steps, reward Average: -150.08, up to now: minReward: -106.0, minAverage: -150.08\n",
      "Episode 106 done after 117 steps, reward Average: -149.25, up to now: minReward: -106.0, minAverage: -149.25\n",
      "Episode 107 done after 114 steps, reward Average: -148.39, up to now: minReward: -106.0, minAverage: -148.39\n",
      "Episode 108 done after 115 steps, reward Average: -147.54, up to now: minReward: -106.0, minAverage: -147.54\n",
      "Episode 109 done after 130 steps, reward Average: -146.84, up to now: minReward: -106.0, minAverage: -146.84\n",
      "Episode 110 done after 125 steps, reward Average: -146.09, up to now: minReward: -106.0, minAverage: -146.09\n",
      "Episode 111 done after 200 steps, reward Average: -146.09, up to now: minReward: -106.0, minAverage: -146.09\n",
      "Episode 112 done after 122 steps, reward Average: -145.31, up to now: minReward: -106.0, minAverage: -145.31\n",
      "Episode 113 done after 130 steps, reward Average: -144.61, up to now: minReward: -106.0, minAverage: -144.61\n",
      "Episode 114 done after 116 steps, reward Average: -143.77, up to now: minReward: -106.0, minAverage: -143.77\n",
      "Episode 115 done after 119 steps, reward Average: -142.98, up to now: minReward: -106.0, minAverage: -142.98\n",
      "Episode 116 done after 200 steps, reward Average: -143.76, up to now: minReward: -106.0, minAverage: -142.98\n",
      "Episode 117 done after 116 steps, reward Average: -142.92, up to now: minReward: -106.0, minAverage: -142.92\n",
      "Episode 118 done after 122 steps, reward Average: -142.14, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 119 done after 200 steps, reward Average: -143.03, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 120 done after 130 steps, reward Average: -143.19, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 121 done after 200 steps, reward Average: -144.03, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 122 done after 124 steps, reward Average: -143.27, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 123 done after 118 steps, reward Average: -143.26, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 124 done after 114 steps, reward Average: -143.26, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 125 done after 119 steps, reward Average: -143.34, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 126 done after 119 steps, reward Average: -143.43, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 127 done after 125 steps, reward Average: -143.49, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 128 done after 125 steps, reward Average: -143.64, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 129 done after 200 steps, reward Average: -144.53, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 130 done after 117 steps, reward Average: -144.62, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 131 done after 118 steps, reward Average: -143.8, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 132 done after 200 steps, reward Average: -144.72, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 133 done after 117 steps, reward Average: -144.83, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 134 done after 200 steps, reward Average: -145.59, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 135 done after 116 steps, reward Average: -144.75, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 136 done after 117 steps, reward Average: -144.6, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 137 done after 116 steps, reward Average: -143.76, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 138 done after 118 steps, reward Average: -142.94, up to now: minReward: -106.0, minAverage: -142.14\n",
      "Episode 139 done after 115 steps, reward Average: -142.09, up to now: minReward: -106.0, minAverage: -142.09\n",
      "Episode 140 done after 117 steps, reward Average: -142.0, up to now: minReward: -106.0, minAverage: -142.0\n",
      "Episode 141 done after 200 steps, reward Average: -142.0, up to now: minReward: -106.0, minAverage: -142.0\n",
      "Episode 142 done after 200 steps, reward Average: -142.83, up to now: minReward: -106.0, minAverage: -142.0\n",
      "Episode 143 done after 119 steps, reward Average: -142.76, up to now: minReward: -106.0, minAverage: -142.0\n",
      "Episode 144 done after 116 steps, reward Average: -142.74, up to now: minReward: -106.0, minAverage: -142.0\n",
      "Episode 145 done after 115 steps, reward Average: -142.53, up to now: minReward: -106.0, minAverage: -142.0\n",
      "Episode 146 done after 114 steps, reward Average: -141.67, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 147 done after 200 steps, reward Average: -142.4, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 148 done after 200 steps, reward Average: -143.22, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 149 done after 129 steps, reward Average: -142.51, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 150 done after 130 steps, reward Average: -141.81, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 151 done after 119 steps, reward Average: -141.83, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 152 done after 200 steps, reward Average: -142.64, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 153 done after 114 steps, reward Average: -142.56, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 154 done after 117 steps, reward Average: -142.44, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 155 done after 115 steps, reward Average: -142.36, up to now: minReward: -106.0, minAverage: -141.67\n",
      "Episode 156 done after 113 steps, reward Average: -141.49, up to now: minReward: -106.0, minAverage: -141.49\n",
      "Episode 157 done after 118 steps, reward Average: -140.67, up to now: minReward: -106.0, minAverage: -140.67\n",
      "Episode 158 done after 117 steps, reward Average: -139.84, up to now: minReward: -106.0, minAverage: -139.84\n",
      "Episode 159 done after 200 steps, reward Average: -139.84, up to now: minReward: -106.0, minAverage: -139.84\n",
      "Episode 160 done after 122 steps, reward Average: -139.06, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 161 done after 115 steps, reward Average: -139.12, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 162 done after 115 steps, reward Average: -139.17, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 163 done after 200 steps, reward Average: -139.91, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 164 done after 200 steps, reward Average: -140.76, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 165 done after 119 steps, reward Average: -140.78, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 166 done after 113 steps, reward Average: -140.67, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 167 done after 117 steps, reward Average: -140.63, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 168 done after 121 steps, reward Average: -140.64, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 169 done after 113 steps, reward Average: -140.62, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 170 done after 113 steps, reward Average: -139.75, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 171 done after 117 steps, reward Average: -139.77, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 172 done after 112 steps, reward Average: -139.81, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 173 done after 114 steps, reward Average: -139.71, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 174 done after 116 steps, reward Average: -139.67, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 175 done after 113 steps, reward Average: -139.62, up to now: minReward: -106.0, minAverage: -139.06\n",
      "Episode 176 done after 116 steps, reward Average: -138.78, up to now: minReward: -106.0, minAverage: -138.78\n",
      "Episode 177 done after 110 steps, reward Average: -137.88, up to now: minReward: -106.0, minAverage: -137.88\n",
      "Episode 178 done after 108 steps, reward Average: -136.96, up to now: minReward: -106.0, minAverage: -136.96\n",
      "Episode 179 done after 107 steps, reward Average: -136.82, up to now: minReward: -106.0, minAverage: -136.82\n",
      "Episode 180 done after 200 steps, reward Average: -137.64, up to now: minReward: -106.0, minAverage: -136.82\n",
      "Episode 181 done after 115 steps, reward Average: -137.57, up to now: minReward: -106.0, minAverage: -136.82\n",
      "Episode 182 done after 122 steps, reward Average: -136.79, up to now: minReward: -106.0, minAverage: -136.79\n",
      "Episode 183 done after 118 steps, reward Average: -136.68, up to now: minReward: -106.0, minAverage: -136.68\n",
      "Episode 184 done after 200 steps, reward Average: -136.68, up to now: minReward: -106.0, minAverage: -136.68\n",
      "Episode 185 done after 120 steps, reward Average: -136.67, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 186 done after 200 steps, reward Average: -137.49, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 187 done after 200 steps, reward Average: -138.32, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 188 done after 108 steps, reward Average: -138.23, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 189 done after 115 steps, reward Average: -138.23, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 190 done after 114 steps, reward Average: -138.18, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 191 done after 114 steps, reward Average: -138.12, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 192 done after 116 steps, reward Average: -138.13, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 193 done after 114 steps, reward Average: -138.08, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 194 done after 114 steps, reward Average: -137.22, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 195 done after 114 steps, reward Average: -137.19, up to now: minReward: -106.0, minAverage: -136.67\n",
      "Episode 196 done after 115 steps, reward Average: -136.34, up to now: minReward: -106.0, minAverage: -136.34\n",
      "Episode 197 done after 116 steps, reward Average: -136.33, up to now: minReward: -106.0, minAverage: -136.33\n",
      "Episode 198 done after 117 steps, reward Average: -136.33, up to now: minReward: -106.0, minAverage: -136.33\n",
      "Episode 199 done after 115 steps, reward Average: -136.22, up to now: minReward: -106.0, minAverage: -136.22\n",
      "Episode 200 done after 115 steps, reward Average: -135.37, up to now: minReward: -106.0, minAverage: -135.37\n",
      "Episode 201 done after 115 steps, reward Average: -135.36, up to now: minReward: -106.0, minAverage: -135.36\n",
      "Episode 202 done after 115 steps, reward Average: -134.51, up to now: minReward: -106.0, minAverage: -134.51\n",
      "Episode 203 done after 114 steps, reward Average: -134.54, up to now: minReward: -106.0, minAverage: -134.51\n",
      "Episode 204 done after 124 steps, reward Average: -133.78, up to now: minReward: -106.0, minAverage: -133.78\n",
      "Episode 205 done after 114 steps, reward Average: -132.92, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 206 done after 200 steps, reward Average: -133.75, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 207 done after 126 steps, reward Average: -133.87, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 208 done after 130 steps, reward Average: -134.02, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 209 done after 114 steps, reward Average: -133.86, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 210 done after 114 steps, reward Average: -133.75, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 211 done after 200 steps, reward Average: -133.75, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 212 done after 115 steps, reward Average: -133.68, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 213 done after 124 steps, reward Average: -133.62, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 214 done after 123 steps, reward Average: -133.69, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 215 done after 118 steps, reward Average: -133.68, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 216 done after 200 steps, reward Average: -133.68, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 217 done after 200 steps, reward Average: -134.52, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 218 done after 200 steps, reward Average: -135.3, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 219 done after 200 steps, reward Average: -135.3, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 220 done after 115 steps, reward Average: -135.15, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 221 done after 114 steps, reward Average: -134.29, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 222 done after 200 steps, reward Average: -135.05, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 223 done after 114 steps, reward Average: -135.01, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 224 done after 119 steps, reward Average: -135.06, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 225 done after 200 steps, reward Average: -135.87, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 226 done after 114 steps, reward Average: -135.82, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 227 done after 200 steps, reward Average: -136.57, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 228 done after 124 steps, reward Average: -136.56, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 229 done after 115 steps, reward Average: -135.71, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 230 done after 200 steps, reward Average: -136.54, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 231 done after 115 steps, reward Average: -136.51, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 232 done after 113 steps, reward Average: -135.64, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 233 done after 127 steps, reward Average: -135.74, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 234 done after 115 steps, reward Average: -134.89, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 235 done after 115 steps, reward Average: -134.88, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 236 done after 114 steps, reward Average: -134.85, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 237 done after 113 steps, reward Average: -134.82, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 238 done after 112 steps, reward Average: -134.76, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 239 done after 123 steps, reward Average: -134.84, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 240 done after 114 steps, reward Average: -134.81, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 241 done after 200 steps, reward Average: -134.81, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 242 done after 112 steps, reward Average: -133.93, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 243 done after 123 steps, reward Average: -133.97, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 244 done after 112 steps, reward Average: -133.93, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 245 done after 115 steps, reward Average: -133.93, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 246 done after 116 steps, reward Average: -133.95, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 247 done after 115 steps, reward Average: -133.1, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 248 done after 200 steps, reward Average: -133.1, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 249 done after 114 steps, reward Average: -132.95, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 250 done after 200 steps, reward Average: -133.65, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 251 done after 129 steps, reward Average: -133.75, up to now: minReward: -106.0, minAverage: -132.92\n",
      "Episode 252 done after 114 steps, reward Average: -132.89, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 253 done after 123 steps, reward Average: -132.98, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 254 done after 115 steps, reward Average: -132.96, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 255 done after 200 steps, reward Average: -133.81, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 256 done after 112 steps, reward Average: -133.8, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 257 done after 200 steps, reward Average: -134.62, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 258 done after 121 steps, reward Average: -134.66, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 259 done after 115 steps, reward Average: -133.81, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 260 done after 113 steps, reward Average: -133.72, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 261 done after 115 steps, reward Average: -133.72, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 262 done after 113 steps, reward Average: -133.7, up to now: minReward: -106.0, minAverage: -132.89\n",
      "Episode 263 done after 114 steps, reward Average: -132.84, up to now: minReward: -106.0, minAverage: -132.84\n",
      "Episode 264 done after 124 steps, reward Average: -132.08, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 265 done after 123 steps, reward Average: -132.12, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 266 done after 123 steps, reward Average: -132.22, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 267 done after 116 steps, reward Average: -132.21, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 268 done after 200 steps, reward Average: -133.0, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 269 done after 200 steps, reward Average: -133.87, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 270 done after 114 steps, reward Average: -133.88, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 271 done after 200 steps, reward Average: -134.71, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 272 done after 115 steps, reward Average: -134.74, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 273 done after 121 steps, reward Average: -134.81, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 274 done after 200 steps, reward Average: -135.65, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 275 done after 115 steps, reward Average: -135.67, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 276 done after 127 steps, reward Average: -135.78, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 277 done after 115 steps, reward Average: -135.83, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 278 done after 113 steps, reward Average: -135.88, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 279 done after 116 steps, reward Average: -135.97, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 280 done after 124 steps, reward Average: -135.21, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 281 done after 115 steps, reward Average: -135.21, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 282 done after 200 steps, reward Average: -135.99, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 283 done after 123 steps, reward Average: -136.04, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 284 done after 200 steps, reward Average: -136.04, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 285 done after 115 steps, reward Average: -135.99, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 286 done after 112 steps, reward Average: -135.11, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 287 done after 200 steps, reward Average: -135.11, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 288 done after 198 steps, reward Average: -136.01, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 289 done after 110 steps, reward Average: -135.96, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 290 done after 112 steps, reward Average: -135.94, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 291 done after 200 steps, reward Average: -136.8, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 292 done after 200 steps, reward Average: -137.64, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 293 done after 116 steps, reward Average: -137.66, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 294 done after 116 steps, reward Average: -137.68, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 295 done after 115 steps, reward Average: -137.69, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 296 done after 125 steps, reward Average: -137.79, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 297 done after 115 steps, reward Average: -137.78, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 298 done after 112 steps, reward Average: -137.73, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 299 done after 115 steps, reward Average: -137.73, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 300 done after 200 steps, reward Average: -138.58, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 301 done after 130 steps, reward Average: -138.73, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 302 done after 114 steps, reward Average: -138.72, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 303 done after 200 steps, reward Average: -139.58, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 304 done after 116 steps, reward Average: -139.5, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 305 done after 117 steps, reward Average: -139.53, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 306 done after 121 steps, reward Average: -138.74, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 307 done after 116 steps, reward Average: -138.64, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 308 done after 115 steps, reward Average: -138.49, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 309 done after 200 steps, reward Average: -139.35, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 310 done after 118 steps, reward Average: -139.39, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 311 done after 121 steps, reward Average: -138.6, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 312 done after 126 steps, reward Average: -138.71, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 313 done after 119 steps, reward Average: -138.66, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 314 done after 126 steps, reward Average: -138.69, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 315 done after 116 steps, reward Average: -138.67, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 316 done after 123 steps, reward Average: -137.9, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 317 done after 119 steps, reward Average: -137.09, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 318 done after 200 steps, reward Average: -137.09, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 319 done after 115 steps, reward Average: -136.24, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 320 done after 116 steps, reward Average: -136.25, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 321 done after 200 steps, reward Average: -137.11, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 322 done after 120 steps, reward Average: -136.31, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 323 done after 125 steps, reward Average: -136.42, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 324 done after 116 steps, reward Average: -136.39, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 325 done after 122 steps, reward Average: -135.61, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 326 done after 116 steps, reward Average: -135.63, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 327 done after 116 steps, reward Average: -134.79, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 328 done after 114 steps, reward Average: -134.69, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 329 done after 116 steps, reward Average: -134.7, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 330 done after 115 steps, reward Average: -133.85, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 331 done after 130 steps, reward Average: -134.0, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 332 done after 119 steps, reward Average: -134.06, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 333 done after 129 steps, reward Average: -134.08, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 334 done after 116 steps, reward Average: -134.09, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 335 done after 117 steps, reward Average: -134.11, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 336 done after 117 steps, reward Average: -134.14, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 337 done after 115 steps, reward Average: -134.16, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 338 done after 122 steps, reward Average: -134.26, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 339 done after 127 steps, reward Average: -134.3, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 340 done after 120 steps, reward Average: -134.36, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 341 done after 118 steps, reward Average: -133.54, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 342 done after 200 steps, reward Average: -134.42, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 343 done after 125 steps, reward Average: -134.44, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 344 done after 130 steps, reward Average: -134.62, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 345 done after 117 steps, reward Average: -134.64, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 346 done after 120 steps, reward Average: -134.68, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 347 done after 118 steps, reward Average: -134.71, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 348 done after 121 steps, reward Average: -133.92, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 349 done after 200 steps, reward Average: -134.78, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 350 done after 117 steps, reward Average: -133.95, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 351 done after 119 steps, reward Average: -133.85, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 352 done after 117 steps, reward Average: -133.88, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 353 done after 130 steps, reward Average: -133.95, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 354 done after 130 steps, reward Average: -134.1, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 355 done after 122 steps, reward Average: -133.32, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 356 done after 130 steps, reward Average: -133.5, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 357 done after 116 steps, reward Average: -132.66, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 358 done after 200 steps, reward Average: -133.45, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 359 done after 119 steps, reward Average: -133.49, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 360 done after 117 steps, reward Average: -133.53, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 361 done after 116 steps, reward Average: -133.54, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 362 done after 116 steps, reward Average: -133.57, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 363 done after 123 steps, reward Average: -133.66, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 364 done after 200 steps, reward Average: -134.42, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 365 done after 114 steps, reward Average: -134.33, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 366 done after 125 steps, reward Average: -134.35, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 367 done after 116 steps, reward Average: -134.35, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 368 done after 200 steps, reward Average: -134.35, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 369 done after 120 steps, reward Average: -133.55, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 370 done after 200 steps, reward Average: -134.41, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 371 done after 120 steps, reward Average: -133.61, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 372 done after 200 steps, reward Average: -134.46, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 373 done after 113 steps, reward Average: -134.38, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 374 done after 127 steps, reward Average: -133.65, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 375 done after 200 steps, reward Average: -134.5, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 376 done after 117 steps, reward Average: -134.4, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 377 done after 122 steps, reward Average: -134.47, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 378 done after 126 steps, reward Average: -134.6, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 379 done after 121 steps, reward Average: -134.65, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 380 done after 115 steps, reward Average: -134.56, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 381 done after 117 steps, reward Average: -134.58, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 382 done after 116 steps, reward Average: -133.74, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 383 done after 200 steps, reward Average: -134.51, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 384 done after 116 steps, reward Average: -133.67, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 385 done after 125 steps, reward Average: -133.77, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 386 done after 113 steps, reward Average: -133.78, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 387 done after 117 steps, reward Average: -132.95, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 388 done after 200 steps, reward Average: -132.97, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 389 done after 123 steps, reward Average: -133.1, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 390 done after 200 steps, reward Average: -133.98, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 391 done after 116 steps, reward Average: -133.14, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 392 done after 200 steps, reward Average: -133.14, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 393 done after 129 steps, reward Average: -133.27, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 394 done after 121 steps, reward Average: -133.32, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 395 done after 116 steps, reward Average: -133.33, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 396 done after 120 steps, reward Average: -133.28, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 397 done after 126 steps, reward Average: -133.39, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 398 done after 114 steps, reward Average: -133.41, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 399 done after 115 steps, reward Average: -133.41, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 400 done after 127 steps, reward Average: -132.68, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 401 done after 115 steps, reward Average: -132.53, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 402 done after 114 steps, reward Average: -132.53, up to now: minReward: -106.0, minAverage: -132.08\n",
      "Episode 403 done after 118 steps, reward Average: -131.71, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 404 done after 200 steps, reward Average: -132.55, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 405 done after 113 steps, reward Average: -132.51, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 406 done after 113 steps, reward Average: -132.43, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 407 done after 200 steps, reward Average: -133.27, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 408 done after 121 steps, reward Average: -133.33, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 409 done after 118 steps, reward Average: -132.51, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 410 done after 126 steps, reward Average: -132.59, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 411 done after 200 steps, reward Average: -133.38, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 412 done after 200 steps, reward Average: -134.12, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 413 done after 112 steps, reward Average: -134.05, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 414 done after 129 steps, reward Average: -134.08, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 415 done after 115 steps, reward Average: -134.07, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 416 done after 116 steps, reward Average: -134.0, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 417 done after 117 steps, reward Average: -133.98, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 418 done after 200 steps, reward Average: -133.98, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 419 done after 200 steps, reward Average: -134.83, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 420 done after 200 steps, reward Average: -135.67, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 421 done after 115 steps, reward Average: -134.82, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 422 done after 117 steps, reward Average: -134.79, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 423 done after 124 steps, reward Average: -134.78, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 424 done after 126 steps, reward Average: -134.88, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 425 done after 117 steps, reward Average: -134.83, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 426 done after 116 steps, reward Average: -134.83, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 427 done after 117 steps, reward Average: -134.84, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 428 done after 123 steps, reward Average: -134.93, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 429 done after 200 steps, reward Average: -135.77, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 430 done after 125 steps, reward Average: -135.87, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 431 done after 121 steps, reward Average: -135.78, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 432 done after 114 steps, reward Average: -135.73, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 433 done after 200 steps, reward Average: -136.44, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 434 done after 125 steps, reward Average: -136.53, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 435 done after 121 steps, reward Average: -136.57, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 436 done after 200 steps, reward Average: -137.4, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 437 done after 113 steps, reward Average: -137.38, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 438 done after 115 steps, reward Average: -137.31, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 439 done after 116 steps, reward Average: -137.2, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 440 done after 200 steps, reward Average: -138.0, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 441 done after 114 steps, reward Average: -137.96, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 442 done after 200 steps, reward Average: -137.96, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 443 done after 115 steps, reward Average: -137.86, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 444 done after 115 steps, reward Average: -137.71, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 445 done after 117 steps, reward Average: -137.71, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 446 done after 126 steps, reward Average: -137.77, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 447 done after 116 steps, reward Average: -137.75, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 448 done after 115 steps, reward Average: -137.69, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 449 done after 121 steps, reward Average: -136.9, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 450 done after 116 steps, reward Average: -136.89, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 451 done after 116 steps, reward Average: -136.86, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 452 done after 130 steps, reward Average: -136.99, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 453 done after 121 steps, reward Average: -136.9, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 454 done after 130 steps, reward Average: -136.9, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 455 done after 116 steps, reward Average: -136.84, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 456 done after 114 steps, reward Average: -136.68, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 457 done after 126 steps, reward Average: -136.78, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 458 done after 116 steps, reward Average: -135.94, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 459 done after 113 steps, reward Average: -135.88, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 460 done after 120 steps, reward Average: -135.91, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 461 done after 113 steps, reward Average: -135.88, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 462 done after 115 steps, reward Average: -135.87, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 463 done after 115 steps, reward Average: -135.79, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 464 done after 115 steps, reward Average: -134.94, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 465 done after 200 steps, reward Average: -135.8, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 466 done after 200 steps, reward Average: -136.55, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 467 done after 125 steps, reward Average: -136.64, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 468 done after 200 steps, reward Average: -136.64, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 469 done after 120 steps, reward Average: -136.64, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 470 done after 120 steps, reward Average: -135.84, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 471 done after 117 steps, reward Average: -135.81, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 472 done after 121 steps, reward Average: -135.02, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 473 done after 115 steps, reward Average: -135.04, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 474 done after 117 steps, reward Average: -134.94, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 475 done after 117 steps, reward Average: -134.11, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 476 done after 117 steps, reward Average: -134.11, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 477 done after 117 steps, reward Average: -134.06, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 478 done after 121 steps, reward Average: -134.01, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 479 done after 122 steps, reward Average: -134.02, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 480 done after 117 steps, reward Average: -134.04, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 481 done after 114 steps, reward Average: -134.01, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 482 done after 119 steps, reward Average: -134.04, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 483 done after 115 steps, reward Average: -133.19, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 484 done after 116 steps, reward Average: -133.19, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 485 done after 117 steps, reward Average: -133.11, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 486 done after 117 steps, reward Average: -133.15, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 487 done after 124 steps, reward Average: -133.22, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 488 done after 133 steps, reward Average: -132.55, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 489 done after 121 steps, reward Average: -132.53, up to now: minReward: -106.0, minAverage: -131.71\n",
      "Episode 490 done after 115 steps, reward Average: -131.68, up to now: minReward: -106.0, minAverage: -131.68\n",
      "Episode 491 done after 117 steps, reward Average: -131.69, up to now: minReward: -106.0, minAverage: -131.68\n",
      "Episode 492 done after 130 steps, reward Average: -130.99, up to now: minReward: -106.0, minAverage: -130.99\n",
      "Episode 493 done after 119 steps, reward Average: -130.89, up to now: minReward: -106.0, minAverage: -130.89\n",
      "Episode 494 done after 115 steps, reward Average: -130.83, up to now: minReward: -106.0, minAverage: -130.83\n",
      "Episode 495 done after 124 steps, reward Average: -130.91, up to now: minReward: -106.0, minAverage: -130.83\n",
      "Episode 496 done after 118 steps, reward Average: -130.89, up to now: minReward: -106.0, minAverage: -130.83\n",
      "Episode 497 done after 117 steps, reward Average: -130.8, up to now: minReward: -106.0, minAverage: -130.8\n",
      "Episode 498 done after 114 steps, reward Average: -130.8, up to now: minReward: -106.0, minAverage: -130.8\n",
      "Episode 499 done after 116 steps, reward Average: -130.81, up to now: minReward: -106.0, minAverage: -130.8\n",
      "final result: \n",
      "381 times arrived in 500 episodes, first time in episode 15\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhagAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27adbdc5550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 25:\n",
    "            return 0.65\n",
    "        elif episode < 35:\n",
    "            return 0.6\n",
    "        elif episode < 45:\n",
    "            return 0.55\n",
    "        elif episode < 55:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.4\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
