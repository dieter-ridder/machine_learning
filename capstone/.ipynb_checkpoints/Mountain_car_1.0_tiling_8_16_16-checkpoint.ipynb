{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Try QLearning - 300 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.2)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 20:09:33,722] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.2\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=False\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 160 steps, reward Average: -197.33333333333334, up to now: minReward: -160.0, minAverage: -197.33333333333334\n",
      "Episode 15 done after 200 steps, reward Average: -197.5, up to now: minReward: -160.0, minAverage: -197.33333333333334\n",
      "Episode 16 done after 174 steps, reward Average: -196.11764705882354, up to now: minReward: -160.0, minAverage: -196.11764705882354\n",
      "Episode 17 done after 168 steps, reward Average: -194.55555555555554, up to now: minReward: -160.0, minAverage: -194.55555555555554\n",
      "Episode 18 done after 200 steps, reward Average: -194.8421052631579, up to now: minReward: -160.0, minAverage: -194.55555555555554\n",
      "Episode 19 done after 160 steps, reward Average: -193.1, up to now: minReward: -160.0, minAverage: -193.1\n",
      "Episode 20 done after 156 steps, reward Average: -191.33333333333334, up to now: minReward: -156.0, minAverage: -191.33333333333334\n",
      "Episode 21 done after 156 steps, reward Average: -189.72727272727272, up to now: minReward: -156.0, minAverage: -189.72727272727272\n",
      "Episode 22 done after 162 steps, reward Average: -188.52173913043478, up to now: minReward: -156.0, minAverage: -188.52173913043478\n",
      "Episode 23 done after 158 steps, reward Average: -187.25, up to now: minReward: -156.0, minAverage: -187.25\n",
      "Episode 24 done after 154 steps, reward Average: -185.92, up to now: minReward: -154.0, minAverage: -185.92\n",
      "Episode 25 done after 151 steps, reward Average: -184.57692307692307, up to now: minReward: -151.0, minAverage: -184.57692307692307\n",
      "Episode 26 done after 153 steps, reward Average: -183.40740740740742, up to now: minReward: -151.0, minAverage: -183.40740740740742\n",
      "Episode 27 done after 99 steps, reward Average: -180.39285714285714, up to now: minReward: -99.0, minAverage: -180.39285714285714\n",
      "Episode 28 done after 169 steps, reward Average: -180.0, up to now: minReward: -99.0, minAverage: -180.0\n",
      "Episode 29 done after 95 steps, reward Average: -177.16666666666666, up to now: minReward: -95.0, minAverage: -177.16666666666666\n",
      "Episode 30 done after 162 steps, reward Average: -176.67741935483872, up to now: minReward: -95.0, minAverage: -176.67741935483872\n",
      "Episode 31 done after 156 steps, reward Average: -176.03125, up to now: minReward: -95.0, minAverage: -176.03125\n",
      "Episode 32 done after 149 steps, reward Average: -175.21212121212122, up to now: minReward: -95.0, minAverage: -175.21212121212122\n",
      "Episode 33 done after 145 steps, reward Average: -174.3235294117647, up to now: minReward: -95.0, minAverage: -174.3235294117647\n",
      "Episode 34 done after 200 steps, reward Average: -175.05714285714285, up to now: minReward: -95.0, minAverage: -174.3235294117647\n",
      "Episode 35 done after 146 steps, reward Average: -174.25, up to now: minReward: -95.0, minAverage: -174.25\n",
      "Episode 36 done after 146 steps, reward Average: -173.48648648648648, up to now: minReward: -95.0, minAverage: -173.48648648648648\n",
      "Episode 37 done after 165 steps, reward Average: -173.26315789473685, up to now: minReward: -95.0, minAverage: -173.26315789473685\n",
      "Episode 38 done after 146 steps, reward Average: -172.56410256410257, up to now: minReward: -95.0, minAverage: -172.56410256410257\n",
      "Episode 39 done after 149 steps, reward Average: -171.975, up to now: minReward: -95.0, minAverage: -171.975\n",
      "Episode 40 done after 145 steps, reward Average: -171.3170731707317, up to now: minReward: -95.0, minAverage: -171.3170731707317\n",
      "Episode 41 done after 101 steps, reward Average: -169.64285714285714, up to now: minReward: -95.0, minAverage: -169.64285714285714\n",
      "Episode 42 done after 148 steps, reward Average: -169.13953488372093, up to now: minReward: -95.0, minAverage: -169.13953488372093\n",
      "Episode 43 done after 149 steps, reward Average: -168.6818181818182, up to now: minReward: -95.0, minAverage: -168.6818181818182\n",
      "Episode 44 done after 146 steps, reward Average: -168.17777777777778, up to now: minReward: -95.0, minAverage: -168.17777777777778\n",
      "Episode 45 done after 148 steps, reward Average: -167.7391304347826, up to now: minReward: -95.0, minAverage: -167.7391304347826\n",
      "Episode 46 done after 144 steps, reward Average: -167.2340425531915, up to now: minReward: -95.0, minAverage: -167.2340425531915\n",
      "Episode 47 done after 147 steps, reward Average: -166.8125, up to now: minReward: -95.0, minAverage: -166.8125\n",
      "Episode 48 done after 146 steps, reward Average: -166.3877551020408, up to now: minReward: -95.0, minAverage: -166.3877551020408\n",
      "Episode 49 done after 147 steps, reward Average: -166.0, up to now: minReward: -95.0, minAverage: -166.0\n",
      "Episode 50 done after 145 steps, reward Average: -165.58823529411765, up to now: minReward: -95.0, minAverage: -165.58823529411765\n",
      "Episode 51 done after 148 steps, reward Average: -165.25, up to now: minReward: -95.0, minAverage: -165.25\n",
      "Episode 52 done after 143 steps, reward Average: -164.83018867924528, up to now: minReward: -95.0, minAverage: -164.83018867924528\n",
      "Episode 53 done after 147 steps, reward Average: -164.5, up to now: minReward: -95.0, minAverage: -164.5\n",
      "Episode 54 done after 149 steps, reward Average: -164.21818181818182, up to now: minReward: -95.0, minAverage: -164.21818181818182\n",
      "Episode 55 done after 157 steps, reward Average: -164.08928571428572, up to now: minReward: -95.0, minAverage: -164.08928571428572\n",
      "Episode 56 done after 173 steps, reward Average: -164.24561403508773, up to now: minReward: -95.0, minAverage: -164.08928571428572\n",
      "Episode 57 done after 146 steps, reward Average: -163.93103448275863, up to now: minReward: -95.0, minAverage: -163.93103448275863\n",
      "Episode 58 done after 143 steps, reward Average: -163.57627118644066, up to now: minReward: -95.0, minAverage: -163.57627118644066\n",
      "Episode 59 done after 142 steps, reward Average: -163.21666666666667, up to now: minReward: -95.0, minAverage: -163.21666666666667\n",
      "Episode 60 done after 108 steps, reward Average: -162.31147540983608, up to now: minReward: -95.0, minAverage: -162.31147540983608\n",
      "Episode 61 done after 143 steps, reward Average: -162.0, up to now: minReward: -95.0, minAverage: -162.0\n",
      "Episode 62 done after 106 steps, reward Average: -161.11111111111111, up to now: minReward: -95.0, minAverage: -161.11111111111111\n",
      "Episode 63 done after 157 steps, reward Average: -161.046875, up to now: minReward: -95.0, minAverage: -161.046875\n",
      "Episode 64 done after 172 steps, reward Average: -161.2153846153846, up to now: minReward: -95.0, minAverage: -161.046875\n",
      "Episode 65 done after 197 steps, reward Average: -161.75757575757575, up to now: minReward: -95.0, minAverage: -161.046875\n",
      "Episode 66 done after 145 steps, reward Average: -161.50746268656715, up to now: minReward: -95.0, minAverage: -161.046875\n",
      "Episode 67 done after 143 steps, reward Average: -161.23529411764707, up to now: minReward: -95.0, minAverage: -161.046875\n",
      "Episode 68 done after 143 steps, reward Average: -160.97101449275362, up to now: minReward: -95.0, minAverage: -160.97101449275362\n",
      "Episode 69 done after 146 steps, reward Average: -160.75714285714287, up to now: minReward: -95.0, minAverage: -160.75714285714287\n",
      "Episode 70 done after 143 steps, reward Average: -160.50704225352112, up to now: minReward: -95.0, minAverage: -160.50704225352112\n",
      "Episode 71 done after 143 steps, reward Average: -160.26388888888889, up to now: minReward: -95.0, minAverage: -160.26388888888889\n",
      "Episode 72 done after 149 steps, reward Average: -160.1095890410959, up to now: minReward: -95.0, minAverage: -160.1095890410959\n",
      "Episode 73 done after 149 steps, reward Average: -159.95945945945945, up to now: minReward: -95.0, minAverage: -159.95945945945945\n",
      "Episode 74 done after 160 steps, reward Average: -159.96, up to now: minReward: -95.0, minAverage: -159.95945945945945\n",
      "Episode 75 done after 143 steps, reward Average: -159.73684210526315, up to now: minReward: -95.0, minAverage: -159.73684210526315\n",
      "Episode 76 done after 143 steps, reward Average: -159.5194805194805, up to now: minReward: -95.0, minAverage: -159.5194805194805\n",
      "Episode 77 done after 147 steps, reward Average: -159.35897435897436, up to now: minReward: -95.0, minAverage: -159.35897435897436\n",
      "Episode 78 done after 106 steps, reward Average: -158.68354430379748, up to now: minReward: -95.0, minAverage: -158.68354430379748\n",
      "Episode 79 done after 144 steps, reward Average: -158.5, up to now: minReward: -95.0, minAverage: -158.5\n",
      "Episode 80 done after 144 steps, reward Average: -158.320987654321, up to now: minReward: -95.0, minAverage: -158.320987654321\n",
      "Episode 81 done after 143 steps, reward Average: -158.1341463414634, up to now: minReward: -95.0, minAverage: -158.1341463414634\n",
      "Episode 82 done after 144 steps, reward Average: -157.96385542168676, up to now: minReward: -95.0, minAverage: -157.96385542168676\n",
      "Episode 83 done after 145 steps, reward Average: -157.8095238095238, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 84 done after 186 steps, reward Average: -158.14117647058825, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 85 done after 159 steps, reward Average: -158.15116279069767, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 86 done after 159 steps, reward Average: -158.16091954022988, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 87 done after 143 steps, reward Average: -157.98863636363637, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 88 done after 172 steps, reward Average: -158.14606741573033, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 89 done after 152 steps, reward Average: -158.07777777777778, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 90 done after 165 steps, reward Average: -158.15384615384616, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 91 done after 177 steps, reward Average: -158.3586956521739, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 92 done after 191 steps, reward Average: -158.70967741935485, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 93 done after 145 steps, reward Average: -158.56382978723406, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 94 done after 143 steps, reward Average: -158.4, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 95 done after 164 steps, reward Average: -158.45833333333334, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 96 done after 145 steps, reward Average: -158.31958762886597, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 97 done after 143 steps, reward Average: -158.16326530612244, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 98 done after 169 steps, reward Average: -158.27272727272728, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 99 done after 143 steps, reward Average: -158.12, up to now: minReward: -95.0, minAverage: -157.8095238095238\n",
      "Episode 100 done after 145 steps, reward Average: -157.57, up to now: minReward: -95.0, minAverage: -157.57\n",
      "Episode 101 done after 165 steps, reward Average: -157.22, up to now: minReward: -95.0, minAverage: -157.22\n",
      "Episode 102 done after 143 steps, reward Average: -156.65, up to now: minReward: -95.0, minAverage: -156.65\n",
      "Episode 103 done after 147 steps, reward Average: -156.12, up to now: minReward: -95.0, minAverage: -156.12\n",
      "Episode 104 done after 157 steps, reward Average: -155.69, up to now: minReward: -95.0, minAverage: -155.69\n",
      "Episode 105 done after 161 steps, reward Average: -155.3, up to now: minReward: -95.0, minAverage: -155.3\n",
      "Episode 106 done after 158 steps, reward Average: -154.88, up to now: minReward: -95.0, minAverage: -154.88\n",
      "Episode 107 done after 143 steps, reward Average: -154.31, up to now: minReward: -95.0, minAverage: -154.31\n",
      "Episode 108 done after 146 steps, reward Average: -153.77, up to now: minReward: -95.0, minAverage: -153.77\n",
      "Episode 109 done after 164 steps, reward Average: -153.41, up to now: minReward: -95.0, minAverage: -153.41\n",
      "Episode 110 done after 147 steps, reward Average: -152.88, up to now: minReward: -95.0, minAverage: -152.88\n",
      "Episode 111 done after 146 steps, reward Average: -152.34, up to now: minReward: -95.0, minAverage: -152.34\n",
      "Episode 112 done after 143 steps, reward Average: -151.77, up to now: minReward: -95.0, minAverage: -151.77\n",
      "Episode 113 done after 160 steps, reward Average: -151.37, up to now: minReward: -95.0, minAverage: -151.37\n",
      "Episode 114 done after 147 steps, reward Average: -151.24, up to now: minReward: -95.0, minAverage: -151.24\n",
      "Episode 115 done after 144 steps, reward Average: -150.68, up to now: minReward: -95.0, minAverage: -150.68\n",
      "Episode 116 done after 144 steps, reward Average: -150.38, up to now: minReward: -95.0, minAverage: -150.38\n",
      "Episode 117 done after 143 steps, reward Average: -150.13, up to now: minReward: -95.0, minAverage: -150.13\n",
      "Episode 118 done after 145 steps, reward Average: -149.58, up to now: minReward: -95.0, minAverage: -149.58\n",
      "Episode 119 done after 144 steps, reward Average: -149.42, up to now: minReward: -95.0, minAverage: -149.42\n",
      "Episode 120 done after 144 steps, reward Average: -149.3, up to now: minReward: -95.0, minAverage: -149.3\n",
      "Episode 121 done after 143 steps, reward Average: -149.17, up to now: minReward: -95.0, minAverage: -149.17\n",
      "Episode 122 done after 166 steps, reward Average: -149.21, up to now: minReward: -95.0, minAverage: -149.17\n",
      "Episode 123 done after 143 steps, reward Average: -149.06, up to now: minReward: -95.0, minAverage: -149.06\n",
      "Episode 124 done after 150 steps, reward Average: -149.02, up to now: minReward: -95.0, minAverage: -149.02\n",
      "Episode 125 done after 145 steps, reward Average: -148.96, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 126 done after 188 steps, reward Average: -149.31, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 127 done after 146 steps, reward Average: -149.78, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 128 done after 143 steps, reward Average: -149.52, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 129 done after 143 steps, reward Average: -150.0, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 130 done after 143 steps, reward Average: -149.81, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 131 done after 143 steps, reward Average: -149.68, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 132 done after 148 steps, reward Average: -149.67, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 133 done after 144 steps, reward Average: -149.66, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 134 done after 146 steps, reward Average: -149.12, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 135 done after 143 steps, reward Average: -149.09, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 136 done after 143 steps, reward Average: -149.06, up to now: minReward: -95.0, minAverage: -148.96\n",
      "Episode 137 done after 144 steps, reward Average: -148.85, up to now: minReward: -95.0, minAverage: -148.85\n",
      "Episode 138 done after 145 steps, reward Average: -148.84, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 139 done after 160 steps, reward Average: -148.95, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 140 done after 154 steps, reward Average: -149.04, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 141 done after 143 steps, reward Average: -149.46, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 142 done after 146 steps, reward Average: -149.44, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 143 done after 158 steps, reward Average: -149.53, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 144 done after 154 steps, reward Average: -149.61, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 145 done after 148 steps, reward Average: -149.61, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 146 done after 143 steps, reward Average: -149.6, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 147 done after 148 steps, reward Average: -149.61, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 148 done after 143 steps, reward Average: -149.58, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 149 done after 185 steps, reward Average: -149.96, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 150 done after 147 steps, reward Average: -149.98, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 151 done after 121 steps, reward Average: -149.71, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 152 done after 145 steps, reward Average: -149.73, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 153 done after 143 steps, reward Average: -149.69, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 154 done after 161 steps, reward Average: -149.81, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 155 done after 149 steps, reward Average: -149.73, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 156 done after 152 steps, reward Average: -149.52, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 157 done after 166 steps, reward Average: -149.72, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 158 done after 151 steps, reward Average: -149.8, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 159 done after 149 steps, reward Average: -149.87, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 160 done after 161 steps, reward Average: -150.4, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 161 done after 124 steps, reward Average: -150.21, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 162 done after 121 steps, reward Average: -150.36, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 163 done after 153 steps, reward Average: -150.32, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 164 done after 144 steps, reward Average: -150.04, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 165 done after 150 steps, reward Average: -149.57, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 166 done after 143 steps, reward Average: -149.55, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 167 done after 144 steps, reward Average: -149.56, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 168 done after 146 steps, reward Average: -149.59, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 169 done after 143 steps, reward Average: -149.56, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 170 done after 149 steps, reward Average: -149.62, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 171 done after 154 steps, reward Average: -149.73, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 172 done after 144 steps, reward Average: -149.68, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 173 done after 144 steps, reward Average: -149.63, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 174 done after 144 steps, reward Average: -149.47, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 175 done after 145 steps, reward Average: -149.49, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 176 done after 143 steps, reward Average: -149.49, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 177 done after 143 steps, reward Average: -149.45, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 178 done after 152 steps, reward Average: -149.91, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 179 done after 170 steps, reward Average: -150.17, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 180 done after 146 steps, reward Average: -150.19, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 181 done after 166 steps, reward Average: -150.42, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 182 done after 143 steps, reward Average: -150.41, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 183 done after 143 steps, reward Average: -150.39, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 184 done after 158 steps, reward Average: -150.11, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 185 done after 148 steps, reward Average: -150.0, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 186 done after 143 steps, reward Average: -149.84, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 187 done after 144 steps, reward Average: -149.85, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 188 done after 143 steps, reward Average: -149.56, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 189 done after 144 steps, reward Average: -149.48, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 190 done after 143 steps, reward Average: -149.26, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 191 done after 144 steps, reward Average: -148.93, up to now: minReward: -95.0, minAverage: -148.84\n",
      "Episode 192 done after 146 steps, reward Average: -148.48, up to now: minReward: -95.0, minAverage: -148.48\n",
      "Episode 193 done after 148 steps, reward Average: -148.51, up to now: minReward: -95.0, minAverage: -148.48\n",
      "Episode 194 done after 161 steps, reward Average: -148.69, up to now: minReward: -95.0, minAverage: -148.48\n",
      "Episode 195 done after 148 steps, reward Average: -148.53, up to now: minReward: -95.0, minAverage: -148.48\n",
      "Episode 196 done after 145 steps, reward Average: -148.53, up to now: minReward: -95.0, minAverage: -148.48\n",
      "Episode 197 done after 159 steps, reward Average: -148.69, up to now: minReward: -95.0, minAverage: -148.48\n",
      "Episode 198 done after 147 steps, reward Average: -148.47, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 199 done after 154 steps, reward Average: -148.58, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 200 done after 160 steps, reward Average: -148.73, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 201 done after 143 steps, reward Average: -148.51, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 202 done after 157 steps, reward Average: -148.65, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 203 done after 154 steps, reward Average: -148.72, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 204 done after 200 steps, reward Average: -149.15, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 205 done after 166 steps, reward Average: -149.2, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 206 done after 159 steps, reward Average: -149.21, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 207 done after 143 steps, reward Average: -149.21, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 208 done after 159 steps, reward Average: -149.34, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 209 done after 156 steps, reward Average: -149.26, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 210 done after 162 steps, reward Average: -149.41, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 211 done after 144 steps, reward Average: -149.39, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 212 done after 145 steps, reward Average: -149.41, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 213 done after 159 steps, reward Average: -149.4, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 214 done after 143 steps, reward Average: -149.36, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 215 done after 156 steps, reward Average: -149.48, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 216 done after 143 steps, reward Average: -149.47, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 217 done after 154 steps, reward Average: -149.58, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 218 done after 143 steps, reward Average: -149.56, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 219 done after 144 steps, reward Average: -149.56, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 220 done after 114 steps, reward Average: -149.26, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 221 done after 150 steps, reward Average: -149.33, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 222 done after 165 steps, reward Average: -149.32, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 223 done after 147 steps, reward Average: -149.36, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 224 done after 144 steps, reward Average: -149.3, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 225 done after 143 steps, reward Average: -149.28, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 226 done after 144 steps, reward Average: -148.84, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 227 done after 144 steps, reward Average: -148.82, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 228 done after 143 steps, reward Average: -148.82, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 229 done after 171 steps, reward Average: -149.1, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 230 done after 143 steps, reward Average: -149.1, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 231 done after 144 steps, reward Average: -149.11, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 232 done after 144 steps, reward Average: -149.07, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 233 done after 144 steps, reward Average: -149.07, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 234 done after 144 steps, reward Average: -149.05, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 235 done after 144 steps, reward Average: -149.06, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 236 done after 145 steps, reward Average: -149.08, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 237 done after 143 steps, reward Average: -149.07, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 238 done after 158 steps, reward Average: -149.2, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 239 done after 143 steps, reward Average: -149.03, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 240 done after 171 steps, reward Average: -149.2, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 241 done after 143 steps, reward Average: -149.2, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 242 done after 161 steps, reward Average: -149.35, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 243 done after 176 steps, reward Average: -149.53, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 244 done after 175 steps, reward Average: -149.74, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 245 done after 175 steps, reward Average: -150.01, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 246 done after 144 steps, reward Average: -150.02, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 247 done after 159 steps, reward Average: -150.13, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 248 done after 144 steps, reward Average: -150.14, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 249 done after 154 steps, reward Average: -149.83, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 250 done after 156 steps, reward Average: -149.92, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 251 done after 143 steps, reward Average: -150.14, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 252 done after 162 steps, reward Average: -150.31, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 253 done after 144 steps, reward Average: -150.32, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 254 done after 147 steps, reward Average: -150.18, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 255 done after 143 steps, reward Average: -150.12, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 256 done after 146 steps, reward Average: -150.06, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 257 done after 157 steps, reward Average: -149.97, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 258 done after 150 steps, reward Average: -149.96, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 259 done after 144 steps, reward Average: -149.91, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 260 done after 155 steps, reward Average: -149.85, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 261 done after 150 steps, reward Average: -150.11, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 262 done after 143 steps, reward Average: -150.33, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 263 done after 144 steps, reward Average: -150.24, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 264 done after 150 steps, reward Average: -150.3, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 265 done after 144 steps, reward Average: -150.24, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 266 done after 143 steps, reward Average: -150.24, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 267 done after 143 steps, reward Average: -150.23, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 268 done after 144 steps, reward Average: -150.21, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 269 done after 147 steps, reward Average: -150.25, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 270 done after 143 steps, reward Average: -150.19, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 271 done after 143 steps, reward Average: -150.08, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 272 done after 150 steps, reward Average: -150.14, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 273 done after 149 steps, reward Average: -150.19, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 274 done after 144 steps, reward Average: -150.19, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 275 done after 143 steps, reward Average: -150.17, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 276 done after 148 steps, reward Average: -150.22, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 277 done after 159 steps, reward Average: -150.38, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 278 done after 143 steps, reward Average: -150.29, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 279 done after 145 steps, reward Average: -150.04, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 280 done after 143 steps, reward Average: -150.01, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 281 done after 143 steps, reward Average: -149.78, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 282 done after 143 steps, reward Average: -149.78, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 283 done after 143 steps, reward Average: -149.78, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 284 done after 167 steps, reward Average: -149.87, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 285 done after 146 steps, reward Average: -149.85, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 286 done after 154 steps, reward Average: -149.96, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 287 done after 155 steps, reward Average: -150.07, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 288 done after 156 steps, reward Average: -150.2, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 289 done after 143 steps, reward Average: -150.19, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 290 done after 144 steps, reward Average: -150.2, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 291 done after 176 steps, reward Average: -150.52, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 292 done after 147 steps, reward Average: -150.53, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 293 done after 155 steps, reward Average: -150.6, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 294 done after 150 steps, reward Average: -150.49, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 295 done after 154 steps, reward Average: -150.55, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 296 done after 142 steps, reward Average: -150.52, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 297 done after 158 steps, reward Average: -150.51, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 298 done after 146 steps, reward Average: -150.5, up to now: minReward: -95.0, minAverage: -148.47\n",
      "Episode 299 done after 128 steps, reward Average: -150.24, up to now: minReward: -95.0, minAverage: -148.47\n",
      "final result: \n",
      "282 times arrived in 300 episodes, first time in episode 14\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22fe40fd2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) +  (R +  Q(S, A)  Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
