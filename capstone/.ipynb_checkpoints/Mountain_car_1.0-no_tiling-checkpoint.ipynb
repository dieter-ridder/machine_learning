{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile Model: #\n",
    "\n",
    "In this class there can be found both the tile coding, as well as the state model to learn.\n",
    "\n",
    "Tile coding is implemented straight forward:\n",
    "The observation, in out case position and velocity, is first shifted to avoid negative coordinated, and later scaled to the grid size. \n",
    "\n",
    "If we expect a grid of 8*8, after scaling and shifting, the range of the input is between [0,8] in both dimensions.\n",
    "\n",
    "For tiling the grids of each tiling level are shifted to its neighbour by 1/numberOfTilings. Before being copied into the coordinates grid (tiling\\*gridsize\\*gridsize)the observation is shifted accordingly and casted to an int.\n",
    "\n",
    "Now we have a lot of discrete points in the space (tiling\\*gridsize\\*gridsize). In a second step this 3D address of a point is mapped to point to a 1D-array to store the states. Mapping is done straigthforward: \n",
    "\n",
    "Tiling 0: mapped to 0...63\n",
    "Tiling 1: mapped to 64 ... 127 and so on.\n",
    "\n",
    "Inside one tiling level it is simiiar:\n",
    "(position,velocity) is mapped to position+8*velocity.\n",
    "\n",
    "As result we get an index vector, containing the index for each tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-30 13:17:32,779] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with tile coding\n",
    "To get some \"inituitive feeling\" about the value add of the tile coding,  the car starts on the left hill, and chooses the action randomly. \n",
    "\n",
    "What we see in the figures showing the discretisation that having several levels of discretisation shifted between each other keeps a bit more information than just increasing the granularity of the discretisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tiles_1_32 = tileModel(1,32)                    #grid: 32*32, 1 tilings\n",
    "tiles_4_16 = tileModel(4,16)                    #grid: 16*16, 4 tilings\n",
    "tiles_8_8  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "###tiles.displayM()\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    position = []\n",
    "    velocity = []\n",
    "    tiling1=np.empty(shape=[nbTimesteps,2,1],dtype=int)\n",
    "    tiling4=np.empty(shape=[nbTimesteps,2,4],dtype=int)\n",
    "    tiling8=np.empty(shape=[nbTimesteps,2,8],dtype=int)\n",
    "    finalRepetition=0\n",
    "\n",
    "    obs = env.reset()\n",
    "    #env.env.state =[-0.5, 0.0]\n",
    "    env.env.state =[-1.174194984, 0.001]\n",
    "    obs=env.env.state\n",
    "    for t in range(nbTimesteps):\n",
    "        i=1\n",
    "        env.render()\n",
    "        coord_1_32,tileIdx_1_32=tiles_1_32.code(obs)        \n",
    "        coord_4_16,tileIdx_4_16=tiles_4_16.code(obs)\n",
    "        coord_8_8,tileIdx_8_8=tiles_8_8.code(obs)\n",
    "        \n",
    "        #for plotting\n",
    "        position.append(obs[0])\n",
    "        velocity.append(obs[1])\n",
    "        tiling1[t]=coord_1_32.T\n",
    "        tiling4[t]=coord_4_16.T\n",
    "        tiling8[t]=coord_8_8.T\n",
    "        finalRepetition=t\n",
    "        \n",
    "        action = env.action_space.sample()\n",
    "        #action = 2\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    \n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "###import matplotlib as mpl\n",
    "###print('tiling1',tiling1)\n",
    "###print('tiling4',tiling4)\n",
    "###print('tiling8',tiling8)\n",
    "    \n",
    "### plot it..\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(position, velocity, 'b-')\n",
    "ax1.set_xlabel('position')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('velocity')\n",
    "ax1.set_title('original observations')\n",
    "plt.show()\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "    \n",
    "ax1.plot(tiling1[:finalRepetition,0,0],tiling1[:finalRepetition,1,0])\n",
    "    \n",
    "\n",
    "ax1.set_xlabel('position')\n",
    "ax1.set_ylabel('velocity')\n",
    "ax1.set_title('nb_Tilings: 1')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "for i in range(4):\n",
    "    ax1.plot([i]*finalRepetition,tiling4[:finalRepetition,0,i],tiling4[:finalRepetition,1,i])\n",
    "    \n",
    "\n",
    "ax1.set_xlabel('tiling')\n",
    "ax1.set_ylabel('position')\n",
    "ax1.set_zlabel('velocity')\n",
    "ax1.set_title('nb_Tilings: 4')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "for i in range(8):\n",
    "    ax1.plot([i]*finalRepetition,tiling8[:finalRepetition,0,i],tiling8[:finalRepetition,1,i])\n",
    "    \n",
    "\n",
    "ax1.set_xlabel('tiling')\n",
    "ax1.set_ylabel('position')\n",
    "ax1.set_zlabel('velocity')\n",
    "ax1.set_title('nb_Tilings: 8')\n",
    "plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Q(s,a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.1\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 18 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 19 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 20 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 21 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 22 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 23 done after 162 steps, reward Average: -198.41666666666666, up to now: minReward: -162.0, minAverage: -198.41666666666666\n",
      "Episode 24 done after 200 steps, reward Average: -198.48, up to now: minReward: -162.0, minAverage: -198.41666666666666\n",
      "Episode 25 done after 169 steps, reward Average: -197.34615384615384, up to now: minReward: -162.0, minAverage: -197.34615384615384\n",
      "Episode 26 done after 200 steps, reward Average: -197.44444444444446, up to now: minReward: -162.0, minAverage: -197.34615384615384\n",
      "Episode 27 done after 200 steps, reward Average: -197.53571428571428, up to now: minReward: -162.0, minAverage: -197.34615384615384\n",
      "Episode 28 done after 166 steps, reward Average: -196.44827586206895, up to now: minReward: -162.0, minAverage: -196.44827586206895\n",
      "Episode 29 done after 200 steps, reward Average: -196.56666666666666, up to now: minReward: -162.0, minAverage: -196.44827586206895\n",
      "Episode 30 done after 200 steps, reward Average: -196.67741935483872, up to now: minReward: -162.0, minAverage: -196.44827586206895\n",
      "Episode 31 done after 200 steps, reward Average: -196.78125, up to now: minReward: -162.0, minAverage: -196.44827586206895\n",
      "Episode 32 done after 164 steps, reward Average: -195.78787878787878, up to now: minReward: -162.0, minAverage: -195.78787878787878\n",
      "Episode 33 done after 166 steps, reward Average: -194.91176470588235, up to now: minReward: -162.0, minAverage: -194.91176470588235\n",
      "Episode 34 done after 200 steps, reward Average: -195.05714285714285, up to now: minReward: -162.0, minAverage: -194.91176470588235\n",
      "Episode 35 done after 200 steps, reward Average: -195.19444444444446, up to now: minReward: -162.0, minAverage: -194.91176470588235\n",
      "Episode 36 done after 174 steps, reward Average: -194.6216216216216, up to now: minReward: -162.0, minAverage: -194.6216216216216\n",
      "Episode 37 done after 200 steps, reward Average: -194.76315789473685, up to now: minReward: -162.0, minAverage: -194.6216216216216\n",
      "Episode 38 done after 200 steps, reward Average: -194.89743589743588, up to now: minReward: -162.0, minAverage: -194.6216216216216\n",
      "Episode 39 done after 200 steps, reward Average: -195.025, up to now: minReward: -162.0, minAverage: -194.6216216216216\n",
      "Episode 40 done after 188 steps, reward Average: -194.85365853658536, up to now: minReward: -162.0, minAverage: -194.6216216216216\n",
      "Episode 41 done after 200 steps, reward Average: -194.97619047619048, up to now: minReward: -162.0, minAverage: -194.6216216216216\n",
      "Episode 42 done after 161 steps, reward Average: -194.1860465116279, up to now: minReward: -161.0, minAverage: -194.1860465116279\n",
      "Episode 43 done after 200 steps, reward Average: -194.3181818181818, up to now: minReward: -161.0, minAverage: -194.1860465116279\n",
      "Episode 44 done after 200 steps, reward Average: -194.44444444444446, up to now: minReward: -161.0, minAverage: -194.1860465116279\n",
      "Episode 45 done after 196 steps, reward Average: -194.47826086956522, up to now: minReward: -161.0, minAverage: -194.1860465116279\n",
      "Episode 46 done after 197 steps, reward Average: -194.53191489361703, up to now: minReward: -161.0, minAverage: -194.1860465116279\n",
      "Episode 47 done after 200 steps, reward Average: -194.64583333333334, up to now: minReward: -161.0, minAverage: -194.1860465116279\n",
      "Episode 48 done after 156 steps, reward Average: -193.85714285714286, up to now: minReward: -156.0, minAverage: -193.85714285714286\n",
      "Episode 49 done after 186 steps, reward Average: -193.7, up to now: minReward: -156.0, minAverage: -193.7\n",
      "Episode 50 done after 193 steps, reward Average: -193.68627450980392, up to now: minReward: -156.0, minAverage: -193.68627450980392\n",
      "Episode 51 done after 154 steps, reward Average: -192.92307692307693, up to now: minReward: -154.0, minAverage: -192.92307692307693\n",
      "Episode 52 done after 163 steps, reward Average: -192.35849056603774, up to now: minReward: -154.0, minAverage: -192.35849056603774\n",
      "Episode 53 done after 150 steps, reward Average: -191.57407407407408, up to now: minReward: -150.0, minAverage: -191.57407407407408\n",
      "Episode 54 done after 200 steps, reward Average: -191.72727272727272, up to now: minReward: -150.0, minAverage: -191.57407407407408\n",
      "Episode 55 done after 200 steps, reward Average: -191.875, up to now: minReward: -150.0, minAverage: -191.57407407407408\n",
      "Episode 56 done after 200 steps, reward Average: -192.01754385964912, up to now: minReward: -150.0, minAverage: -191.57407407407408\n",
      "Episode 57 done after 159 steps, reward Average: -191.44827586206895, up to now: minReward: -150.0, minAverage: -191.44827586206895\n",
      "Episode 58 done after 158 steps, reward Average: -190.88135593220338, up to now: minReward: -150.0, minAverage: -190.88135593220338\n",
      "Episode 59 done after 164 steps, reward Average: -190.43333333333334, up to now: minReward: -150.0, minAverage: -190.43333333333334\n",
      "Episode 60 done after 145 steps, reward Average: -189.68852459016392, up to now: minReward: -145.0, minAverage: -189.68852459016392\n",
      "Episode 61 done after 153 steps, reward Average: -189.09677419354838, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 62 done after 200 steps, reward Average: -189.26984126984127, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 63 done after 183 steps, reward Average: -189.171875, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 64 done after 200 steps, reward Average: -189.33846153846153, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 65 done after 196 steps, reward Average: -189.43939393939394, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 66 done after 200 steps, reward Average: -189.59701492537314, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 67 done after 196 steps, reward Average: -189.69117647058823, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 68 done after 175 steps, reward Average: -189.47826086956522, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 69 done after 200 steps, reward Average: -189.62857142857143, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 70 done after 200 steps, reward Average: -189.77464788732394, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 71 done after 149 steps, reward Average: -189.20833333333334, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 72 done after 200 steps, reward Average: -189.35616438356163, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 73 done after 196 steps, reward Average: -189.44594594594594, up to now: minReward: -145.0, minAverage: -189.09677419354838\n",
      "Episode 74 done after 162 steps, reward Average: -189.08, up to now: minReward: -145.0, minAverage: -189.08\n",
      "Episode 75 done after 154 steps, reward Average: -188.6184210526316, up to now: minReward: -145.0, minAverage: -188.6184210526316\n",
      "Episode 76 done after 200 steps, reward Average: -188.76623376623377, up to now: minReward: -145.0, minAverage: -188.6184210526316\n",
      "Episode 77 done after 187 steps, reward Average: -188.74358974358975, up to now: minReward: -145.0, minAverage: -188.6184210526316\n",
      "Episode 78 done after 121 steps, reward Average: -187.8860759493671, up to now: minReward: -121.0, minAverage: -187.8860759493671\n",
      "Episode 79 done after 117 steps, reward Average: -187.0, up to now: minReward: -117.0, minAverage: -187.0\n",
      "Episode 80 done after 166 steps, reward Average: -186.74074074074073, up to now: minReward: -117.0, minAverage: -186.74074074074073\n",
      "Episode 81 done after 151 steps, reward Average: -186.3048780487805, up to now: minReward: -117.0, minAverage: -186.3048780487805\n",
      "Episode 82 done after 160 steps, reward Average: -185.9879518072289, up to now: minReward: -117.0, minAverage: -185.9879518072289\n",
      "Episode 83 done after 150 steps, reward Average: -185.5595238095238, up to now: minReward: -117.0, minAverage: -185.5595238095238\n",
      "Episode 84 done after 193 steps, reward Average: -185.64705882352942, up to now: minReward: -117.0, minAverage: -185.5595238095238\n",
      "Episode 85 done after 182 steps, reward Average: -185.6046511627907, up to now: minReward: -117.0, minAverage: -185.5595238095238\n",
      "Episode 86 done after 156 steps, reward Average: -185.26436781609195, up to now: minReward: -117.0, minAverage: -185.26436781609195\n",
      "Episode 87 done after 154 steps, reward Average: -184.9090909090909, up to now: minReward: -117.0, minAverage: -184.9090909090909\n",
      "Episode 88 done after 155 steps, reward Average: -184.57303370786516, up to now: minReward: -117.0, minAverage: -184.57303370786516\n",
      "Episode 89 done after 153 steps, reward Average: -184.22222222222223, up to now: minReward: -117.0, minAverage: -184.22222222222223\n",
      "Episode 90 done after 150 steps, reward Average: -183.84615384615384, up to now: minReward: -117.0, minAverage: -183.84615384615384\n",
      "Episode 91 done after 148 steps, reward Average: -183.45652173913044, up to now: minReward: -117.0, minAverage: -183.45652173913044\n",
      "Episode 92 done after 157 steps, reward Average: -183.1720430107527, up to now: minReward: -117.0, minAverage: -183.1720430107527\n",
      "Episode 93 done after 200 steps, reward Average: -183.35106382978722, up to now: minReward: -117.0, minAverage: -183.1720430107527\n",
      "Episode 94 done after 180 steps, reward Average: -183.31578947368422, up to now: minReward: -117.0, minAverage: -183.1720430107527\n",
      "Episode 95 done after 191 steps, reward Average: -183.39583333333334, up to now: minReward: -117.0, minAverage: -183.1720430107527\n",
      "Episode 96 done after 173 steps, reward Average: -183.28865979381445, up to now: minReward: -117.0, minAverage: -183.1720430107527\n",
      "Episode 97 done after 153 steps, reward Average: -182.9795918367347, up to now: minReward: -117.0, minAverage: -182.9795918367347\n",
      "Episode 98 done after 186 steps, reward Average: -183.010101010101, up to now: minReward: -117.0, minAverage: -182.9795918367347\n",
      "Episode 99 done after 153 steps, reward Average: -182.71, up to now: minReward: -117.0, minAverage: -182.71\n",
      "Episode 100 done after 184 steps, reward Average: -182.55, up to now: minReward: -117.0, minAverage: -182.55\n",
      "Episode 101 done after 163 steps, reward Average: -182.18, up to now: minReward: -117.0, minAverage: -182.18\n",
      "Episode 102 done after 141 steps, reward Average: -181.59, up to now: minReward: -117.0, minAverage: -181.59\n",
      "Episode 103 done after 146 steps, reward Average: -181.05, up to now: minReward: -117.0, minAverage: -181.05\n",
      "Episode 104 done after 146 steps, reward Average: -180.51, up to now: minReward: -117.0, minAverage: -180.51\n",
      "Episode 105 done after 184 steps, reward Average: -180.35, up to now: minReward: -117.0, minAverage: -180.35\n",
      "Episode 106 done after 155 steps, reward Average: -179.9, up to now: minReward: -117.0, minAverage: -179.9\n",
      "Episode 107 done after 117 steps, reward Average: -179.07, up to now: minReward: -117.0, minAverage: -179.07\n",
      "Episode 108 done after 122 steps, reward Average: -178.29, up to now: minReward: -117.0, minAverage: -178.29\n",
      "Episode 109 done after 159 steps, reward Average: -177.88, up to now: minReward: -117.0, minAverage: -177.88\n",
      "Episode 110 done after 156 steps, reward Average: -177.44, up to now: minReward: -117.0, minAverage: -177.44\n",
      "Episode 111 done after 147 steps, reward Average: -176.91, up to now: minReward: -117.0, minAverage: -176.91\n",
      "Episode 112 done after 157 steps, reward Average: -176.48, up to now: minReward: -117.0, minAverage: -176.48\n",
      "Episode 113 done after 152 steps, reward Average: -176.0, up to now: minReward: -117.0, minAverage: -176.0\n",
      "Episode 114 done after 145 steps, reward Average: -175.45, up to now: minReward: -117.0, minAverage: -175.45\n",
      "Episode 115 done after 130 steps, reward Average: -174.75, up to now: minReward: -117.0, minAverage: -174.75\n",
      "Episode 116 done after 120 steps, reward Average: -173.95, up to now: minReward: -117.0, minAverage: -173.95\n",
      "Episode 117 done after 189 steps, reward Average: -173.84, up to now: minReward: -117.0, minAverage: -173.84\n",
      "Episode 118 done after 153 steps, reward Average: -173.37, up to now: minReward: -117.0, minAverage: -173.37\n",
      "Episode 119 done after 146 steps, reward Average: -172.83, up to now: minReward: -117.0, minAverage: -172.83\n",
      "Episode 120 done after 149 steps, reward Average: -172.32, up to now: minReward: -117.0, minAverage: -172.32\n",
      "Episode 121 done after 149 steps, reward Average: -171.81, up to now: minReward: -117.0, minAverage: -171.81\n",
      "Episode 122 done after 146 steps, reward Average: -171.27, up to now: minReward: -117.0, minAverage: -171.27\n",
      "Episode 123 done after 147 steps, reward Average: -171.12, up to now: minReward: -117.0, minAverage: -171.12\n",
      "Episode 124 done after 158 steps, reward Average: -170.7, up to now: minReward: -117.0, minAverage: -170.7\n",
      "Episode 125 done after 187 steps, reward Average: -170.88, up to now: minReward: -117.0, minAverage: -170.7\n",
      "Episode 126 done after 185 steps, reward Average: -170.73, up to now: minReward: -117.0, minAverage: -170.7\n",
      "Episode 127 done after 152 steps, reward Average: -170.25, up to now: minReward: -117.0, minAverage: -170.25\n",
      "Episode 128 done after 154 steps, reward Average: -170.13, up to now: minReward: -117.0, minAverage: -170.13\n",
      "Episode 129 done after 119 steps, reward Average: -169.32, up to now: minReward: -117.0, minAverage: -169.32\n",
      "Episode 130 done after 116 steps, reward Average: -168.48, up to now: minReward: -116.0, minAverage: -168.48\n",
      "Episode 131 done after 120 steps, reward Average: -167.68, up to now: minReward: -116.0, minAverage: -167.68\n",
      "Episode 132 done after 159 steps, reward Average: -167.63, up to now: minReward: -116.0, minAverage: -167.63\n",
      "Episode 133 done after 176 steps, reward Average: -167.73, up to now: minReward: -116.0, minAverage: -167.63\n",
      "Episode 134 done after 186 steps, reward Average: -167.59, up to now: minReward: -116.0, minAverage: -167.59\n",
      "Episode 135 done after 152 steps, reward Average: -167.11, up to now: minReward: -116.0, minAverage: -167.11\n",
      "Episode 136 done after 150 steps, reward Average: -166.87, up to now: minReward: -116.0, minAverage: -166.87\n",
      "Episode 137 done after 200 steps, reward Average: -166.87, up to now: minReward: -116.0, minAverage: -166.87\n",
      "Episode 138 done after 193 steps, reward Average: -166.8, up to now: minReward: -116.0, minAverage: -166.8\n",
      "Episode 139 done after 181 steps, reward Average: -166.61, up to now: minReward: -116.0, minAverage: -166.61\n",
      "Episode 140 done after 123 steps, reward Average: -165.96, up to now: minReward: -116.0, minAverage: -165.96\n",
      "Episode 141 done after 200 steps, reward Average: -165.96, up to now: minReward: -116.0, minAverage: -165.96\n",
      "Episode 142 done after 157 steps, reward Average: -165.92, up to now: minReward: -116.0, minAverage: -165.92\n",
      "Episode 143 done after 130 steps, reward Average: -165.22, up to now: minReward: -116.0, minAverage: -165.22\n",
      "Episode 144 done after 149 steps, reward Average: -164.71, up to now: minReward: -116.0, minAverage: -164.71\n",
      "Episode 145 done after 145 steps, reward Average: -164.2, up to now: minReward: -116.0, minAverage: -164.2\n",
      "Episode 146 done after 134 steps, reward Average: -163.57, up to now: minReward: -116.0, minAverage: -163.57\n",
      "Episode 147 done after 119 steps, reward Average: -162.76, up to now: minReward: -116.0, minAverage: -162.76\n",
      "Episode 148 done after 121 steps, reward Average: -162.41, up to now: minReward: -116.0, minAverage: -162.41\n",
      "Episode 149 done after 159 steps, reward Average: -162.14, up to now: minReward: -116.0, minAverage: -162.14\n",
      "Episode 150 done after 164 steps, reward Average: -161.85, up to now: minReward: -116.0, minAverage: -161.85\n",
      "Episode 151 done after 162 steps, reward Average: -161.93, up to now: minReward: -116.0, minAverage: -161.85\n",
      "Episode 152 done after 154 steps, reward Average: -161.84, up to now: minReward: -116.0, minAverage: -161.84\n",
      "Episode 153 done after 160 steps, reward Average: -161.94, up to now: minReward: -116.0, minAverage: -161.84\n",
      "Episode 154 done after 200 steps, reward Average: -161.94, up to now: minReward: -116.0, minAverage: -161.84\n",
      "Episode 155 done after 162 steps, reward Average: -161.56, up to now: minReward: -116.0, minAverage: -161.56\n",
      "Episode 156 done after 192 steps, reward Average: -161.48, up to now: minReward: -116.0, minAverage: -161.48\n",
      "Episode 157 done after 155 steps, reward Average: -161.44, up to now: minReward: -116.0, minAverage: -161.44\n",
      "Episode 158 done after 182 steps, reward Average: -161.68, up to now: minReward: -116.0, minAverage: -161.44\n",
      "Episode 159 done after 153 steps, reward Average: -161.57, up to now: minReward: -116.0, minAverage: -161.44\n",
      "Episode 160 done after 176 steps, reward Average: -161.88, up to now: minReward: -116.0, minAverage: -161.44\n",
      "Episode 161 done after 157 steps, reward Average: -161.92, up to now: minReward: -116.0, minAverage: -161.44\n",
      "Episode 162 done after 150 steps, reward Average: -161.42, up to now: minReward: -116.0, minAverage: -161.42\n",
      "Episode 163 done after 152 steps, reward Average: -161.11, up to now: minReward: -116.0, minAverage: -161.11\n",
      "Episode 164 done after 147 steps, reward Average: -160.58, up to now: minReward: -116.0, minAverage: -160.58\n",
      "Episode 165 done after 200 steps, reward Average: -160.62, up to now: minReward: -116.0, minAverage: -160.58\n",
      "Episode 166 done after 150 steps, reward Average: -160.12, up to now: minReward: -116.0, minAverage: -160.12\n",
      "Episode 167 done after 200 steps, reward Average: -160.16, up to now: minReward: -116.0, minAverage: -160.12\n",
      "Episode 168 done after 175 steps, reward Average: -160.16, up to now: minReward: -116.0, minAverage: -160.12\n",
      "Episode 169 done after 116 steps, reward Average: -159.32, up to now: minReward: -116.0, minAverage: -159.32\n",
      "Episode 170 done after 116 steps, reward Average: -158.48, up to now: minReward: -116.0, minAverage: -158.48\n",
      "Episode 171 done after 160 steps, reward Average: -158.59, up to now: minReward: -116.0, minAverage: -158.48\n",
      "Episode 172 done after 111 steps, reward Average: -157.7, up to now: minReward: -111.0, minAverage: -157.7\n",
      "Episode 173 done after 120 steps, reward Average: -156.94, up to now: minReward: -111.0, minAverage: -156.94\n",
      "Episode 174 done after 110 steps, reward Average: -156.42, up to now: minReward: -110.0, minAverage: -156.42\n",
      "Episode 175 done after 115 steps, reward Average: -156.03, up to now: minReward: -110.0, minAverage: -156.03\n",
      "Episode 176 done after 150 steps, reward Average: -155.53, up to now: minReward: -110.0, minAverage: -155.53\n",
      "Episode 177 done after 146 steps, reward Average: -155.12, up to now: minReward: -110.0, minAverage: -155.12\n",
      "Episode 178 done after 119 steps, reward Average: -155.1, up to now: minReward: -110.0, minAverage: -155.1\n",
      "Episode 179 done after 150 steps, reward Average: -155.43, up to now: minReward: -110.0, minAverage: -155.1\n",
      "Episode 180 done after 172 steps, reward Average: -155.49, up to now: minReward: -110.0, minAverage: -155.1\n",
      "Episode 181 done after 154 steps, reward Average: -155.52, up to now: minReward: -110.0, minAverage: -155.1\n",
      "Episode 182 done after 138 steps, reward Average: -155.3, up to now: minReward: -110.0, minAverage: -155.1\n",
      "Episode 183 done after 133 steps, reward Average: -155.13, up to now: minReward: -110.0, minAverage: -155.1\n",
      "Episode 184 done after 165 steps, reward Average: -154.85, up to now: minReward: -110.0, minAverage: -154.85\n",
      "Episode 185 done after 116 steps, reward Average: -154.19, up to now: minReward: -110.0, minAverage: -154.19\n",
      "Episode 186 done after 118 steps, reward Average: -153.81, up to now: minReward: -110.0, minAverage: -153.81\n",
      "Episode 187 done after 152 steps, reward Average: -153.79, up to now: minReward: -110.0, minAverage: -153.79\n",
      "Episode 188 done after 128 steps, reward Average: -153.52, up to now: minReward: -110.0, minAverage: -153.52\n",
      "Episode 189 done after 128 steps, reward Average: -153.27, up to now: minReward: -110.0, minAverage: -153.27\n",
      "Episode 190 done after 163 steps, reward Average: -153.4, up to now: minReward: -110.0, minAverage: -153.27\n",
      "Episode 191 done after 174 steps, reward Average: -153.66, up to now: minReward: -110.0, minAverage: -153.27\n",
      "Episode 192 done after 151 steps, reward Average: -153.6, up to now: minReward: -110.0, minAverage: -153.27\n",
      "Episode 193 done after 169 steps, reward Average: -153.29, up to now: minReward: -110.0, minAverage: -153.27\n",
      "Episode 194 done after 156 steps, reward Average: -153.05, up to now: minReward: -110.0, minAverage: -153.05\n",
      "Episode 195 done after 128 steps, reward Average: -152.42, up to now: minReward: -110.0, minAverage: -152.42\n",
      "Episode 196 done after 157 steps, reward Average: -152.26, up to now: minReward: -110.0, minAverage: -152.26\n",
      "Episode 197 done after 164 steps, reward Average: -152.37, up to now: minReward: -110.0, minAverage: -152.26\n",
      "Episode 198 done after 172 steps, reward Average: -152.23, up to now: minReward: -110.0, minAverage: -152.23\n",
      "Episode 199 done after 162 steps, reward Average: -152.32, up to now: minReward: -110.0, minAverage: -152.23\n",
      "Episode 200 done after 164 steps, reward Average: -152.12, up to now: minReward: -110.0, minAverage: -152.12\n",
      "Episode 201 done after 160 steps, reward Average: -152.09, up to now: minReward: -110.0, minAverage: -152.09\n",
      "Episode 202 done after 116 steps, reward Average: -151.84, up to now: minReward: -110.0, minAverage: -151.84\n",
      "Episode 203 done after 114 steps, reward Average: -151.52, up to now: minReward: -110.0, minAverage: -151.52\n",
      "Episode 204 done after 114 steps, reward Average: -151.2, up to now: minReward: -110.0, minAverage: -151.2\n",
      "Episode 205 done after 153 steps, reward Average: -150.89, up to now: minReward: -110.0, minAverage: -150.89\n",
      "Episode 206 done after 152 steps, reward Average: -150.86, up to now: minReward: -110.0, minAverage: -150.86\n",
      "Episode 207 done after 115 steps, reward Average: -150.84, up to now: minReward: -110.0, minAverage: -150.84\n",
      "Episode 208 done after 146 steps, reward Average: -151.08, up to now: minReward: -110.0, minAverage: -150.84\n",
      "Episode 209 done after 148 steps, reward Average: -150.97, up to now: minReward: -110.0, minAverage: -150.84\n",
      "Episode 210 done after 151 steps, reward Average: -150.92, up to now: minReward: -110.0, minAverage: -150.84\n",
      "Episode 211 done after 151 steps, reward Average: -150.96, up to now: minReward: -110.0, minAverage: -150.84\n",
      "Episode 212 done after 153 steps, reward Average: -150.92, up to now: minReward: -110.0, minAverage: -150.84\n",
      "Episode 213 done after 118 steps, reward Average: -150.58, up to now: minReward: -110.0, minAverage: -150.58\n",
      "Episode 214 done after 163 steps, reward Average: -150.76, up to now: minReward: -110.0, minAverage: -150.58\n",
      "Episode 215 done after 164 steps, reward Average: -151.1, up to now: minReward: -110.0, minAverage: -150.58\n",
      "Episode 216 done after 139 steps, reward Average: -151.29, up to now: minReward: -110.0, minAverage: -150.58\n",
      "Episode 217 done after 115 steps, reward Average: -150.55, up to now: minReward: -110.0, minAverage: -150.55\n",
      "Episode 218 done after 117 steps, reward Average: -150.19, up to now: minReward: -110.0, minAverage: -150.19\n",
      "Episode 219 done after 169 steps, reward Average: -150.42, up to now: minReward: -110.0, minAverage: -150.19\n",
      "Episode 220 done after 119 steps, reward Average: -150.12, up to now: minReward: -110.0, minAverage: -150.12\n",
      "Episode 221 done after 159 steps, reward Average: -150.22, up to now: minReward: -110.0, minAverage: -150.12\n",
      "Episode 222 done after 115 steps, reward Average: -149.91, up to now: minReward: -110.0, minAverage: -149.91\n",
      "Episode 223 done after 112 steps, reward Average: -149.56, up to now: minReward: -110.0, minAverage: -149.56\n",
      "Episode 224 done after 117 steps, reward Average: -149.15, up to now: minReward: -110.0, minAverage: -149.15\n",
      "Episode 225 done after 133 steps, reward Average: -148.61, up to now: minReward: -110.0, minAverage: -148.61\n",
      "Episode 226 done after 180 steps, reward Average: -148.56, up to now: minReward: -110.0, minAverage: -148.56\n",
      "Episode 227 done after 160 steps, reward Average: -148.64, up to now: minReward: -110.0, minAverage: -148.56\n",
      "Episode 228 done after 131 steps, reward Average: -148.41, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 229 done after 174 steps, reward Average: -148.96, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 230 done after 115 steps, reward Average: -148.95, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 231 done after 186 steps, reward Average: -149.61, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 232 done after 165 steps, reward Average: -149.67, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 233 done after 157 steps, reward Average: -149.48, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 234 done after 156 steps, reward Average: -149.18, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 235 done after 147 steps, reward Average: -149.13, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 236 done after 149 steps, reward Average: -149.12, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 237 done after 147 steps, reward Average: -148.59, up to now: minReward: -110.0, minAverage: -148.41\n",
      "Episode 238 done after 148 steps, reward Average: -148.14, up to now: minReward: -110.0, minAverage: -148.14\n",
      "Episode 239 done after 148 steps, reward Average: -147.81, up to now: minReward: -110.0, minAverage: -147.81\n",
      "Episode 240 done after 151 steps, reward Average: -148.09, up to now: minReward: -110.0, minAverage: -147.81\n",
      "Episode 241 done after 149 steps, reward Average: -147.58, up to now: minReward: -110.0, minAverage: -147.58\n",
      "Episode 242 done after 133 steps, reward Average: -147.34, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 243 done after 152 steps, reward Average: -147.56, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 244 done after 140 steps, reward Average: -147.47, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 245 done after 149 steps, reward Average: -147.51, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 246 done after 152 steps, reward Average: -147.69, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 247 done after 159 steps, reward Average: -148.09, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 248 done after 152 steps, reward Average: -148.4, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 249 done after 134 steps, reward Average: -148.15, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 250 done after 158 steps, reward Average: -148.09, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 251 done after 156 steps, reward Average: -148.03, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 252 done after 189 steps, reward Average: -148.38, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 253 done after 184 steps, reward Average: -148.62, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 254 done after 168 steps, reward Average: -148.3, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 255 done after 128 steps, reward Average: -147.96, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 256 done after 200 steps, reward Average: -148.04, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 257 done after 185 steps, reward Average: -148.34, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 258 done after 200 steps, reward Average: -148.52, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 259 done after 196 steps, reward Average: -148.95, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 260 done after 200 steps, reward Average: -149.19, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 261 done after 200 steps, reward Average: -149.62, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 262 done after 178 steps, reward Average: -149.9, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 263 done after 172 steps, reward Average: -150.1, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 264 done after 151 steps, reward Average: -150.14, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 265 done after 152 steps, reward Average: -149.66, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 266 done after 149 steps, reward Average: -149.65, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 267 done after 200 steps, reward Average: -149.65, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 268 done after 178 steps, reward Average: -149.68, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 269 done after 150 steps, reward Average: -150.02, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 270 done after 200 steps, reward Average: -150.86, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 271 done after 200 steps, reward Average: -151.26, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 272 done after 195 steps, reward Average: -152.1, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 273 done after 174 steps, reward Average: -152.64, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 274 done after 120 steps, reward Average: -152.74, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 275 done after 171 steps, reward Average: -153.3, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 276 done after 160 steps, reward Average: -153.4, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 277 done after 157 steps, reward Average: -153.51, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 278 done after 153 steps, reward Average: -153.85, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 279 done after 200 steps, reward Average: -154.35, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 280 done after 197 steps, reward Average: -154.6, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 281 done after 157 steps, reward Average: -154.63, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 282 done after 200 steps, reward Average: -155.25, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 283 done after 187 steps, reward Average: -155.79, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 284 done after 173 steps, reward Average: -155.87, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 285 done after 147 steps, reward Average: -156.18, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 286 done after 169 steps, reward Average: -156.69, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 287 done after 200 steps, reward Average: -157.17, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 288 done after 153 steps, reward Average: -157.42, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 289 done after 133 steps, reward Average: -157.47, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 290 done after 158 steps, reward Average: -157.42, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 291 done after 114 steps, reward Average: -156.82, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 292 done after 151 steps, reward Average: -156.82, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 293 done after 200 steps, reward Average: -157.13, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 294 done after 148 steps, reward Average: -157.05, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 295 done after 117 steps, reward Average: -156.94, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 296 done after 194 steps, reward Average: -157.31, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 297 done after 159 steps, reward Average: -157.26, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 298 done after 124 steps, reward Average: -156.78, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 299 done after 147 steps, reward Average: -156.63, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 300 done after 181 steps, reward Average: -156.8, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 301 done after 186 steps, reward Average: -157.06, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 302 done after 182 steps, reward Average: -157.72, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 303 done after 165 steps, reward Average: -158.23, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 304 done after 169 steps, reward Average: -158.78, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 305 done after 145 steps, reward Average: -158.7, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 306 done after 167 steps, reward Average: -158.85, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 307 done after 158 steps, reward Average: -159.28, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 308 done after 126 steps, reward Average: -159.08, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 309 done after 158 steps, reward Average: -159.18, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 310 done after 197 steps, reward Average: -159.64, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 311 done after 149 steps, reward Average: -159.62, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 312 done after 165 steps, reward Average: -159.74, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 313 done after 200 steps, reward Average: -160.56, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 314 done after 172 steps, reward Average: -160.65, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 315 done after 148 steps, reward Average: -160.49, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 316 done after 151 steps, reward Average: -160.61, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 317 done after 185 steps, reward Average: -161.31, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 318 done after 127 steps, reward Average: -161.41, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 319 done after 170 steps, reward Average: -161.42, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 320 done after 117 steps, reward Average: -161.4, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 321 done after 148 steps, reward Average: -161.29, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 322 done after 119 steps, reward Average: -161.33, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 323 done after 147 steps, reward Average: -161.68, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 324 done after 123 steps, reward Average: -161.74, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 325 done after 169 steps, reward Average: -162.1, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 326 done after 119 steps, reward Average: -161.49, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 327 done after 120 steps, reward Average: -161.09, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 328 done after 122 steps, reward Average: -161.0, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 329 done after 115 steps, reward Average: -160.41, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 330 done after 200 steps, reward Average: -161.26, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 331 done after 200 steps, reward Average: -161.4, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 332 done after 169 steps, reward Average: -161.44, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 333 done after 163 steps, reward Average: -161.5, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 334 done after 128 steps, reward Average: -161.22, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 335 done after 124 steps, reward Average: -160.99, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 336 done after 113 steps, reward Average: -160.63, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 337 done after 143 steps, reward Average: -160.59, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 338 done after 114 steps, reward Average: -160.25, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 339 done after 122 steps, reward Average: -159.99, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 340 done after 119 steps, reward Average: -159.67, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 341 done after 116 steps, reward Average: -159.34, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 342 done after 183 steps, reward Average: -159.84, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 343 done after 156 steps, reward Average: -159.88, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 344 done after 154 steps, reward Average: -160.02, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 345 done after 167 steps, reward Average: -160.2, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 346 done after 159 steps, reward Average: -160.27, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 347 done after 168 steps, reward Average: -160.36, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 348 done after 189 steps, reward Average: -160.73, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 349 done after 122 steps, reward Average: -160.61, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 350 done after 177 steps, reward Average: -160.8, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 351 done after 161 steps, reward Average: -160.85, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 352 done after 158 steps, reward Average: -160.54, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 353 done after 192 steps, reward Average: -160.62, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 354 done after 152 steps, reward Average: -160.46, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 355 done after 154 steps, reward Average: -160.72, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 356 done after 152 steps, reward Average: -160.24, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 357 done after 152 steps, reward Average: -159.91, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 358 done after 146 steps, reward Average: -159.37, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 359 done after 200 steps, reward Average: -159.41, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 360 done after 200 steps, reward Average: -159.41, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 361 done after 200 steps, reward Average: -159.41, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 362 done after 185 steps, reward Average: -159.48, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 363 done after 156 steps, reward Average: -159.32, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 364 done after 150 steps, reward Average: -159.31, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 365 done after 152 steps, reward Average: -159.31, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 366 done after 156 steps, reward Average: -159.38, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 367 done after 155 steps, reward Average: -158.93, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 368 done after 159 steps, reward Average: -158.74, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 369 done after 159 steps, reward Average: -158.83, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 370 done after 129 steps, reward Average: -158.12, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 371 done after 163 steps, reward Average: -157.75, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 372 done after 115 steps, reward Average: -156.95, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 373 done after 163 steps, reward Average: -156.84, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 374 done after 119 steps, reward Average: -156.83, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 375 done after 121 steps, reward Average: -156.33, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 376 done after 146 steps, reward Average: -156.19, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 377 done after 200 steps, reward Average: -156.62, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 378 done after 195 steps, reward Average: -157.04, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 379 done after 192 steps, reward Average: -156.96, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 380 done after 200 steps, reward Average: -156.99, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 381 done after 122 steps, reward Average: -156.64, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 382 done after 124 steps, reward Average: -155.88, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 383 done after 127 steps, reward Average: -155.28, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 384 done after 197 steps, reward Average: -155.52, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 385 done after 176 steps, reward Average: -155.81, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 386 done after 200 steps, reward Average: -156.12, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 387 done after 131 steps, reward Average: -155.43, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 388 done after 198 steps, reward Average: -155.88, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 389 done after 200 steps, reward Average: -156.55, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 390 done after 200 steps, reward Average: -156.97, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 391 done after 171 steps, reward Average: -157.54, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 392 done after 192 steps, reward Average: -157.95, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 393 done after 200 steps, reward Average: -157.95, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 394 done after 200 steps, reward Average: -158.47, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 395 done after 160 steps, reward Average: -158.9, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 396 done after 200 steps, reward Average: -158.96, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 397 done after 189 steps, reward Average: -159.26, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 398 done after 179 steps, reward Average: -159.81, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 399 done after 192 steps, reward Average: -160.26, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 400 done after 181 steps, reward Average: -160.26, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 401 done after 172 steps, reward Average: -160.12, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 402 done after 200 steps, reward Average: -160.3, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 403 done after 200 steps, reward Average: -160.65, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 404 done after 182 steps, reward Average: -160.78, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 405 done after 195 steps, reward Average: -161.28, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 406 done after 200 steps, reward Average: -161.61, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 407 done after 200 steps, reward Average: -162.03, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 408 done after 167 steps, reward Average: -162.44, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 409 done after 118 steps, reward Average: -162.04, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 410 done after 154 steps, reward Average: -161.61, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 411 done after 160 steps, reward Average: -161.72, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 412 done after 156 steps, reward Average: -161.63, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 413 done after 173 steps, reward Average: -161.36, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 414 done after 124 steps, reward Average: -160.88, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 415 done after 132 steps, reward Average: -160.72, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 416 done after 168 steps, reward Average: -160.89, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 417 done after 116 steps, reward Average: -160.2, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 418 done after 131 steps, reward Average: -160.24, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 419 done after 148 steps, reward Average: -160.02, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 420 done after 146 steps, reward Average: -160.31, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 421 done after 152 steps, reward Average: -160.35, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 422 done after 152 steps, reward Average: -160.68, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 423 done after 151 steps, reward Average: -160.72, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 424 done after 180 steps, reward Average: -161.29, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 425 done after 172 steps, reward Average: -161.32, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 426 done after 153 steps, reward Average: -161.66, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 427 done after 156 steps, reward Average: -162.02, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 428 done after 189 steps, reward Average: -162.69, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 429 done after 157 steps, reward Average: -163.11, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 430 done after 117 steps, reward Average: -162.28, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 431 done after 164 steps, reward Average: -161.92, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 432 done after 195 steps, reward Average: -162.18, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 433 done after 181 steps, reward Average: -162.36, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 434 done after 190 steps, reward Average: -162.98, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 435 done after 156 steps, reward Average: -163.3, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 436 done after 155 steps, reward Average: -163.72, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 437 done after 149 steps, reward Average: -163.78, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 438 done after 160 steps, reward Average: -164.24, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 439 done after 198 steps, reward Average: -165.0, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 440 done after 200 steps, reward Average: -165.81, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 441 done after 160 steps, reward Average: -166.25, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 442 done after 156 steps, reward Average: -165.98, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 443 done after 188 steps, reward Average: -166.3, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 444 done after 156 steps, reward Average: -166.32, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 445 done after 156 steps, reward Average: -166.21, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 446 done after 118 steps, reward Average: -165.8, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 447 done after 113 steps, reward Average: -165.25, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 448 done after 155 steps, reward Average: -164.91, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 449 done after 183 steps, reward Average: -165.52, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 450 done after 161 steps, reward Average: -165.36, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 451 done after 186 steps, reward Average: -165.61, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 452 done after 126 steps, reward Average: -165.29, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 453 done after 169 steps, reward Average: -165.06, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 454 done after 166 steps, reward Average: -165.2, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 455 done after 200 steps, reward Average: -165.66, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 456 done after 185 steps, reward Average: -165.99, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 457 done after 200 steps, reward Average: -166.47, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 458 done after 188 steps, reward Average: -166.89, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 459 done after 118 steps, reward Average: -166.07, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 460 done after 123 steps, reward Average: -165.3, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 461 done after 156 steps, reward Average: -164.86, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 462 done after 158 steps, reward Average: -164.59, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 463 done after 164 steps, reward Average: -164.67, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 464 done after 121 steps, reward Average: -164.38, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 465 done after 120 steps, reward Average: -164.06, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 466 done after 180 steps, reward Average: -164.3, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 467 done after 182 steps, reward Average: -164.57, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 468 done after 195 steps, reward Average: -164.93, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 469 done after 120 steps, reward Average: -164.54, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 470 done after 200 steps, reward Average: -165.25, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 471 done after 183 steps, reward Average: -165.45, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 472 done after 121 steps, reward Average: -165.51, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 473 done after 121 steps, reward Average: -165.09, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 474 done after 136 steps, reward Average: -165.26, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 475 done after 122 steps, reward Average: -165.27, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 476 done after 124 steps, reward Average: -165.05, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 477 done after 125 steps, reward Average: -164.3, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 478 done after 162 steps, reward Average: -163.97, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 479 done after 153 steps, reward Average: -163.58, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 480 done after 126 steps, reward Average: -162.84, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 481 done after 200 steps, reward Average: -163.62, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 482 done after 163 steps, reward Average: -164.01, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 483 done after 184 steps, reward Average: -164.58, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 484 done after 147 steps, reward Average: -164.08, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 485 done after 200 steps, reward Average: -164.32, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 486 done after 145 steps, reward Average: -163.77, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 487 done after 146 steps, reward Average: -163.92, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 488 done after 200 steps, reward Average: -163.94, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 489 done after 192 steps, reward Average: -163.86, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 490 done after 176 steps, reward Average: -163.62, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 491 done after 189 steps, reward Average: -163.8, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 492 done after 147 steps, reward Average: -163.35, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 493 done after 156 steps, reward Average: -162.91, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 494 done after 161 steps, reward Average: -162.52, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 495 done after 152 steps, reward Average: -162.44, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 496 done after 145 steps, reward Average: -161.89, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 497 done after 143 steps, reward Average: -161.43, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 498 done after 144 steps, reward Average: -161.08, up to now: minReward: -110.0, minAverage: -147.34\n",
      "Episode 499 done after 184 steps, reward Average: -161.0, up to now: minReward: -110.0, minAverage: -147.34\n",
      "final result: \n",
      "410 times arrived in 500 episodes, first time in episode 23\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x177ae4b73c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next)\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
