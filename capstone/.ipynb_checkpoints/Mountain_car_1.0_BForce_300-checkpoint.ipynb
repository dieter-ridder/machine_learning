{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Brute Force version, 300 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 12:18:30,236] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 198 steps, reward Average: -199.88888888888889, up to now: minReward: -198.0, minAverage: -199.88888888888889\n",
      "Episode 18 done after 196 steps, reward Average: -199.68421052631578, up to now: minReward: -196.0, minAverage: -199.68421052631578\n",
      "Episode 19 done after 118 steps, reward Average: -195.6, up to now: minReward: -118.0, minAverage: -195.6\n",
      "Episode 20 done after 108 steps, reward Average: -191.42857142857142, up to now: minReward: -108.0, minAverage: -191.42857142857142\n",
      "Episode 21 done after 110 steps, reward Average: -187.72727272727272, up to now: minReward: -108.0, minAverage: -187.72727272727272\n",
      "Episode 22 done after 200 steps, reward Average: -188.2608695652174, up to now: minReward: -108.0, minAverage: -187.72727272727272\n",
      "Episode 23 done after 112 steps, reward Average: -185.08333333333334, up to now: minReward: -108.0, minAverage: -185.08333333333334\n",
      "Episode 24 done after 110 steps, reward Average: -182.08, up to now: minReward: -108.0, minAverage: -182.08\n",
      "Episode 25 done after 109 steps, reward Average: -179.26923076923077, up to now: minReward: -108.0, minAverage: -179.26923076923077\n",
      "Episode 26 done after 107 steps, reward Average: -176.59259259259258, up to now: minReward: -107.0, minAverage: -176.59259259259258\n",
      "Episode 27 done after 107 steps, reward Average: -174.10714285714286, up to now: minReward: -107.0, minAverage: -174.10714285714286\n",
      "Episode 28 done after 108 steps, reward Average: -171.82758620689654, up to now: minReward: -107.0, minAverage: -171.82758620689654\n",
      "Episode 29 done after 106 steps, reward Average: -169.63333333333333, up to now: minReward: -106.0, minAverage: -169.63333333333333\n",
      "Episode 30 done after 107 steps, reward Average: -167.61290322580646, up to now: minReward: -106.0, minAverage: -167.61290322580646\n",
      "Episode 31 done after 105 steps, reward Average: -165.65625, up to now: minReward: -105.0, minAverage: -165.65625\n",
      "Episode 32 done after 107 steps, reward Average: -163.87878787878788, up to now: minReward: -105.0, minAverage: -163.87878787878788\n",
      "Episode 33 done after 107 steps, reward Average: -162.2058823529412, up to now: minReward: -105.0, minAverage: -162.2058823529412\n",
      "Episode 34 done after 102 steps, reward Average: -160.4857142857143, up to now: minReward: -102.0, minAverage: -160.4857142857143\n",
      "Episode 35 done after 102 steps, reward Average: -158.86111111111111, up to now: minReward: -102.0, minAverage: -158.86111111111111\n",
      "Episode 36 done after 104 steps, reward Average: -157.3783783783784, up to now: minReward: -102.0, minAverage: -157.3783783783784\n",
      "Episode 37 done after 104 steps, reward Average: -155.97368421052633, up to now: minReward: -102.0, minAverage: -155.97368421052633\n",
      "Episode 38 done after 104 steps, reward Average: -154.64102564102564, up to now: minReward: -102.0, minAverage: -154.64102564102564\n",
      "Episode 39 done after 99 steps, reward Average: -153.25, up to now: minReward: -99.0, minAverage: -153.25\n",
      "Episode 40 done after 104 steps, reward Average: -152.0487804878049, up to now: minReward: -99.0, minAverage: -152.0487804878049\n",
      "Episode 41 done after 102 steps, reward Average: -150.85714285714286, up to now: minReward: -99.0, minAverage: -150.85714285714286\n",
      "Episode 42 done after 103 steps, reward Average: -149.74418604651163, up to now: minReward: -99.0, minAverage: -149.74418604651163\n",
      "Episode 43 done after 105 steps, reward Average: -148.72727272727272, up to now: minReward: -99.0, minAverage: -148.72727272727272\n",
      "Episode 44 done after 103 steps, reward Average: -147.7111111111111, up to now: minReward: -99.0, minAverage: -147.7111111111111\n",
      "Episode 45 done after 96 steps, reward Average: -146.58695652173913, up to now: minReward: -96.0, minAverage: -146.58695652173913\n",
      "Episode 46 done after 102 steps, reward Average: -145.63829787234042, up to now: minReward: -96.0, minAverage: -145.63829787234042\n",
      "Episode 47 done after 104 steps, reward Average: -144.77083333333334, up to now: minReward: -96.0, minAverage: -144.77083333333334\n",
      "Episode 48 done after 99 steps, reward Average: -143.83673469387756, up to now: minReward: -96.0, minAverage: -143.83673469387756\n",
      "Episode 49 done after 105 steps, reward Average: -143.06, up to now: minReward: -96.0, minAverage: -143.06\n",
      "final result: \n",
      "32 times arrived in 50 episodes, first time in episode 17\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20b120a9e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
