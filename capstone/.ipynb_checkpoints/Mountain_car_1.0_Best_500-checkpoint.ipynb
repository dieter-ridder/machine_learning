{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Best version, 500 Episodes #\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 12:01:54,952] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 181 steps, reward Average: -197.625, up to now: minReward: -181.0, minAverage: -197.625\n",
      "Episode 8 done after 200 steps, reward Average: -197.88888888888889, up to now: minReward: -181.0, minAverage: -197.625\n",
      "Episode 9 done after 200 steps, reward Average: -198.1, up to now: minReward: -181.0, minAverage: -197.625\n",
      "Episode 10 done after 200 steps, reward Average: -198.27272727272728, up to now: minReward: -181.0, minAverage: -197.625\n",
      "Episode 11 done after 200 steps, reward Average: -198.41666666666666, up to now: minReward: -181.0, minAverage: -197.625\n",
      "Episode 12 done after 200 steps, reward Average: -198.53846153846155, up to now: minReward: -181.0, minAverage: -197.625\n",
      "Episode 13 done after 161 steps, reward Average: -195.85714285714286, up to now: minReward: -161.0, minAverage: -195.85714285714286\n",
      "Episode 14 done after 157 steps, reward Average: -193.26666666666668, up to now: minReward: -157.0, minAverage: -193.26666666666668\n",
      "Episode 15 done after 200 steps, reward Average: -193.6875, up to now: minReward: -157.0, minAverage: -193.26666666666668\n",
      "Episode 16 done after 159 steps, reward Average: -191.64705882352942, up to now: minReward: -157.0, minAverage: -191.64705882352942\n",
      "Episode 17 done after 153 steps, reward Average: -189.5, up to now: minReward: -153.0, minAverage: -189.5\n",
      "Episode 18 done after 151 steps, reward Average: -187.47368421052633, up to now: minReward: -151.0, minAverage: -187.47368421052633\n",
      "Episode 19 done after 143 steps, reward Average: -185.25, up to now: minReward: -143.0, minAverage: -185.25\n",
      "Episode 20 done after 139 steps, reward Average: -183.04761904761904, up to now: minReward: -139.0, minAverage: -183.04761904761904\n",
      "Episode 21 done after 140 steps, reward Average: -181.0909090909091, up to now: minReward: -139.0, minAverage: -181.0909090909091\n",
      "Episode 22 done after 139 steps, reward Average: -179.2608695652174, up to now: minReward: -139.0, minAverage: -179.2608695652174\n",
      "Episode 23 done after 140 steps, reward Average: -177.625, up to now: minReward: -139.0, minAverage: -177.625\n",
      "Episode 24 done after 149 steps, reward Average: -176.48, up to now: minReward: -139.0, minAverage: -176.48\n",
      "Episode 25 done after 137 steps, reward Average: -174.96153846153845, up to now: minReward: -137.0, minAverage: -174.96153846153845\n",
      "Episode 26 done after 136 steps, reward Average: -173.5185185185185, up to now: minReward: -136.0, minAverage: -173.5185185185185\n",
      "Episode 27 done after 138 steps, reward Average: -172.25, up to now: minReward: -136.0, minAverage: -172.25\n",
      "Episode 28 done after 135 steps, reward Average: -170.9655172413793, up to now: minReward: -135.0, minAverage: -170.9655172413793\n",
      "Episode 29 done after 137 steps, reward Average: -169.83333333333334, up to now: minReward: -135.0, minAverage: -169.83333333333334\n",
      "Episode 30 done after 135 steps, reward Average: -168.70967741935485, up to now: minReward: -135.0, minAverage: -168.70967741935485\n",
      "Episode 31 done after 135 steps, reward Average: -167.65625, up to now: minReward: -135.0, minAverage: -167.65625\n",
      "Episode 32 done after 135 steps, reward Average: -166.66666666666666, up to now: minReward: -135.0, minAverage: -166.66666666666666\n",
      "Episode 33 done after 136 steps, reward Average: -165.76470588235293, up to now: minReward: -135.0, minAverage: -165.76470588235293\n",
      "Episode 34 done after 133 steps, reward Average: -164.82857142857142, up to now: minReward: -133.0, minAverage: -164.82857142857142\n",
      "Episode 35 done after 137 steps, reward Average: -164.05555555555554, up to now: minReward: -133.0, minAverage: -164.05555555555554\n",
      "Episode 36 done after 134 steps, reward Average: -163.24324324324326, up to now: minReward: -133.0, minAverage: -163.24324324324326\n",
      "Episode 37 done after 133 steps, reward Average: -162.44736842105263, up to now: minReward: -133.0, minAverage: -162.44736842105263\n",
      "Episode 38 done after 136 steps, reward Average: -161.76923076923077, up to now: minReward: -133.0, minAverage: -161.76923076923077\n",
      "Episode 39 done after 135 steps, reward Average: -161.1, up to now: minReward: -133.0, minAverage: -161.1\n",
      "Episode 40 done after 133 steps, reward Average: -160.41463414634146, up to now: minReward: -133.0, minAverage: -160.41463414634146\n",
      "Episode 41 done after 134 steps, reward Average: -159.78571428571428, up to now: minReward: -133.0, minAverage: -159.78571428571428\n",
      "Episode 42 done after 138 steps, reward Average: -159.27906976744185, up to now: minReward: -133.0, minAverage: -159.27906976744185\n",
      "Episode 43 done after 133 steps, reward Average: -158.6818181818182, up to now: minReward: -133.0, minAverage: -158.6818181818182\n",
      "Episode 44 done after 135 steps, reward Average: -158.15555555555557, up to now: minReward: -133.0, minAverage: -158.15555555555557\n",
      "Episode 45 done after 135 steps, reward Average: -157.65217391304347, up to now: minReward: -133.0, minAverage: -157.65217391304347\n",
      "Episode 46 done after 133 steps, reward Average: -157.12765957446808, up to now: minReward: -133.0, minAverage: -157.12765957446808\n",
      "Episode 47 done after 135 steps, reward Average: -156.66666666666666, up to now: minReward: -133.0, minAverage: -156.66666666666666\n",
      "Episode 48 done after 134 steps, reward Average: -156.20408163265307, up to now: minReward: -133.0, minAverage: -156.20408163265307\n",
      "Episode 49 done after 134 steps, reward Average: -155.76, up to now: minReward: -133.0, minAverage: -155.76\n",
      "Episode 50 done after 134 steps, reward Average: -155.33333333333334, up to now: minReward: -133.0, minAverage: -155.33333333333334\n",
      "Episode 51 done after 134 steps, reward Average: -154.92307692307693, up to now: minReward: -133.0, minAverage: -154.92307692307693\n",
      "Episode 52 done after 134 steps, reward Average: -154.52830188679246, up to now: minReward: -133.0, minAverage: -154.52830188679246\n",
      "Episode 53 done after 135 steps, reward Average: -154.16666666666666, up to now: minReward: -133.0, minAverage: -154.16666666666666\n",
      "Episode 54 done after 134 steps, reward Average: -153.8, up to now: minReward: -133.0, minAverage: -153.8\n",
      "Episode 55 done after 134 steps, reward Average: -153.44642857142858, up to now: minReward: -133.0, minAverage: -153.44642857142858\n",
      "Episode 56 done after 134 steps, reward Average: -153.10526315789474, up to now: minReward: -133.0, minAverage: -153.10526315789474\n",
      "Episode 57 done after 134 steps, reward Average: -152.77586206896552, up to now: minReward: -133.0, minAverage: -152.77586206896552\n",
      "Episode 58 done after 133 steps, reward Average: -152.4406779661017, up to now: minReward: -133.0, minAverage: -152.4406779661017\n",
      "Episode 59 done after 135 steps, reward Average: -152.15, up to now: minReward: -133.0, minAverage: -152.15\n",
      "Episode 60 done after 135 steps, reward Average: -151.86885245901638, up to now: minReward: -133.0, minAverage: -151.86885245901638\n",
      "Episode 61 done after 134 steps, reward Average: -151.58064516129033, up to now: minReward: -133.0, minAverage: -151.58064516129033\n",
      "Episode 62 done after 135 steps, reward Average: -151.31746031746033, up to now: minReward: -133.0, minAverage: -151.31746031746033\n",
      "Episode 63 done after 134 steps, reward Average: -151.046875, up to now: minReward: -133.0, minAverage: -151.046875\n",
      "Episode 64 done after 134 steps, reward Average: -150.7846153846154, up to now: minReward: -133.0, minAverage: -150.7846153846154\n",
      "Episode 65 done after 136 steps, reward Average: -150.56060606060606, up to now: minReward: -133.0, minAverage: -150.56060606060606\n",
      "Episode 66 done after 134 steps, reward Average: -150.3134328358209, up to now: minReward: -133.0, minAverage: -150.3134328358209\n",
      "Episode 67 done after 134 steps, reward Average: -150.0735294117647, up to now: minReward: -133.0, minAverage: -150.0735294117647\n",
      "Episode 68 done after 135 steps, reward Average: -149.85507246376812, up to now: minReward: -133.0, minAverage: -149.85507246376812\n",
      "Episode 69 done after 135 steps, reward Average: -149.64285714285714, up to now: minReward: -133.0, minAverage: -149.64285714285714\n",
      "Episode 70 done after 134 steps, reward Average: -149.42253521126761, up to now: minReward: -133.0, minAverage: -149.42253521126761\n",
      "Episode 71 done after 136 steps, reward Average: -149.23611111111111, up to now: minReward: -133.0, minAverage: -149.23611111111111\n",
      "Episode 72 done after 134 steps, reward Average: -149.02739726027397, up to now: minReward: -133.0, minAverage: -149.02739726027397\n",
      "Episode 73 done after 136 steps, reward Average: -148.85135135135135, up to now: minReward: -133.0, minAverage: -148.85135135135135\n",
      "Episode 74 done after 135 steps, reward Average: -148.66666666666666, up to now: minReward: -133.0, minAverage: -148.66666666666666\n",
      "Episode 75 done after 135 steps, reward Average: -148.48684210526315, up to now: minReward: -133.0, minAverage: -148.48684210526315\n",
      "Episode 76 done after 139 steps, reward Average: -148.36363636363637, up to now: minReward: -133.0, minAverage: -148.36363636363637\n",
      "Episode 77 done after 133 steps, reward Average: -148.16666666666666, up to now: minReward: -133.0, minAverage: -148.16666666666666\n",
      "Episode 78 done after 134 steps, reward Average: -147.9873417721519, up to now: minReward: -133.0, minAverage: -147.9873417721519\n",
      "Episode 79 done after 134 steps, reward Average: -147.8125, up to now: minReward: -133.0, minAverage: -147.8125\n",
      "Episode 80 done after 134 steps, reward Average: -147.64197530864197, up to now: minReward: -133.0, minAverage: -147.64197530864197\n",
      "Episode 81 done after 135 steps, reward Average: -147.4878048780488, up to now: minReward: -133.0, minAverage: -147.4878048780488\n",
      "Episode 82 done after 140 steps, reward Average: -147.3975903614458, up to now: minReward: -133.0, minAverage: -147.3975903614458\n",
      "Episode 83 done after 133 steps, reward Average: -147.22619047619048, up to now: minReward: -133.0, minAverage: -147.22619047619048\n",
      "Episode 84 done after 134 steps, reward Average: -147.0705882352941, up to now: minReward: -133.0, minAverage: -147.0705882352941\n",
      "Episode 85 done after 134 steps, reward Average: -146.91860465116278, up to now: minReward: -133.0, minAverage: -146.91860465116278\n",
      "Episode 86 done after 139 steps, reward Average: -146.82758620689654, up to now: minReward: -133.0, minAverage: -146.82758620689654\n",
      "Episode 87 done after 139 steps, reward Average: -146.73863636363637, up to now: minReward: -133.0, minAverage: -146.73863636363637\n",
      "Episode 88 done after 133 steps, reward Average: -146.58426966292134, up to now: minReward: -133.0, minAverage: -146.58426966292134\n",
      "Episode 89 done after 137 steps, reward Average: -146.4777777777778, up to now: minReward: -133.0, minAverage: -146.4777777777778\n",
      "Episode 90 done after 133 steps, reward Average: -146.32967032967034, up to now: minReward: -133.0, minAverage: -146.32967032967034\n",
      "Episode 91 done after 134 steps, reward Average: -146.19565217391303, up to now: minReward: -133.0, minAverage: -146.19565217391303\n",
      "Episode 92 done after 134 steps, reward Average: -146.06451612903226, up to now: minReward: -133.0, minAverage: -146.06451612903226\n",
      "Episode 93 done after 133 steps, reward Average: -145.9255319148936, up to now: minReward: -133.0, minAverage: -145.9255319148936\n",
      "Episode 94 done after 134 steps, reward Average: -145.8, up to now: minReward: -133.0, minAverage: -145.8\n",
      "Episode 95 done after 135 steps, reward Average: -145.6875, up to now: minReward: -133.0, minAverage: -145.6875\n",
      "Episode 96 done after 140 steps, reward Average: -145.62886597938143, up to now: minReward: -133.0, minAverage: -145.62886597938143\n",
      "Episode 97 done after 133 steps, reward Average: -145.5, up to now: minReward: -133.0, minAverage: -145.5\n",
      "Episode 98 done after 134 steps, reward Average: -145.3838383838384, up to now: minReward: -133.0, minAverage: -145.3838383838384\n",
      "Episode 99 done after 134 steps, reward Average: -145.27, up to now: minReward: -133.0, minAverage: -145.27\n",
      "Episode 100 done after 134 steps, reward Average: -144.61, up to now: minReward: -133.0, minAverage: -144.61\n",
      "Episode 101 done after 134 steps, reward Average: -143.95, up to now: minReward: -133.0, minAverage: -143.95\n",
      "Episode 102 done after 134 steps, reward Average: -143.29, up to now: minReward: -133.0, minAverage: -143.29\n",
      "Episode 103 done after 134 steps, reward Average: -142.63, up to now: minReward: -133.0, minAverage: -142.63\n",
      "Episode 104 done after 134 steps, reward Average: -141.97, up to now: minReward: -133.0, minAverage: -141.97\n",
      "Episode 105 done after 134 steps, reward Average: -141.31, up to now: minReward: -133.0, minAverage: -141.31\n",
      "Episode 106 done after 136 steps, reward Average: -140.67, up to now: minReward: -133.0, minAverage: -140.67\n",
      "Episode 107 done after 134 steps, reward Average: -140.2, up to now: minReward: -133.0, minAverage: -140.2\n",
      "Episode 108 done after 134 steps, reward Average: -139.54, up to now: minReward: -133.0, minAverage: -139.54\n",
      "Episode 109 done after 136 steps, reward Average: -138.9, up to now: minReward: -133.0, minAverage: -138.9\n",
      "Episode 110 done after 135 steps, reward Average: -138.25, up to now: minReward: -133.0, minAverage: -138.25\n",
      "Episode 111 done after 133 steps, reward Average: -137.58, up to now: minReward: -133.0, minAverage: -137.58\n",
      "Episode 112 done after 135 steps, reward Average: -136.93, up to now: minReward: -133.0, minAverage: -136.93\n",
      "Episode 113 done after 134 steps, reward Average: -136.66, up to now: minReward: -133.0, minAverage: -136.66\n",
      "Episode 114 done after 133 steps, reward Average: -136.42, up to now: minReward: -133.0, minAverage: -136.42\n",
      "Episode 115 done after 135 steps, reward Average: -135.77, up to now: minReward: -133.0, minAverage: -135.77\n",
      "Episode 116 done after 134 steps, reward Average: -135.52, up to now: minReward: -133.0, minAverage: -135.52\n",
      "Episode 117 done after 134 steps, reward Average: -135.33, up to now: minReward: -133.0, minAverage: -135.33\n",
      "Episode 118 done after 135 steps, reward Average: -135.17, up to now: minReward: -133.0, minAverage: -135.17\n",
      "Episode 119 done after 134 steps, reward Average: -135.08, up to now: minReward: -133.0, minAverage: -135.08\n",
      "Episode 120 done after 136 steps, reward Average: -135.05, up to now: minReward: -133.0, minAverage: -135.05\n",
      "Episode 121 done after 134 steps, reward Average: -134.99, up to now: minReward: -133.0, minAverage: -134.99\n",
      "Episode 122 done after 134 steps, reward Average: -134.94, up to now: minReward: -133.0, minAverage: -134.94\n",
      "Episode 123 done after 134 steps, reward Average: -134.88, up to now: minReward: -133.0, minAverage: -134.88\n",
      "Episode 124 done after 134 steps, reward Average: -134.73, up to now: minReward: -133.0, minAverage: -134.73\n",
      "Episode 125 done after 133 steps, reward Average: -134.69, up to now: minReward: -133.0, minAverage: -134.69\n",
      "Episode 126 done after 134 steps, reward Average: -134.67, up to now: minReward: -133.0, minAverage: -134.67\n",
      "Episode 127 done after 134 steps, reward Average: -134.63, up to now: minReward: -133.0, minAverage: -134.63\n",
      "Episode 128 done after 134 steps, reward Average: -134.62, up to now: minReward: -133.0, minAverage: -134.62\n",
      "Episode 129 done after 133 steps, reward Average: -134.58, up to now: minReward: -133.0, minAverage: -134.58\n",
      "Episode 130 done after 135 steps, reward Average: -134.58, up to now: minReward: -133.0, minAverage: -134.58\n",
      "Episode 131 done after 135 steps, reward Average: -134.58, up to now: minReward: -133.0, minAverage: -134.58\n",
      "Episode 132 done after 134 steps, reward Average: -134.57, up to now: minReward: -133.0, minAverage: -134.57\n",
      "Episode 133 done after 134 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.55\n",
      "Episode 134 done after 134 steps, reward Average: -134.56, up to now: minReward: -133.0, minAverage: -134.55\n",
      "Episode 135 done after 134 steps, reward Average: -134.53, up to now: minReward: -133.0, minAverage: -134.53\n",
      "Episode 136 done after 134 steps, reward Average: -134.53, up to now: minReward: -133.0, minAverage: -134.53\n",
      "Episode 137 done after 136 steps, reward Average: -134.56, up to now: minReward: -133.0, minAverage: -134.53\n",
      "Episode 138 done after 133 steps, reward Average: -134.53, up to now: minReward: -133.0, minAverage: -134.53\n",
      "Episode 139 done after 134 steps, reward Average: -134.52, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 140 done after 138 steps, reward Average: -134.57, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 141 done after 134 steps, reward Average: -134.57, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 142 done after 137 steps, reward Average: -134.56, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 143 done after 134 steps, reward Average: -134.57, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 144 done after 134 steps, reward Average: -134.56, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 145 done after 134 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 146 done after 135 steps, reward Average: -134.57, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 147 done after 140 steps, reward Average: -134.62, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 148 done after 133 steps, reward Average: -134.61, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 149 done after 134 steps, reward Average: -134.61, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 150 done after 134 steps, reward Average: -134.61, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 151 done after 134 steps, reward Average: -134.61, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 152 done after 133 steps, reward Average: -134.6, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 153 done after 135 steps, reward Average: -134.6, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 154 done after 134 steps, reward Average: -134.6, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 155 done after 134 steps, reward Average: -134.6, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 156 done after 135 steps, reward Average: -134.61, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 157 done after 140 steps, reward Average: -134.67, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 158 done after 134 steps, reward Average: -134.68, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 159 done after 133 steps, reward Average: -134.66, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 160 done after 134 steps, reward Average: -134.65, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 161 done after 135 steps, reward Average: -134.66, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 162 done after 134 steps, reward Average: -134.65, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 163 done after 134 steps, reward Average: -134.65, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 164 done after 134 steps, reward Average: -134.65, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 165 done after 140 steps, reward Average: -134.69, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 166 done after 135 steps, reward Average: -134.7, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 167 done after 134 steps, reward Average: -134.7, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 168 done after 134 steps, reward Average: -134.69, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 169 done after 134 steps, reward Average: -134.68, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 170 done after 134 steps, reward Average: -134.68, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 171 done after 134 steps, reward Average: -134.66, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 172 done after 134 steps, reward Average: -134.66, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 173 done after 134 steps, reward Average: -134.64, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 174 done after 134 steps, reward Average: -134.63, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 175 done after 134 steps, reward Average: -134.62, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 176 done after 134 steps, reward Average: -134.57, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 177 done after 135 steps, reward Average: -134.59, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 178 done after 134 steps, reward Average: -134.59, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 179 done after 134 steps, reward Average: -134.59, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 180 done after 134 steps, reward Average: -134.59, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 181 done after 137 steps, reward Average: -134.61, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 182 done after 133 steps, reward Average: -134.54, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 183 done after 134 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 184 done after 134 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 185 done after 134 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.52\n",
      "Episode 186 done after 134 steps, reward Average: -134.5, up to now: minReward: -133.0, minAverage: -134.5\n",
      "Episode 187 done after 133 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 188 done after 136 steps, reward Average: -134.47, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 189 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 190 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 191 done after 136 steps, reward Average: -134.47, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 192 done after 133 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 193 done after 135 steps, reward Average: -134.48, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 194 done after 134 steps, reward Average: -134.48, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 195 done after 134 steps, reward Average: -134.47, up to now: minReward: -133.0, minAverage: -134.44\n",
      "Episode 196 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 197 done after 134 steps, reward Average: -134.42, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 198 done after 134 steps, reward Average: -134.42, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 199 done after 138 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 200 done after 134 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 201 done after 133 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 202 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 203 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 204 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 205 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 206 done after 135 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 207 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 208 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 209 done after 135 steps, reward Average: -134.43, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 210 done after 134 steps, reward Average: -134.42, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 211 done after 137 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 212 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 213 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 214 done after 134 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 215 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 216 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 217 done after 133 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 218 done after 134 steps, reward Average: -134.43, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 219 done after 136 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 220 done after 134 steps, reward Average: -134.43, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 221 done after 134 steps, reward Average: -134.43, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 222 done after 135 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 223 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 224 done after 135 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 225 done after 134 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 226 done after 134 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 227 done after 134 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 228 done after 139 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 229 done after 134 steps, reward Average: -134.52, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 230 done after 134 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 231 done after 134 steps, reward Average: -134.5, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 232 done after 135 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 233 done after 134 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 234 done after 134 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 235 done after 134 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 236 done after 134 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 237 done after 133 steps, reward Average: -134.48, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 238 done after 133 steps, reward Average: -134.48, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 239 done after 134 steps, reward Average: -134.48, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 240 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 241 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 242 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 243 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 244 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 245 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.41\n",
      "Episode 246 done after 134 steps, reward Average: -134.4, up to now: minReward: -133.0, minAverage: -134.4\n",
      "Episode 247 done after 134 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.34\n",
      "Episode 248 done after 133 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.34\n",
      "Episode 249 done after 133 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.33\n",
      "Episode 250 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.33\n",
      "Episode 251 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.33\n",
      "Episode 252 done after 134 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.33\n",
      "Episode 253 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.33\n",
      "Episode 254 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.33\n",
      "Episode 255 done after 135 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.33\n",
      "Episode 256 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.33\n",
      "Episode 257 done after 134 steps, reward Average: -134.27, up to now: minReward: -133.0, minAverage: -134.27\n",
      "Episode 258 done after 133 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.26\n",
      "Episode 259 done after 135 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.26\n",
      "Episode 260 done after 133 steps, reward Average: -134.27, up to now: minReward: -133.0, minAverage: -134.26\n",
      "Episode 261 done after 134 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.26\n",
      "Episode 262 done after 134 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.26\n",
      "Episode 263 done after 137 steps, reward Average: -134.29, up to now: minReward: -133.0, minAverage: -134.26\n",
      "Episode 264 done after 133 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.26\n",
      "Episode 265 done after 134 steps, reward Average: -134.22, up to now: minReward: -133.0, minAverage: -134.22\n",
      "Episode 266 done after 133 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 267 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 268 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 269 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 270 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 271 done after 135 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 272 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 273 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 274 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 275 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 276 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 277 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 278 done after 135 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 279 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 280 done after 133 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.2\n",
      "Episode 281 done after 133 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.16\n",
      "Episode 282 done after 133 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.16\n",
      "Episode 283 done after 134 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.16\n",
      "Episode 284 done after 136 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.16\n",
      "Episode 285 done after 133 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.16\n",
      "Episode 286 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.16\n",
      "Episode 287 done after 134 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.16\n",
      "Episode 288 done after 133 steps, reward Average: -134.15, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 289 done after 137 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 290 done after 134 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 291 done after 135 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 292 done after 134 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 293 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 294 done after 140 steps, reward Average: -134.23, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 295 done after 134 steps, reward Average: -134.23, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 296 done after 134 steps, reward Average: -134.23, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 297 done after 134 steps, reward Average: -134.23, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 298 done after 134 steps, reward Average: -134.23, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 299 done after 134 steps, reward Average: -134.19, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 300 done after 134 steps, reward Average: -134.19, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 301 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 302 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 303 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 304 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 305 done after 136 steps, reward Average: -134.22, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 306 done after 133 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 307 done after 135 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 308 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 309 done after 135 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 310 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 311 done after 134 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 312 done after 134 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 313 done after 133 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 314 done after 133 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 315 done after 133 steps, reward Average: -134.15, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 316 done after 134 steps, reward Average: -134.15, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 317 done after 134 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 318 done after 134 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.15\n",
      "Episode 319 done after 134 steps, reward Average: -134.14, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 320 done after 134 steps, reward Average: -134.14, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 321 done after 140 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 322 done after 135 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 323 done after 134 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 324 done after 136 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 325 done after 133 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 326 done after 136 steps, reward Average: -134.22, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 327 done after 134 steps, reward Average: -134.22, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 328 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 329 done after 133 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 330 done after 136 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 331 done after 134 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 332 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 333 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 334 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 335 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 336 done after 133 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 337 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 338 done after 133 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 339 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 340 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 341 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 342 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 343 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 344 done after 134 steps, reward Average: -134.17, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 345 done after 133 steps, reward Average: -134.16, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 346 done after 136 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 347 done after 134 steps, reward Average: -134.18, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 348 done after 135 steps, reward Average: -134.2, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 349 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 350 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 351 done after 134 steps, reward Average: -134.21, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 352 done after 139 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 353 done after 134 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 354 done after 133 steps, reward Average: -134.25, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 355 done after 133 steps, reward Average: -134.23, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 356 done after 134 steps, reward Average: -134.23, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 357 done after 137 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 358 done after 133 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 359 done after 134 steps, reward Average: -134.25, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 360 done after 136 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 361 done after 137 steps, reward Average: -134.31, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 362 done after 134 steps, reward Average: -134.31, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 363 done after 134 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 364 done after 134 steps, reward Average: -134.29, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 365 done after 134 steps, reward Average: -134.29, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 366 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 367 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 368 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 369 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 370 done after 135 steps, reward Average: -134.31, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 371 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 372 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 373 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 374 done after 133 steps, reward Average: -134.29, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 375 done after 133 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 376 done after 134 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 377 done after 135 steps, reward Average: -134.29, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 378 done after 135 steps, reward Average: -134.29, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 379 done after 133 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 380 done after 134 steps, reward Average: -134.29, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 381 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 382 done after 134 steps, reward Average: -134.31, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 383 done after 135 steps, reward Average: -134.32, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 384 done after 134 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 385 done after 134 steps, reward Average: -134.31, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 386 done after 133 steps, reward Average: -134.3, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 387 done after 133 steps, reward Average: -134.29, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 388 done after 135 steps, reward Average: -134.31, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 389 done after 134 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 390 done after 134 steps, reward Average: -134.28, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 391 done after 134 steps, reward Average: -134.27, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 392 done after 138 steps, reward Average: -134.31, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 393 done after 135 steps, reward Average: -134.32, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 394 done after 134 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 395 done after 134 steps, reward Average: -134.26, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 396 done after 140 steps, reward Average: -134.32, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 397 done after 134 steps, reward Average: -134.32, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 398 done after 134 steps, reward Average: -134.32, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 399 done after 134 steps, reward Average: -134.32, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 400 done after 137 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 401 done after 134 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 402 done after 134 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 403 done after 134 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 404 done after 133 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 405 done after 135 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 406 done after 135 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 407 done after 135 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 408 done after 133 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 409 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 410 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 411 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 412 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 413 done after 134 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 414 done after 135 steps, reward Average: -134.36, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 415 done after 134 steps, reward Average: -134.37, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 416 done after 134 steps, reward Average: -134.37, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 417 done after 134 steps, reward Average: -134.37, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 418 done after 134 steps, reward Average: -134.37, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 419 done after 134 steps, reward Average: -134.37, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 420 done after 137 steps, reward Average: -134.4, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 421 done after 135 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 422 done after 135 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 423 done after 134 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 424 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 425 done after 134 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 426 done after 135 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 427 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 428 done after 134 steps, reward Average: -134.33, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 429 done after 134 steps, reward Average: -134.34, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 430 done after 137 steps, reward Average: -134.35, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 431 done after 139 steps, reward Average: -134.4, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 432 done after 133 steps, reward Average: -134.39, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 433 done after 134 steps, reward Average: -134.39, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 434 done after 134 steps, reward Average: -134.39, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 435 done after 135 steps, reward Average: -134.4, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 436 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 437 done after 135 steps, reward Average: -134.42, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 438 done after 134 steps, reward Average: -134.43, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 439 done after 133 steps, reward Average: -134.42, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 440 done after 133 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 441 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 442 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 443 done after 134 steps, reward Average: -134.41, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 444 done after 135 steps, reward Average: -134.42, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 445 done after 134 steps, reward Average: -134.43, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 446 done after 141 steps, reward Average: -134.48, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 447 done after 140 steps, reward Average: -134.54, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 448 done after 134 steps, reward Average: -134.53, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 449 done after 135 steps, reward Average: -134.54, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 450 done after 133 steps, reward Average: -134.53, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 451 done after 136 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 452 done after 136 steps, reward Average: -134.52, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 453 done after 133 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 454 done after 134 steps, reward Average: -134.52, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 455 done after 134 steps, reward Average: -134.53, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 456 done after 134 steps, reward Average: -134.53, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 457 done after 134 steps, reward Average: -134.5, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 458 done after 133 steps, reward Average: -134.5, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 459 done after 134 steps, reward Average: -134.5, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 460 done after 134 steps, reward Average: -134.48, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 461 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 462 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 463 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 464 done after 135 steps, reward Average: -134.46, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 465 done after 133 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 466 done after 133 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 467 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 468 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 469 done after 134 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 470 done after 135 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 471 done after 135 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 472 done after 134 steps, reward Average: -134.45, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 473 done after 133 steps, reward Average: -134.44, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 474 done after 140 steps, reward Average: -134.51, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 475 done after 134 steps, reward Average: -134.52, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 476 done after 136 steps, reward Average: -134.54, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 477 done after 134 steps, reward Average: -134.53, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 478 done after 137 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 479 done after 134 steps, reward Average: -134.56, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 480 done after 134 steps, reward Average: -134.56, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 481 done after 133 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 482 done after 134 steps, reward Average: -134.55, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 483 done after 136 steps, reward Average: -134.56, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 484 done after 135 steps, reward Average: -134.57, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 485 done after 134 steps, reward Average: -134.57, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 486 done after 135 steps, reward Average: -134.59, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 487 done after 144 steps, reward Average: -134.7, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 488 done after 134 steps, reward Average: -134.69, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 489 done after 133 steps, reward Average: -134.68, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 490 done after 134 steps, reward Average: -134.68, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 491 done after 134 steps, reward Average: -134.68, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 492 done after 134 steps, reward Average: -134.64, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 493 done after 138 steps, reward Average: -134.67, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 494 done after 134 steps, reward Average: -134.67, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 495 done after 133 steps, reward Average: -134.66, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 496 done after 134 steps, reward Average: -134.6, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 497 done after 134 steps, reward Average: -134.6, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 498 done after 133 steps, reward Average: -134.59, up to now: minReward: -133.0, minAverage: -134.14\n",
      "Episode 499 done after 134 steps, reward Average: -134.59, up to now: minReward: -133.0, minAverage: -134.14\n",
      "final result: \n",
      "487 times arrived in 500 episodes, first time in episode 7\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fc16aa3240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
