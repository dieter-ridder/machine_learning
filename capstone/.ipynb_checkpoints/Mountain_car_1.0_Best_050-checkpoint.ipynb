{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling 888, Best version, 50 Episodes #\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 11:42:03,031] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 50\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=50\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 184 steps, reward Average: -198.85714285714286, up to now: minReward: -184.0, minAverage: -198.85714285714286\n",
      "Episode 14 done after 200 steps, reward Average: -198.93333333333334, up to now: minReward: -184.0, minAverage: -198.85714285714286\n",
      "Episode 15 done after 190 steps, reward Average: -198.375, up to now: minReward: -184.0, minAverage: -198.375\n",
      "Episode 16 done after 200 steps, reward Average: -198.47058823529412, up to now: minReward: -184.0, minAverage: -198.375\n",
      "Episode 17 done after 180 steps, reward Average: -197.44444444444446, up to now: minReward: -180.0, minAverage: -197.44444444444446\n",
      "Episode 18 done after 178 steps, reward Average: -196.42105263157896, up to now: minReward: -178.0, minAverage: -196.42105263157896\n",
      "Episode 19 done after 200 steps, reward Average: -196.6, up to now: minReward: -178.0, minAverage: -196.42105263157896\n",
      "Episode 20 done after 200 steps, reward Average: -196.76190476190476, up to now: minReward: -178.0, minAverage: -196.42105263157896\n",
      "Episode 21 done after 183 steps, reward Average: -196.13636363636363, up to now: minReward: -178.0, minAverage: -196.13636363636363\n",
      "Episode 22 done after 169 steps, reward Average: -194.95652173913044, up to now: minReward: -169.0, minAverage: -194.95652173913044\n",
      "Episode 23 done after 171 steps, reward Average: -193.95833333333334, up to now: minReward: -169.0, minAverage: -193.95833333333334\n",
      "Episode 24 done after 173 steps, reward Average: -193.12, up to now: minReward: -169.0, minAverage: -193.12\n",
      "Episode 25 done after 200 steps, reward Average: -193.3846153846154, up to now: minReward: -169.0, minAverage: -193.12\n",
      "Episode 26 done after 168 steps, reward Average: -192.44444444444446, up to now: minReward: -168.0, minAverage: -192.44444444444446\n",
      "Episode 27 done after 170 steps, reward Average: -191.64285714285714, up to now: minReward: -168.0, minAverage: -191.64285714285714\n",
      "Episode 28 done after 168 steps, reward Average: -190.82758620689654, up to now: minReward: -168.0, minAverage: -190.82758620689654\n",
      "Episode 29 done after 166 steps, reward Average: -190.0, up to now: minReward: -166.0, minAverage: -190.0\n",
      "Episode 30 done after 166 steps, reward Average: -189.2258064516129, up to now: minReward: -166.0, minAverage: -189.2258064516129\n",
      "Episode 31 done after 167 steps, reward Average: -188.53125, up to now: minReward: -166.0, minAverage: -188.53125\n",
      "Episode 32 done after 165 steps, reward Average: -187.8181818181818, up to now: minReward: -165.0, minAverage: -187.8181818181818\n",
      "Episode 33 done after 164 steps, reward Average: -187.11764705882354, up to now: minReward: -164.0, minAverage: -187.11764705882354\n",
      "Episode 34 done after 170 steps, reward Average: -186.62857142857143, up to now: minReward: -164.0, minAverage: -186.62857142857143\n",
      "Episode 35 done after 165 steps, reward Average: -186.02777777777777, up to now: minReward: -164.0, minAverage: -186.02777777777777\n",
      "Episode 36 done after 164 steps, reward Average: -185.43243243243242, up to now: minReward: -164.0, minAverage: -185.43243243243242\n",
      "Episode 37 done after 167 steps, reward Average: -184.94736842105263, up to now: minReward: -164.0, minAverage: -184.94736842105263\n",
      "Episode 38 done after 165 steps, reward Average: -184.43589743589743, up to now: minReward: -164.0, minAverage: -184.43589743589743\n",
      "Episode 39 done after 163 steps, reward Average: -183.9, up to now: minReward: -163.0, minAverage: -183.9\n",
      "Episode 40 done after 165 steps, reward Average: -183.4390243902439, up to now: minReward: -163.0, minAverage: -183.4390243902439\n",
      "Episode 41 done after 163 steps, reward Average: -182.95238095238096, up to now: minReward: -163.0, minAverage: -182.95238095238096\n",
      "Episode 42 done after 165 steps, reward Average: -182.53488372093022, up to now: minReward: -163.0, minAverage: -182.53488372093022\n",
      "Episode 43 done after 200 steps, reward Average: -182.9318181818182, up to now: minReward: -163.0, minAverage: -182.53488372093022\n",
      "Episode 44 done after 164 steps, reward Average: -182.51111111111112, up to now: minReward: -163.0, minAverage: -182.51111111111112\n",
      "Episode 45 done after 163 steps, reward Average: -182.08695652173913, up to now: minReward: -163.0, minAverage: -182.08695652173913\n",
      "Episode 46 done after 200 steps, reward Average: -182.46808510638297, up to now: minReward: -163.0, minAverage: -182.08695652173913\n",
      "Episode 47 done after 162 steps, reward Average: -182.04166666666666, up to now: minReward: -162.0, minAverage: -182.04166666666666\n",
      "Episode 48 done after 174 steps, reward Average: -181.87755102040816, up to now: minReward: -162.0, minAverage: -181.87755102040816\n",
      "Episode 49 done after 169 steps, reward Average: -181.62, up to now: minReward: -162.0, minAverage: -181.62\n",
      "final result: \n",
      "30 times arrived in 50 episodes, first time in episode 13\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEICAYAAABMGMOEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e25aa5f518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
