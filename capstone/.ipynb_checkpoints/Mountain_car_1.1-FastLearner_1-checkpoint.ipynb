{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New Master, to check stability #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.2)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-06-01 22:22:21,022] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configuration of framework and evaluation\n",
    "nbCycles = 4\n",
    "nbEpisodes = 50\n",
    "nbTimesteps = 200\n",
    "printEpisodeResult=False\n",
    "\n",
    "# Policy parameter setting for the run\n",
    "policySARSA=True             #T: SARSA, F: QLearning\n",
    "\n",
    "# strategies: -1: static, 0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "policyP = {\n",
    "    # Epsilon for epsilon-greedy\n",
    "    'epsilon':{'strategy':-1, 'static':0.1, 'start':0.1, 'base':0.001, 'interval':200},\n",
    "    # Discount factor for Q(S',A')\n",
    "    'gamma'  :{'strategy':-1, 'static':0.9,  'start':1.0, 'base':0.5,   'interval':200},\n",
    "    # learning rate\n",
    "    'alpha'  :{'strategy':-1, 'static':0.5,  'start':1.0, 'base':0.2,   'interval':200}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation reset\n",
      "evaluation initialized\n"
     ]
    }
   ],
   "source": [
    "class evaluation:\n",
    "    def __init__(self, nbEpisodes, nbTimesteps):\n",
    "        self.nbTimesteps=nbTimesteps\n",
    "        self.episodesEvaluated=100\n",
    "        if nbEpisodes < self.episodesEvaluated:\n",
    "            self.episodesEvaluated=int(nbEpisodes/2)+1\n",
    "        self.rewardLimit=-110\n",
    "\n",
    "        self.cycleFirstArrival=[]\n",
    "        self.cycleArrivedNb=[]\n",
    "        self.cycleArrivePrct=[]\n",
    "        self.cycleRewardMax=[]\n",
    "        self.cycleRewardMaxEpi=[]\n",
    "        self.cycleAverageMax=[]\n",
    "        self.cycleAverageMaxEpi=[]\n",
    "        self.cycleRewAverages=[]\n",
    "        self.cycleProblemSolved=[]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.reset()\n",
    "        print ('evaluation initialized')\n",
    "        \n",
    "    def reset(self):\n",
    "        self.lastEpisode=-1\n",
    "        self.arrivedNb=0                 #how often did the car arrive\n",
    "        self.arrivedFirst=0              #which episode for the first time \n",
    "        self.rewardMax=-200              #max reward seen\n",
    "        self.rewardMaxEpisode=0          #in which episode did we see the max reward\n",
    "        self.rewardTracker = np.zeros(self.episodesEvaluated)\n",
    "        self.rewardAverages=[]\n",
    "        self.problemSolved=False\n",
    "        self.averageMax=-200\n",
    "        self.averageMaxEpisode=0\n",
    "        self.lastAverage=-200\n",
    "        print ('evaluation reset')\n",
    "        \n",
    "    def EpiTrack (self,episode, step, rwdAcc):\n",
    "        self.lastEpisode=episode\n",
    "        if rwdAcc>self.rewardMax:\n",
    "            self.rewardMax=rwdAcc\n",
    "            self.rewardMaxEpisode=episode\n",
    "            \n",
    "        self.rewardTracker[episode%self.episodesEvaluated]=rwdAcc\n",
    "        \n",
    "        if episode>self.episodesEvaluated:\n",
    "            self.lastAverage=np.average(self.rewardTracker)\n",
    "        else:\n",
    "            self.lastAverage=np.average(self.rewardTracker[0:episode+1])\n",
    "            \n",
    "        self.rewardAverages.append(self.lastAverage)\n",
    "        if self.lastAverage>=self.rewardLimit:\n",
    "            self.problemSolved=True\n",
    "            \n",
    "        if self.lastAverage>self.averageMax:\n",
    "            self.averageMax=self.lastAverage\n",
    "            self.averageMaxEpisode=episode\n",
    "            \n",
    "        if step < self.nbTimesteps-1:\n",
    "            self.arrivedNb+=1\n",
    "            if self.arrivedFirst==0:\n",
    "                self.arrivedFirst=episode\n",
    "                \n",
    "    def EpiEval(self,episode,step):\n",
    "        print('\\rEpisode {} done: steps: {}, r-average: {:.1f}, arrived {}({:.1%}), maxReward: {} in {}, best average: {:.1f} in {}'.\\\n",
    "              format(episode, step+1, self.lastAverage, self.arrivedNb,self.arrivedNb/(episode+1), \\\n",
    "                     self.rewardMax, self.rewardMaxEpisode, self.averageMax,self.averageMaxEpisode),\\\n",
    "              end='')\n",
    "        \n",
    "    def CycleStart(self,i):\n",
    "        print ('cycle {} starts:'.format(i))\n",
    "        \n",
    "    def CycleTrack(self):\n",
    "        self.cycleFirstArrival.append(self.arrivedFirst)\n",
    "        self.cycleArrivedNb.append(self.arrivedNb)\n",
    "        self.cycleArrivePrct.append(self.arrivedNb/(self.lastEpisode+1))\n",
    "        self.cycleRewardMax.append(self.rewardMax)\n",
    "        self.cycleRewardMaxEpi.append(self.rewardMaxEpisode)\n",
    "        self.cycleAverageMax.append(self.averageMax)\n",
    "        self.cycleAverageMaxEpi.append(self.averageMaxEpisode)\n",
    "        self.cycleRewAverages.append(self.rewardAverages)\n",
    "        self.cycleProblemSolved.append(self.problemSolved)\n",
    "        \n",
    "    def CycleEval(self,i):\n",
    "        print ('\\n\\ncycle {} done:'.format(i))\n",
    "        print ('number episodes: {}'.format(self.lastEpisode+1))\n",
    "        print ('first time arrived in episode: {}'.format(self.arrivedFirst))\n",
    "        print ('car arrived: {} ({:.1%})'.format(self.arrivedNb, self.arrivedNb/(self.lastEpisode+1)))\n",
    "        print ('best reward: {} in episode {}'.format(self.rewardMax,self.rewardMaxEpisode))\n",
    "        print ('best average: {:.1f} in episode {}'.format(self.averageMax, self.averageMaxEpisode))\n",
    "        print ('problem solved:{}'.format(self.problemSolved))\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(range(len(self.rewardAverages)),self.rewardAverages,'-')\n",
    "        ax.set_title(\"development of rewardAverages\")\n",
    "        plt.show()\n",
    "        \n",
    "    def TestEval(self,nbCycles):\n",
    "        print ('test finished after {} runs'.format(nbCycles))\n",
    "        print ('first arrival: mean: {:.0f}, std: {:.1f}'.format(np.mean(self.cycleFirstArrival), np.std(self.cycleFirstArrival)))\n",
    "        print ('nb of arrival: mean: {:.0f}, std: {:.1f}'.format(np.mean(self.cycleArrivedNb), np.std(self.cycleArrivedNb)))\n",
    "        print ('nb of arrival: mean: {:.0%}, std: {:.1%}'.format(np.mean(self.cycleArrivePrct), np.std(self.cycleArrivePrct)))\n",
    "        print ('best reward  : mean: {:.0f}, std: {:.1f}'.format(np.mean(self.cycleRewardMax), np.std(self.cycleRewardMax)))\n",
    "        print ('in episode   : mean: {:.0f}, std: {:.1f}'.format(np.mean(self.cycleRewardMaxEpi), np.std(self.cycleRewardMaxEpi)))\n",
    "        print ('best average : mean: {:.0f}, std: {:.1f}'.format(np.mean(self.cycleAverageMax), np.std(self.cycleAverageMax)))\n",
    "        print ('in episode   : mean: {:.0f}, std: {:.1f}'.format(np.mean(self.cycleAverageMaxEpi), np.std(self.cycleAverageMaxEpi)))\n",
    "        print ('prblem solved: mean: {:.0f}, std: {:.1f}'.format(np.mean(self.cycleProblemSolved), np.std(self.cycleProblemSolved)))\n",
    "               \n",
    "        #self.cycleRewAverages.append(self.rewardAverages)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        for avgs in self.cycleRewAverages:\n",
    "            ax.plot(range(len(avgs)),avgs,'-')\n",
    "        ax.set_title(\"development of rewardAverages\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "evaluation=evaluation(nbEpisodes,nbTimesteps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation reset\n",
      "cycle 0 starts:\n",
      "Episode 6 done: steps: 200, r-average: -200.0, arrived 0(0.0%), maxReward: -200 in 0, best average: -200.0 in 0"
     ]
    }
   ],
   "source": [
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, pName):\n",
    "    global nbEpisodes,policyP\n",
    "    param=policyP[pName]\n",
    "    if param['strategy']==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif param['strategy'] ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif param['strategy'] ==2:\n",
    "        return (0.5)**episode\n",
    "    elif param['strategy'] ==3:\n",
    "        return 0.05\n",
    "    elif param['strategy'] ==4:\n",
    "        return max(param['start']-episode/param['interval'], param['base'])\n",
    "    elif param['strategy'] ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif param['strategy'] ==6:       #slow learner\n",
    "        if episode < 20:\n",
    "            return 0.5\n",
    "        elif episode < 100:\n",
    "            return 0.25\n",
    "        else:\n",
    "            return 0.1\n",
    "    elif param['strategy'] ==7:              #fast learner\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return param['static']\n",
    "    \n",
    "for cycle in range(nbCycles):\n",
    "    #old evaluation\n",
    "    lastDelta=-1000.0\n",
    "    colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "\n",
    "    #prepare cycle\n",
    "    tileModel.resetStates()\n",
    "    evaluation.reset()\n",
    "    evaluation.CycleStart(cycle)\n",
    "\n",
    "    for episode in range(nbEpisodes):\n",
    "    \n",
    "        # decay - if administrated the parameters\n",
    "        epsilon=decayFunction(episode, 'epsilon')\n",
    "        gamma=decayFunction(episode, 'gamma')\n",
    "        alpha=decayFunction(episode, 'alpha')\n",
    "        \n",
    "        # prepare the model\n",
    "        state = env.reset()\n",
    "        Q=tileModel.getQ(state)\n",
    "        action = policy(Q,epsilon)\n",
    "\n",
    "        #prepare evaluation\n",
    "        rewardAccumulated =0\n",
    "        deltaQAs=[0.0]\n",
    "   \n",
    "        for t in range(nbTimesteps):\n",
    "            env.render()\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            rewardAccumulated+=reward\n",
    "            ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "            if done:\n",
    "                evaluation.EpiTrack(episode,t,rewardAccumulated)\n",
    "                evaluation.EpiEval(episode,t)\n",
    "                break\n",
    "\n",
    "            #update Q(S,A)-Table according to:\n",
    "            #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "            #start with the information from the old state:\n",
    "            #difference between Q(S,a) and actual reward\n",
    "            #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "            #now we choose the next state/action pair:\n",
    "            Q_next=tileModel.getQ(state_next)\n",
    "            action_next=policy(Q_next,epsilon)\n",
    "            action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "            if policySARSA:\n",
    "                action_next_learning=action_next\n",
    "            else:\n",
    "                action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "            # take into account the value of the next (Q(S',A') and update the tile model\n",
    "            deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "            deltaQAs.append(deltaQA)\n",
    "        \n",
    "            ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "            ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "            ### print ('deltaQA:',deltaQA)\n",
    "            tileModel.updateQ(state, action, deltaQA)\n",
    "            ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "            ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "            ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "            ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "            ###lastDelta=deltaQA\n",
    "        \n",
    "            # prepare next round:\n",
    "            state=state_next\n",
    "            action=action_next\n",
    "            Q=Q_next\n",
    "               \n",
    "        if printEpisodeResult:\n",
    "            #evaluation of episode: development of deltaQA\n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_subplot(111)\n",
    "            ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "            ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "            plt.show()\n",
    "\n",
    "            #evaluation of episode: states\n",
    "            plotInput=tileModel.preparePlot()\n",
    "            ### print ('states:', tileModel.states)\n",
    "            ### print ('plotInput:', plotInput)\n",
    "    \n",
    "            fig = plt.figure()\n",
    "            ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "            x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "            y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "            yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "            xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "            colors=['r','b','g']\n",
    "            labels=['back','neutral','forward']\n",
    "            for i in range(tileModel.nbActions):\n",
    "                ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "            ax2.set_xlabel('velocity')\n",
    "            ax2.set_ylabel('position')\n",
    "            ax2.set_zlabel('action')\n",
    "            ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "            ax2.legend()\n",
    "            plt.show()\n",
    "\n",
    "            #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "            minVal=np.zeros(3)\n",
    "            minCoo=np.empty([3,2])\n",
    "            maxVal=np.zeros(3)\n",
    "            maxCoo=np.empty([3,2])\n",
    "            for ix in x:\n",
    "                for iy in y:\n",
    "                    if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                        colorplot[ix,iy]='g'\n",
    "                    elif colorplot[ix,iy]=='g':\n",
    "                        colorplot[ix,iy]='blue'\n",
    "                    else:\n",
    "                        colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "            xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "            yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax3 = fig.add_subplot(111)\n",
    "    \n",
    "            for i in x:\n",
    "                ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "            #ax3.set_xticklabels(xUnscaled)\n",
    "            #ax3.set_yticklabels(yUnscaled)\n",
    "            ax3.set_xlabel('velocity')\n",
    "            ax3.set_ylabel('position')\n",
    "            ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "            plt.show()\n",
    "        \n",
    "    evaluation.CycleTrack()\n",
    "    evaluation.CycleEval(cycle)\n",
    "    \n",
    "evaluation.TestEval(nbCycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
