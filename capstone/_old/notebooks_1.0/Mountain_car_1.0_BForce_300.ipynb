{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Brute Force version, 300 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 12:50:32,309] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 162 steps, reward Average: -197.28571428571428, up to now: minReward: -162.0, minAverage: -197.28571428571428\n",
      "Episode 14 done after 200 steps, reward Average: -197.46666666666667, up to now: minReward: -162.0, minAverage: -197.28571428571428\n",
      "Episode 15 done after 200 steps, reward Average: -197.625, up to now: minReward: -162.0, minAverage: -197.28571428571428\n",
      "Episode 16 done after 167 steps, reward Average: -195.8235294117647, up to now: minReward: -162.0, minAverage: -195.8235294117647\n",
      "Episode 17 done after 200 steps, reward Average: -196.05555555555554, up to now: minReward: -162.0, minAverage: -195.8235294117647\n",
      "Episode 18 done after 151 steps, reward Average: -193.68421052631578, up to now: minReward: -151.0, minAverage: -193.68421052631578\n",
      "Episode 19 done after 151 steps, reward Average: -191.55, up to now: minReward: -151.0, minAverage: -191.55\n",
      "Episode 20 done after 152 steps, reward Average: -189.66666666666666, up to now: minReward: -151.0, minAverage: -189.66666666666666\n",
      "Episode 21 done after 145 steps, reward Average: -187.63636363636363, up to now: minReward: -145.0, minAverage: -187.63636363636363\n",
      "Episode 22 done after 144 steps, reward Average: -185.7391304347826, up to now: minReward: -144.0, minAverage: -185.7391304347826\n",
      "Episode 23 done after 140 steps, reward Average: -183.83333333333334, up to now: minReward: -140.0, minAverage: -183.83333333333334\n",
      "Episode 24 done after 142 steps, reward Average: -182.16, up to now: minReward: -140.0, minAverage: -182.16\n",
      "Episode 25 done after 142 steps, reward Average: -180.6153846153846, up to now: minReward: -140.0, minAverage: -180.6153846153846\n",
      "Episode 26 done after 140 steps, reward Average: -179.11111111111111, up to now: minReward: -140.0, minAverage: -179.11111111111111\n",
      "Episode 27 done after 138 steps, reward Average: -177.64285714285714, up to now: minReward: -138.0, minAverage: -177.64285714285714\n",
      "Episode 28 done after 137 steps, reward Average: -176.24137931034483, up to now: minReward: -137.0, minAverage: -176.24137931034483\n",
      "Episode 29 done after 137 steps, reward Average: -174.93333333333334, up to now: minReward: -137.0, minAverage: -174.93333333333334\n",
      "Episode 30 done after 137 steps, reward Average: -173.70967741935485, up to now: minReward: -137.0, minAverage: -173.70967741935485\n",
      "Episode 31 done after 138 steps, reward Average: -172.59375, up to now: minReward: -137.0, minAverage: -172.59375\n",
      "Episode 32 done after 137 steps, reward Average: -171.5151515151515, up to now: minReward: -137.0, minAverage: -171.5151515151515\n",
      "Episode 33 done after 139 steps, reward Average: -170.55882352941177, up to now: minReward: -137.0, minAverage: -170.55882352941177\n",
      "Episode 34 done after 137 steps, reward Average: -169.6, up to now: minReward: -137.0, minAverage: -169.6\n",
      "Episode 35 done after 135 steps, reward Average: -168.63888888888889, up to now: minReward: -135.0, minAverage: -168.63888888888889\n",
      "Episode 36 done after 135 steps, reward Average: -167.72972972972974, up to now: minReward: -135.0, minAverage: -167.72972972972974\n",
      "Episode 37 done after 139 steps, reward Average: -166.97368421052633, up to now: minReward: -135.0, minAverage: -166.97368421052633\n",
      "Episode 38 done after 132 steps, reward Average: -166.07692307692307, up to now: minReward: -132.0, minAverage: -166.07692307692307\n",
      "Episode 39 done after 138 steps, reward Average: -165.375, up to now: minReward: -132.0, minAverage: -165.375\n",
      "Episode 40 done after 135 steps, reward Average: -164.6341463414634, up to now: minReward: -132.0, minAverage: -164.6341463414634\n",
      "Episode 41 done after 134 steps, reward Average: -163.9047619047619, up to now: minReward: -132.0, minAverage: -163.9047619047619\n",
      "Episode 42 done after 134 steps, reward Average: -163.2093023255814, up to now: minReward: -132.0, minAverage: -163.2093023255814\n",
      "Episode 43 done after 136 steps, reward Average: -162.5909090909091, up to now: minReward: -132.0, minAverage: -162.5909090909091\n",
      "Episode 44 done after 135 steps, reward Average: -161.9777777777778, up to now: minReward: -132.0, minAverage: -161.9777777777778\n",
      "Episode 45 done after 132 steps, reward Average: -161.32608695652175, up to now: minReward: -132.0, minAverage: -161.32608695652175\n",
      "Episode 46 done after 136 steps, reward Average: -160.7872340425532, up to now: minReward: -132.0, minAverage: -160.7872340425532\n",
      "Episode 47 done after 134 steps, reward Average: -160.22916666666666, up to now: minReward: -132.0, minAverage: -160.22916666666666\n",
      "Episode 48 done after 132 steps, reward Average: -159.6530612244898, up to now: minReward: -132.0, minAverage: -159.6530612244898\n",
      "Episode 49 done after 134 steps, reward Average: -159.14, up to now: minReward: -132.0, minAverage: -159.14\n",
      "Episode 50 done after 137 steps, reward Average: -158.7058823529412, up to now: minReward: -132.0, minAverage: -158.7058823529412\n",
      "Episode 51 done after 131 steps, reward Average: -158.17307692307693, up to now: minReward: -131.0, minAverage: -158.17307692307693\n",
      "Episode 52 done after 136 steps, reward Average: -157.75471698113208, up to now: minReward: -131.0, minAverage: -157.75471698113208\n",
      "Episode 53 done after 132 steps, reward Average: -157.27777777777777, up to now: minReward: -131.0, minAverage: -157.27777777777777\n",
      "Episode 54 done after 132 steps, reward Average: -156.8181818181818, up to now: minReward: -131.0, minAverage: -156.8181818181818\n",
      "Episode 55 done after 131 steps, reward Average: -156.35714285714286, up to now: minReward: -131.0, minAverage: -156.35714285714286\n",
      "Episode 56 done after 135 steps, reward Average: -155.98245614035088, up to now: minReward: -131.0, minAverage: -155.98245614035088\n",
      "Episode 57 done after 132 steps, reward Average: -155.56896551724137, up to now: minReward: -131.0, minAverage: -155.56896551724137\n",
      "Episode 58 done after 137 steps, reward Average: -155.25423728813558, up to now: minReward: -131.0, minAverage: -155.25423728813558\n",
      "Episode 59 done after 133 steps, reward Average: -154.88333333333333, up to now: minReward: -131.0, minAverage: -154.88333333333333\n",
      "Episode 60 done after 132 steps, reward Average: -154.50819672131146, up to now: minReward: -131.0, minAverage: -154.50819672131146\n",
      "Episode 61 done after 134 steps, reward Average: -154.17741935483872, up to now: minReward: -131.0, minAverage: -154.17741935483872\n",
      "Episode 62 done after 132 steps, reward Average: -153.82539682539684, up to now: minReward: -131.0, minAverage: -153.82539682539684\n",
      "Episode 63 done after 133 steps, reward Average: -153.5, up to now: minReward: -131.0, minAverage: -153.5\n",
      "Episode 64 done after 134 steps, reward Average: -153.2, up to now: minReward: -131.0, minAverage: -153.2\n",
      "Episode 65 done after 132 steps, reward Average: -152.87878787878788, up to now: minReward: -131.0, minAverage: -152.87878787878788\n",
      "Episode 66 done after 134 steps, reward Average: -152.59701492537314, up to now: minReward: -131.0, minAverage: -152.59701492537314\n",
      "Episode 67 done after 128 steps, reward Average: -152.23529411764707, up to now: minReward: -128.0, minAverage: -152.23529411764707\n",
      "Episode 68 done after 136 steps, reward Average: -152.0, up to now: minReward: -128.0, minAverage: -152.0\n",
      "Episode 69 done after 128 steps, reward Average: -151.65714285714284, up to now: minReward: -128.0, minAverage: -151.65714285714284\n",
      "Episode 70 done after 135 steps, reward Average: -151.42253521126761, up to now: minReward: -128.0, minAverage: -151.42253521126761\n",
      "Episode 71 done after 135 steps, reward Average: -151.19444444444446, up to now: minReward: -128.0, minAverage: -151.19444444444446\n",
      "Episode 72 done after 137 steps, reward Average: -151.0, up to now: minReward: -128.0, minAverage: -151.0\n",
      "Episode 73 done after 134 steps, reward Average: -150.77027027027026, up to now: minReward: -128.0, minAverage: -150.77027027027026\n",
      "Episode 74 done after 136 steps, reward Average: -150.57333333333332, up to now: minReward: -128.0, minAverage: -150.57333333333332\n",
      "Episode 75 done after 136 steps, reward Average: -150.3815789473684, up to now: minReward: -128.0, minAverage: -150.3815789473684\n",
      "Episode 76 done after 132 steps, reward Average: -150.14285714285714, up to now: minReward: -128.0, minAverage: -150.14285714285714\n",
      "Episode 77 done after 131 steps, reward Average: -149.89743589743588, up to now: minReward: -128.0, minAverage: -149.89743589743588\n",
      "Episode 78 done after 132 steps, reward Average: -149.67088607594937, up to now: minReward: -128.0, minAverage: -149.67088607594937\n",
      "Episode 79 done after 136 steps, reward Average: -149.5, up to now: minReward: -128.0, minAverage: -149.5\n",
      "Episode 80 done after 133 steps, reward Average: -149.2962962962963, up to now: minReward: -128.0, minAverage: -149.2962962962963\n",
      "Episode 81 done after 125 steps, reward Average: -149.0, up to now: minReward: -125.0, minAverage: -149.0\n",
      "Episode 82 done after 134 steps, reward Average: -148.81927710843374, up to now: minReward: -125.0, minAverage: -148.81927710843374\n",
      "Episode 83 done after 131 steps, reward Average: -148.60714285714286, up to now: minReward: -125.0, minAverage: -148.60714285714286\n",
      "Episode 84 done after 131 steps, reward Average: -148.4, up to now: minReward: -125.0, minAverage: -148.4\n",
      "Episode 85 done after 137 steps, reward Average: -148.2674418604651, up to now: minReward: -125.0, minAverage: -148.2674418604651\n",
      "Episode 86 done after 130 steps, reward Average: -148.05747126436782, up to now: minReward: -125.0, minAverage: -148.05747126436782\n",
      "Episode 87 done after 132 steps, reward Average: -147.875, up to now: minReward: -125.0, minAverage: -147.875\n",
      "Episode 88 done after 134 steps, reward Average: -147.7191011235955, up to now: minReward: -125.0, minAverage: -147.7191011235955\n",
      "Episode 89 done after 132 steps, reward Average: -147.54444444444445, up to now: minReward: -125.0, minAverage: -147.54444444444445\n",
      "Episode 90 done after 132 steps, reward Average: -147.37362637362637, up to now: minReward: -125.0, minAverage: -147.37362637362637\n",
      "Episode 91 done after 132 steps, reward Average: -147.20652173913044, up to now: minReward: -125.0, minAverage: -147.20652173913044\n",
      "Episode 92 done after 131 steps, reward Average: -147.03225806451613, up to now: minReward: -125.0, minAverage: -147.03225806451613\n",
      "Episode 93 done after 126 steps, reward Average: -146.80851063829786, up to now: minReward: -125.0, minAverage: -146.80851063829786\n",
      "Episode 94 done after 133 steps, reward Average: -146.66315789473686, up to now: minReward: -125.0, minAverage: -146.66315789473686\n",
      "Episode 95 done after 132 steps, reward Average: -146.51041666666666, up to now: minReward: -125.0, minAverage: -146.51041666666666\n",
      "Episode 96 done after 133 steps, reward Average: -146.37113402061857, up to now: minReward: -125.0, minAverage: -146.37113402061857\n",
      "Episode 97 done after 136 steps, reward Average: -146.26530612244898, up to now: minReward: -125.0, minAverage: -146.26530612244898\n",
      "Episode 98 done after 137 steps, reward Average: -146.17171717171718, up to now: minReward: -125.0, minAverage: -146.17171717171718\n",
      "Episode 99 done after 128 steps, reward Average: -145.99, up to now: minReward: -125.0, minAverage: -145.99\n",
      "Episode 100 done after 131 steps, reward Average: -145.3, up to now: minReward: -125.0, minAverage: -145.3\n",
      "Episode 101 done after 136 steps, reward Average: -144.66, up to now: minReward: -125.0, minAverage: -144.66\n",
      "Episode 102 done after 129 steps, reward Average: -143.95, up to now: minReward: -125.0, minAverage: -143.95\n",
      "Episode 103 done after 132 steps, reward Average: -143.27, up to now: minReward: -125.0, minAverage: -143.27\n",
      "Episode 104 done after 136 steps, reward Average: -142.63, up to now: minReward: -125.0, minAverage: -142.63\n",
      "Episode 105 done after 135 steps, reward Average: -141.98, up to now: minReward: -125.0, minAverage: -141.98\n",
      "Episode 106 done after 131 steps, reward Average: -141.29, up to now: minReward: -125.0, minAverage: -141.29\n",
      "Episode 107 done after 126 steps, reward Average: -140.55, up to now: minReward: -125.0, minAverage: -140.55\n",
      "Episode 108 done after 131 steps, reward Average: -139.86, up to now: minReward: -125.0, minAverage: -139.86\n",
      "Episode 109 done after 131 steps, reward Average: -139.17, up to now: minReward: -125.0, minAverage: -139.17\n",
      "Episode 110 done after 131 steps, reward Average: -138.48, up to now: minReward: -125.0, minAverage: -138.48\n",
      "Episode 111 done after 134 steps, reward Average: -137.82, up to now: minReward: -125.0, minAverage: -137.82\n",
      "Episode 112 done after 133 steps, reward Average: -137.15, up to now: minReward: -125.0, minAverage: -137.15\n",
      "Episode 113 done after 137 steps, reward Average: -136.9, up to now: minReward: -125.0, minAverage: -136.9\n",
      "Episode 114 done after 136 steps, reward Average: -136.26, up to now: minReward: -125.0, minAverage: -136.26\n",
      "Episode 115 done after 131 steps, reward Average: -135.57, up to now: minReward: -125.0, minAverage: -135.57\n",
      "Episode 116 done after 135 steps, reward Average: -135.25, up to now: minReward: -125.0, minAverage: -135.25\n",
      "Episode 117 done after 132 steps, reward Average: -134.57, up to now: minReward: -125.0, minAverage: -134.57\n",
      "Episode 118 done after 126 steps, reward Average: -134.32, up to now: minReward: -125.0, minAverage: -134.32\n",
      "Episode 119 done after 136 steps, reward Average: -134.17, up to now: minReward: -125.0, minAverage: -134.17\n",
      "Episode 120 done after 135 steps, reward Average: -134.0, up to now: minReward: -125.0, minAverage: -134.0\n",
      "Episode 121 done after 137 steps, reward Average: -133.92, up to now: minReward: -125.0, minAverage: -133.92\n",
      "Episode 122 done after 131 steps, reward Average: -133.79, up to now: minReward: -125.0, minAverage: -133.79\n",
      "Episode 123 done after 130 steps, reward Average: -133.69, up to now: minReward: -125.0, minAverage: -133.69\n",
      "Episode 124 done after 136 steps, reward Average: -133.63, up to now: minReward: -125.0, minAverage: -133.63\n",
      "Episode 125 done after 132 steps, reward Average: -133.53, up to now: minReward: -125.0, minAverage: -133.53\n",
      "Episode 126 done after 136 steps, reward Average: -133.49, up to now: minReward: -125.0, minAverage: -133.49\n",
      "Episode 127 done after 130 steps, reward Average: -133.41, up to now: minReward: -125.0, minAverage: -133.41\n",
      "Episode 128 done after 131 steps, reward Average: -133.35, up to now: minReward: -125.0, minAverage: -133.35\n",
      "Episode 129 done after 129 steps, reward Average: -133.27, up to now: minReward: -125.0, minAverage: -133.27\n",
      "Episode 130 done after 132 steps, reward Average: -133.22, up to now: minReward: -125.0, minAverage: -133.22\n",
      "Episode 131 done after 128 steps, reward Average: -133.12, up to now: minReward: -125.0, minAverage: -133.12\n",
      "Episode 132 done after 136 steps, reward Average: -133.11, up to now: minReward: -125.0, minAverage: -133.11\n",
      "Episode 133 done after 128 steps, reward Average: -133.0, up to now: minReward: -125.0, minAverage: -133.0\n",
      "Episode 134 done after 137 steps, reward Average: -133.0, up to now: minReward: -125.0, minAverage: -133.0\n",
      "Episode 135 done after 131 steps, reward Average: -132.96, up to now: minReward: -125.0, minAverage: -132.96\n",
      "Episode 136 done after 136 steps, reward Average: -132.97, up to now: minReward: -125.0, minAverage: -132.96\n",
      "Episode 137 done after 127 steps, reward Average: -132.85, up to now: minReward: -125.0, minAverage: -132.85\n",
      "Episode 138 done after 134 steps, reward Average: -132.87, up to now: minReward: -125.0, minAverage: -132.85\n",
      "Episode 139 done after 132 steps, reward Average: -132.81, up to now: minReward: -125.0, minAverage: -132.81\n",
      "Episode 140 done after 137 steps, reward Average: -132.83, up to now: minReward: -125.0, minAverage: -132.81\n",
      "Episode 141 done after 128 steps, reward Average: -132.77, up to now: minReward: -125.0, minAverage: -132.77\n",
      "Episode 142 done after 136 steps, reward Average: -132.79, up to now: minReward: -125.0, minAverage: -132.77\n",
      "Episode 143 done after 127 steps, reward Average: -132.7, up to now: minReward: -125.0, minAverage: -132.7\n",
      "Episode 144 done after 136 steps, reward Average: -132.71, up to now: minReward: -125.0, minAverage: -132.7\n",
      "Episode 145 done after 135 steps, reward Average: -132.74, up to now: minReward: -125.0, minAverage: -132.7\n",
      "Episode 146 done after 133 steps, reward Average: -132.71, up to now: minReward: -125.0, minAverage: -132.7\n",
      "Episode 147 done after 131 steps, reward Average: -132.68, up to now: minReward: -125.0, minAverage: -132.68\n",
      "Episode 148 done after 133 steps, reward Average: -132.69, up to now: minReward: -125.0, minAverage: -132.68\n",
      "Episode 149 done after 132 steps, reward Average: -132.67, up to now: minReward: -125.0, minAverage: -132.67\n",
      "Episode 150 done after 130 steps, reward Average: -132.6, up to now: minReward: -125.0, minAverage: -132.6\n",
      "Episode 151 done after 126 steps, reward Average: -132.55, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 152 done after 136 steps, reward Average: -132.55, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 153 done after 136 steps, reward Average: -132.59, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 154 done after 136 steps, reward Average: -132.63, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 155 done after 127 steps, reward Average: -132.59, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 156 done after 137 steps, reward Average: -132.61, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 157 done after 133 steps, reward Average: -132.62, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 158 done after 131 steps, reward Average: -132.56, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 159 done after 136 steps, reward Average: -132.59, up to now: minReward: -125.0, minAverage: -132.55\n",
      "Episode 160 done after 126 steps, reward Average: -132.53, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 161 done after 137 steps, reward Average: -132.56, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 162 done after 134 steps, reward Average: -132.58, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 163 done after 131 steps, reward Average: -132.56, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 164 done after 136 steps, reward Average: -132.58, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 165 done after 128 steps, reward Average: -132.54, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 166 done after 134 steps, reward Average: -132.54, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 167 done after 136 steps, reward Average: -132.62, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 168 done after 134 steps, reward Average: -132.6, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 169 done after 137 steps, reward Average: -132.69, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 170 done after 135 steps, reward Average: -132.69, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 171 done after 133 steps, reward Average: -132.67, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 172 done after 128 steps, reward Average: -132.58, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 173 done after 135 steps, reward Average: -132.59, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 174 done after 135 steps, reward Average: -132.58, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 175 done after 136 steps, reward Average: -132.58, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 176 done after 128 steps, reward Average: -132.54, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 177 done after 134 steps, reward Average: -132.57, up to now: minReward: -125.0, minAverage: -132.53\n",
      "Episode 178 done after 127 steps, reward Average: -132.52, up to now: minReward: -125.0, minAverage: -132.52\n",
      "Episode 179 done after 128 steps, reward Average: -132.44, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 180 done after 137 steps, reward Average: -132.48, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 181 done after 131 steps, reward Average: -132.54, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 182 done after 136 steps, reward Average: -132.56, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 183 done after 133 steps, reward Average: -132.58, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 184 done after 134 steps, reward Average: -132.61, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 185 done after 136 steps, reward Average: -132.6, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 186 done after 135 steps, reward Average: -132.65, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 187 done after 134 steps, reward Average: -132.67, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 188 done after 127 steps, reward Average: -132.6, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 189 done after 136 steps, reward Average: -132.64, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 190 done after 128 steps, reward Average: -132.6, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 191 done after 136 steps, reward Average: -132.64, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 192 done after 136 steps, reward Average: -132.69, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 193 done after 137 steps, reward Average: -132.8, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 194 done after 134 steps, reward Average: -132.81, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 195 done after 132 steps, reward Average: -132.81, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 196 done after 128 steps, reward Average: -132.76, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 197 done after 131 steps, reward Average: -132.71, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 198 done after 135 steps, reward Average: -132.69, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 199 done after 127 steps, reward Average: -132.68, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 200 done after 135 steps, reward Average: -132.72, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 201 done after 136 steps, reward Average: -132.72, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 202 done after 132 steps, reward Average: -132.75, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 203 done after 134 steps, reward Average: -132.77, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 204 done after 128 steps, reward Average: -132.69, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 205 done after 132 steps, reward Average: -132.66, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 206 done after 134 steps, reward Average: -132.69, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 207 done after 131 steps, reward Average: -132.74, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 208 done after 136 steps, reward Average: -132.79, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 209 done after 130 steps, reward Average: -132.78, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 210 done after 136 steps, reward Average: -132.83, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 211 done after 134 steps, reward Average: -132.83, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 212 done after 135 steps, reward Average: -132.85, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 213 done after 136 steps, reward Average: -132.84, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 214 done after 134 steps, reward Average: -132.82, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 215 done after 136 steps, reward Average: -132.87, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 216 done after 133 steps, reward Average: -132.85, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 217 done after 134 steps, reward Average: -132.87, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 218 done after 133 steps, reward Average: -132.94, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 219 done after 134 steps, reward Average: -132.92, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 220 done after 130 steps, reward Average: -132.87, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 221 done after 135 steps, reward Average: -132.85, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 222 done after 130 steps, reward Average: -132.84, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 223 done after 133 steps, reward Average: -132.87, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 224 done after 132 steps, reward Average: -132.83, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 225 done after 126 steps, reward Average: -132.77, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 226 done after 136 steps, reward Average: -132.77, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 227 done after 137 steps, reward Average: -132.84, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 228 done after 135 steps, reward Average: -132.88, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 229 done after 135 steps, reward Average: -132.94, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 230 done after 136 steps, reward Average: -132.98, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 231 done after 137 steps, reward Average: -133.07, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 232 done after 133 steps, reward Average: -133.04, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 233 done after 136 steps, reward Average: -133.12, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 234 done after 133 steps, reward Average: -133.08, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 235 done after 136 steps, reward Average: -133.13, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 236 done after 136 steps, reward Average: -133.13, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 237 done after 127 steps, reward Average: -133.13, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 238 done after 127 steps, reward Average: -133.06, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 239 done after 135 steps, reward Average: -133.09, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 240 done after 136 steps, reward Average: -133.08, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 241 done after 128 steps, reward Average: -133.08, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 242 done after 130 steps, reward Average: -133.02, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 243 done after 136 steps, reward Average: -133.11, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 244 done after 132 steps, reward Average: -133.07, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 245 done after 135 steps, reward Average: -133.07, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 246 done after 126 steps, reward Average: -133.0, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 247 done after 135 steps, reward Average: -133.04, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 248 done after 136 steps, reward Average: -133.07, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 249 done after 135 steps, reward Average: -133.1, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 250 done after 136 steps, reward Average: -133.16, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 251 done after 134 steps, reward Average: -133.24, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 252 done after 135 steps, reward Average: -133.23, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 253 done after 137 steps, reward Average: -133.24, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 254 done after 137 steps, reward Average: -133.25, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 255 done after 128 steps, reward Average: -133.26, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 256 done after 136 steps, reward Average: -133.25, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 257 done after 133 steps, reward Average: -133.25, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 258 done after 126 steps, reward Average: -133.2, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 259 done after 128 steps, reward Average: -133.12, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 260 done after 137 steps, reward Average: -133.23, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 261 done after 126 steps, reward Average: -133.12, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 262 done after 137 steps, reward Average: -133.15, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 263 done after 133 steps, reward Average: -133.17, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 264 done after 136 steps, reward Average: -133.17, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 265 done after 128 steps, reward Average: -133.17, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 266 done after 134 steps, reward Average: -133.17, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 267 done after 136 steps, reward Average: -133.17, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 268 done after 130 steps, reward Average: -133.13, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 269 done after 136 steps, reward Average: -133.12, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 270 done after 135 steps, reward Average: -133.12, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 271 done after 134 steps, reward Average: -133.13, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 272 done after 135 steps, reward Average: -133.2, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 273 done after 133 steps, reward Average: -133.18, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 274 done after 130 steps, reward Average: -133.13, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 275 done after 128 steps, reward Average: -133.05, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 276 done after 137 steps, reward Average: -133.14, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 277 done after 134 steps, reward Average: -133.14, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 278 done after 136 steps, reward Average: -133.23, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 279 done after 132 steps, reward Average: -133.27, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 280 done after 132 steps, reward Average: -133.22, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 281 done after 128 steps, reward Average: -133.19, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 282 done after 132 steps, reward Average: -133.15, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 283 done after 134 steps, reward Average: -133.16, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 284 done after 136 steps, reward Average: -133.18, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 285 done after 133 steps, reward Average: -133.15, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 286 done after 136 steps, reward Average: -133.16, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 287 done after 134 steps, reward Average: -133.16, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 288 done after 136 steps, reward Average: -133.25, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 289 done after 132 steps, reward Average: -133.21, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 290 done after 128 steps, reward Average: -133.21, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 291 done after 128 steps, reward Average: -133.13, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 292 done after 131 steps, reward Average: -133.08, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 293 done after 129 steps, reward Average: -133.0, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 294 done after 135 steps, reward Average: -133.01, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 295 done after 127 steps, reward Average: -132.96, up to now: minReward: -125.0, minAverage: -132.44\n",
      "Episode 296 done after 124 steps, reward Average: -132.92, up to now: minReward: -124.0, minAverage: -132.44\n",
      "Episode 297 done after 136 steps, reward Average: -132.97, up to now: minReward: -124.0, minAverage: -132.44\n",
      "Episode 298 done after 131 steps, reward Average: -132.93, up to now: minReward: -124.0, minAverage: -132.44\n",
      "Episode 299 done after 134 steps, reward Average: -133.0, up to now: minReward: -124.0, minAverage: -132.44\n",
      "final result: \n",
      "284 times arrived in 300 episodes, first time in episode 13\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZQMREQE+P/D3NkG9n3/TQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x155ea8204e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
