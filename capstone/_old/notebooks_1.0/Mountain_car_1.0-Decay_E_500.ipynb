{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling 8*8*8, Decay Epsilon - 500 Episodes#\n",
    "\n",
    "Base parameters:\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 1/(epsiode+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 11:48:54,633] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=1           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 18 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 19 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 20 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 21 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 22 done after 176 steps, reward Average: -198.95652173913044, up to now: minReward: -176.0, minAverage: -198.95652173913044\n",
      "Episode 23 done after 200 steps, reward Average: -199.0, up to now: minReward: -176.0, minAverage: -198.95652173913044\n",
      "Episode 24 done after 181 steps, reward Average: -198.28, up to now: minReward: -176.0, minAverage: -198.28\n",
      "Episode 25 done after 200 steps, reward Average: -198.34615384615384, up to now: minReward: -176.0, minAverage: -198.28\n",
      "Episode 26 done after 200 steps, reward Average: -198.40740740740742, up to now: minReward: -176.0, minAverage: -198.28\n",
      "Episode 27 done after 200 steps, reward Average: -198.46428571428572, up to now: minReward: -176.0, minAverage: -198.28\n",
      "Episode 28 done after 164 steps, reward Average: -197.27586206896552, up to now: minReward: -164.0, minAverage: -197.27586206896552\n",
      "Episode 29 done after 200 steps, reward Average: -197.36666666666667, up to now: minReward: -164.0, minAverage: -197.27586206896552\n",
      "Episode 30 done after 174 steps, reward Average: -196.61290322580646, up to now: minReward: -164.0, minAverage: -196.61290322580646\n",
      "Episode 31 done after 152 steps, reward Average: -195.21875, up to now: minReward: -152.0, minAverage: -195.21875\n",
      "Episode 32 done after 200 steps, reward Average: -195.36363636363637, up to now: minReward: -152.0, minAverage: -195.21875\n",
      "Episode 33 done after 200 steps, reward Average: -195.5, up to now: minReward: -152.0, minAverage: -195.21875\n",
      "Episode 34 done after 154 steps, reward Average: -194.31428571428572, up to now: minReward: -152.0, minAverage: -194.31428571428572\n",
      "Episode 35 done after 151 steps, reward Average: -193.11111111111111, up to now: minReward: -151.0, minAverage: -193.11111111111111\n",
      "Episode 36 done after 150 steps, reward Average: -191.94594594594594, up to now: minReward: -150.0, minAverage: -191.94594594594594\n",
      "Episode 37 done after 159 steps, reward Average: -191.07894736842104, up to now: minReward: -150.0, minAverage: -191.07894736842104\n",
      "Episode 38 done after 200 steps, reward Average: -191.30769230769232, up to now: minReward: -150.0, minAverage: -191.07894736842104\n",
      "Episode 39 done after 200 steps, reward Average: -191.525, up to now: minReward: -150.0, minAverage: -191.07894736842104\n",
      "Episode 40 done after 152 steps, reward Average: -190.5609756097561, up to now: minReward: -150.0, minAverage: -190.5609756097561\n",
      "Episode 41 done after 169 steps, reward Average: -190.04761904761904, up to now: minReward: -150.0, minAverage: -190.04761904761904\n",
      "Episode 42 done after 161 steps, reward Average: -189.37209302325581, up to now: minReward: -150.0, minAverage: -189.37209302325581\n",
      "Episode 43 done after 200 steps, reward Average: -189.61363636363637, up to now: minReward: -150.0, minAverage: -189.37209302325581\n",
      "Episode 44 done after 200 steps, reward Average: -189.84444444444443, up to now: minReward: -150.0, minAverage: -189.37209302325581\n",
      "Episode 45 done after 163 steps, reward Average: -189.2608695652174, up to now: minReward: -150.0, minAverage: -189.2608695652174\n",
      "Episode 46 done after 152 steps, reward Average: -188.46808510638297, up to now: minReward: -150.0, minAverage: -188.46808510638297\n",
      "Episode 47 done after 146 steps, reward Average: -187.58333333333334, up to now: minReward: -146.0, minAverage: -187.58333333333334\n",
      "Episode 48 done after 148 steps, reward Average: -186.77551020408163, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 49 done after 200 steps, reward Average: -187.04, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 50 done after 200 steps, reward Average: -187.2941176470588, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 51 done after 200 steps, reward Average: -187.53846153846155, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 52 done after 200 steps, reward Average: -187.77358490566039, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 53 done after 200 steps, reward Average: -188.0, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 54 done after 200 steps, reward Average: -188.21818181818182, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 55 done after 200 steps, reward Average: -188.42857142857142, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 56 done after 200 steps, reward Average: -188.6315789473684, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 57 done after 200 steps, reward Average: -188.82758620689654, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 58 done after 200 steps, reward Average: -189.01694915254237, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 59 done after 200 steps, reward Average: -189.2, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 60 done after 200 steps, reward Average: -189.37704918032787, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 61 done after 200 steps, reward Average: -189.5483870967742, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 62 done after 200 steps, reward Average: -189.71428571428572, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 63 done after 200 steps, reward Average: -189.875, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 64 done after 200 steps, reward Average: -190.03076923076924, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 65 done after 200 steps, reward Average: -190.1818181818182, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 66 done after 200 steps, reward Average: -190.32835820895522, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 67 done after 200 steps, reward Average: -190.47058823529412, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 68 done after 200 steps, reward Average: -190.6086956521739, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 69 done after 200 steps, reward Average: -190.74285714285713, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 70 done after 200 steps, reward Average: -190.8732394366197, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 71 done after 200 steps, reward Average: -191.0, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 72 done after 200 steps, reward Average: -191.12328767123287, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 73 done after 200 steps, reward Average: -191.24324324324326, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 74 done after 200 steps, reward Average: -191.36, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 75 done after 200 steps, reward Average: -191.47368421052633, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 76 done after 200 steps, reward Average: -191.58441558441558, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 77 done after 200 steps, reward Average: -191.69230769230768, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 78 done after 200 steps, reward Average: -191.79746835443038, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 79 done after 200 steps, reward Average: -191.9, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 80 done after 200 steps, reward Average: -192.0, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 81 done after 200 steps, reward Average: -192.09756097560975, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 82 done after 200 steps, reward Average: -192.19277108433735, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 83 done after 200 steps, reward Average: -192.28571428571428, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 84 done after 200 steps, reward Average: -192.3764705882353, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 85 done after 200 steps, reward Average: -192.46511627906978, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 86 done after 156 steps, reward Average: -192.04597701149424, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 87 done after 156 steps, reward Average: -191.63636363636363, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 88 done after 153 steps, reward Average: -191.20224719101122, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 89 done after 161 steps, reward Average: -190.86666666666667, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 90 done after 156 steps, reward Average: -190.4835164835165, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 91 done after 200 steps, reward Average: -190.58695652173913, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 92 done after 148 steps, reward Average: -190.1290322580645, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 93 done after 200 steps, reward Average: -190.2340425531915, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 94 done after 200 steps, reward Average: -190.33684210526314, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 95 done after 200 steps, reward Average: -190.4375, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 96 done after 149 steps, reward Average: -190.01030927835052, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 97 done after 200 steps, reward Average: -190.1122448979592, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 98 done after 200 steps, reward Average: -190.21212121212122, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 99 done after 166 steps, reward Average: -189.97, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 100 done after 146 steps, reward Average: -189.43, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 101 done after 200 steps, reward Average: -189.43, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 102 done after 155 steps, reward Average: -188.98, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 103 done after 200 steps, reward Average: -188.98, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 104 done after 200 steps, reward Average: -188.98, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 105 done after 200 steps, reward Average: -188.98, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 106 done after 200 steps, reward Average: -188.98, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 107 done after 150 steps, reward Average: -188.48, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 108 done after 200 steps, reward Average: -188.48, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 109 done after 200 steps, reward Average: -188.48, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 110 done after 200 steps, reward Average: -188.48, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 111 done after 200 steps, reward Average: -188.48, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 112 done after 200 steps, reward Average: -188.48, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 113 done after 151 steps, reward Average: -187.99, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 114 done after 147 steps, reward Average: -187.46, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 115 done after 148 steps, reward Average: -186.94, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 116 done after 200 steps, reward Average: -186.94, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 117 done after 200 steps, reward Average: -186.94, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 118 done after 200 steps, reward Average: -186.94, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 119 done after 200 steps, reward Average: -186.94, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 120 done after 200 steps, reward Average: -186.94, up to now: minReward: -146.0, minAverage: -186.77551020408163\n",
      "Episode 121 done after 150 steps, reward Average: -186.44, up to now: minReward: -146.0, minAverage: -186.44\n",
      "Episode 122 done after 200 steps, reward Average: -186.68, up to now: minReward: -146.0, minAverage: -186.44\n",
      "Episode 123 done after 200 steps, reward Average: -186.68, up to now: minReward: -146.0, minAverage: -186.44\n",
      "Episode 124 done after 200 steps, reward Average: -186.87, up to now: minReward: -146.0, minAverage: -186.44\n",
      "Episode 125 done after 200 steps, reward Average: -186.87, up to now: minReward: -146.0, minAverage: -186.44\n",
      "Episode 126 done after 150 steps, reward Average: -186.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 127 done after 200 steps, reward Average: -186.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 128 done after 200 steps, reward Average: -186.73, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 129 done after 200 steps, reward Average: -186.73, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 130 done after 200 steps, reward Average: -186.99, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 131 done after 148 steps, reward Average: -186.95, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 132 done after 200 steps, reward Average: -186.95, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 133 done after 200 steps, reward Average: -186.95, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 134 done after 200 steps, reward Average: -187.41, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 135 done after 200 steps, reward Average: -187.9, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 136 done after 200 steps, reward Average: -188.4, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 137 done after 200 steps, reward Average: -188.81, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 138 done after 200 steps, reward Average: -188.81, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 139 done after 200 steps, reward Average: -188.81, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 140 done after 200 steps, reward Average: -189.29, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 141 done after 200 steps, reward Average: -189.6, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 142 done after 152 steps, reward Average: -189.51, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 143 done after 148 steps, reward Average: -188.99, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 144 done after 200 steps, reward Average: -188.99, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 145 done after 200 steps, reward Average: -189.36, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 146 done after 147 steps, reward Average: -189.31, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 147 done after 200 steps, reward Average: -189.85, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 148 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 149 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 150 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 151 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 152 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 153 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 154 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 155 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 156 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 157 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 158 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 159 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 160 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 161 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 162 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 163 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 164 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 165 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 166 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 167 done after 200 steps, reward Average: -190.37, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 168 done after 147 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 169 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 170 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 171 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 172 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 173 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 174 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 175 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 176 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 177 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 178 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 179 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 180 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 181 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 182 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 183 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 184 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 185 done after 200 steps, reward Average: -189.84, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 186 done after 200 steps, reward Average: -190.28, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 187 done after 200 steps, reward Average: -190.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 188 done after 163 steps, reward Average: -190.82, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 189 done after 200 steps, reward Average: -191.21, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 190 done after 200 steps, reward Average: -191.65, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 191 done after 162 steps, reward Average: -191.27, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 192 done after 200 steps, reward Average: -191.79, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 193 done after 200 steps, reward Average: -191.79, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 194 done after 200 steps, reward Average: -191.79, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 195 done after 200 steps, reward Average: -191.79, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 196 done after 200 steps, reward Average: -192.3, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 197 done after 200 steps, reward Average: -192.3, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 198 done after 200 steps, reward Average: -192.3, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 199 done after 200 steps, reward Average: -192.64, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 200 done after 200 steps, reward Average: -193.18, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 201 done after 200 steps, reward Average: -193.18, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 202 done after 200 steps, reward Average: -193.63, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 203 done after 200 steps, reward Average: -193.63, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 204 done after 200 steps, reward Average: -193.63, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 205 done after 200 steps, reward Average: -193.63, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 206 done after 200 steps, reward Average: -193.63, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 207 done after 200 steps, reward Average: -194.13, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 208 done after 200 steps, reward Average: -194.13, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 209 done after 200 steps, reward Average: -194.13, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 210 done after 200 steps, reward Average: -194.13, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 211 done after 200 steps, reward Average: -194.13, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 212 done after 200 steps, reward Average: -194.13, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 213 done after 200 steps, reward Average: -194.62, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 214 done after 200 steps, reward Average: -195.15, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 215 done after 200 steps, reward Average: -195.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 216 done after 200 steps, reward Average: -195.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 217 done after 200 steps, reward Average: -195.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 218 done after 200 steps, reward Average: -195.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 219 done after 200 steps, reward Average: -195.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 220 done after 200 steps, reward Average: -195.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 221 done after 200 steps, reward Average: -196.17, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 222 done after 200 steps, reward Average: -196.17, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 223 done after 200 steps, reward Average: -196.17, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 224 done after 200 steps, reward Average: -196.17, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 225 done after 200 steps, reward Average: -196.17, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 226 done after 200 steps, reward Average: -196.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 227 done after 200 steps, reward Average: -196.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 228 done after 200 steps, reward Average: -196.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 229 done after 200 steps, reward Average: -196.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 230 done after 200 steps, reward Average: -196.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 231 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 232 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 233 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 234 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 235 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 236 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 237 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 238 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 239 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 240 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 241 done after 200 steps, reward Average: -197.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 242 done after 200 steps, reward Average: -197.67, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 243 done after 200 steps, reward Average: -198.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 244 done after 200 steps, reward Average: -198.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 245 done after 200 steps, reward Average: -198.19, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 246 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 247 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 248 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 249 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 250 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 251 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 252 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 253 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 254 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 255 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 256 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 257 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 258 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 259 done after 200 steps, reward Average: -198.72, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 260 done after 160 steps, reward Average: -198.32, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 261 done after 200 steps, reward Average: -198.32, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 262 done after 200 steps, reward Average: -198.32, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 263 done after 200 steps, reward Average: -198.32, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 264 done after 200 steps, reward Average: -198.32, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 265 done after 155 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 266 done after 147 steps, reward Average: -197.34, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 267 done after 200 steps, reward Average: -197.34, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 268 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 269 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 270 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 271 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 272 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 273 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 274 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 275 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 276 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 277 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 278 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 279 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 280 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 281 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 282 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 283 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 284 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 285 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 286 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 287 done after 200 steps, reward Average: -197.87, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 288 done after 200 steps, reward Average: -198.24, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 289 done after 200 steps, reward Average: -198.24, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 290 done after 200 steps, reward Average: -198.24, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 291 done after 149 steps, reward Average: -198.11, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 292 done after 200 steps, reward Average: -198.11, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 293 done after 200 steps, reward Average: -198.11, up to now: minReward: -146.0, minAverage: -186.37\n",
      "Episode 294 done after 141 steps, reward Average: -197.52, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 295 done after 200 steps, reward Average: -197.52, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 296 done after 200 steps, reward Average: -197.52, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 297 done after 200 steps, reward Average: -197.52, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 298 done after 200 steps, reward Average: -197.52, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 299 done after 142 steps, reward Average: -196.94, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 300 done after 200 steps, reward Average: -196.94, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 301 done after 200 steps, reward Average: -196.94, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 302 done after 200 steps, reward Average: -196.94, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 303 done after 200 steps, reward Average: -196.94, up to now: minReward: -141.0, minAverage: -186.37\n",
      "Episode 304 done after 138 steps, reward Average: -196.32, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 305 done after 200 steps, reward Average: -196.32, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 306 done after 141 steps, reward Average: -195.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 307 done after 150 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 308 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 309 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 310 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 311 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 312 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 313 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 314 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 315 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 316 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 317 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 318 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 319 done after 200 steps, reward Average: -195.23, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 320 done after 150 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 321 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 322 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 323 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 324 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 325 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 326 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 327 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 328 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 329 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 330 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 331 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 332 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 333 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 334 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 335 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 336 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 337 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 338 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 339 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 340 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 341 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 342 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 343 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 344 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 345 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 346 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 347 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 348 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 349 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 350 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 351 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 352 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 353 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 354 done after 200 steps, reward Average: -194.73, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 355 done after 186 steps, reward Average: -194.59, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 356 done after 200 steps, reward Average: -194.59, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 357 done after 200 steps, reward Average: -194.59, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 358 done after 200 steps, reward Average: -194.59, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 359 done after 200 steps, reward Average: -194.59, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 360 done after 158 steps, reward Average: -194.57, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 361 done after 142 steps, reward Average: -193.99, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 362 done after 155 steps, reward Average: -193.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 363 done after 156 steps, reward Average: -193.1, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 364 done after 148 steps, reward Average: -192.58, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 365 done after 200 steps, reward Average: -193.03, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 366 done after 200 steps, reward Average: -193.56, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 367 done after 150 steps, reward Average: -193.06, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 368 done after 148 steps, reward Average: -192.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 369 done after 200 steps, reward Average: -192.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 370 done after 200 steps, reward Average: -192.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 371 done after 200 steps, reward Average: -192.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 372 done after 200 steps, reward Average: -192.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 373 done after 200 steps, reward Average: -192.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 374 done after 200 steps, reward Average: -192.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 375 done after 152 steps, reward Average: -192.06, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 376 done after 200 steps, reward Average: -192.06, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 377 done after 200 steps, reward Average: -192.06, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 378 done after 200 steps, reward Average: -192.06, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 379 done after 150 steps, reward Average: -191.56, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 380 done after 200 steps, reward Average: -191.56, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 381 done after 200 steps, reward Average: -191.56, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 382 done after 200 steps, reward Average: -191.56, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 383 done after 200 steps, reward Average: -191.56, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 384 done after 200 steps, reward Average: -191.56, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 385 done after 147 steps, reward Average: -191.03, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 386 done after 147 steps, reward Average: -190.5, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 387 done after 200 steps, reward Average: -190.5, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 388 done after 145 steps, reward Average: -189.95, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 389 done after 157 steps, reward Average: -189.52, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 390 done after 200 steps, reward Average: -189.52, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 391 done after 146 steps, reward Average: -189.49, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 392 done after 146 steps, reward Average: -188.95, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 393 done after 146 steps, reward Average: -188.41, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 394 done after 146 steps, reward Average: -188.46, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 395 done after 200 steps, reward Average: -188.46, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 396 done after 152 steps, reward Average: -187.98, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 397 done after 200 steps, reward Average: -187.98, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 398 done after 156 steps, reward Average: -187.54, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 399 done after 152 steps, reward Average: -187.64, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 400 done after 200 steps, reward Average: -187.64, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 401 done after 200 steps, reward Average: -187.64, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 402 done after 151 steps, reward Average: -187.15, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 403 done after 151 steps, reward Average: -186.66, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 404 done after 150 steps, reward Average: -186.78, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 405 done after 200 steps, reward Average: -186.78, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 406 done after 146 steps, reward Average: -186.83, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 407 done after 200 steps, reward Average: -187.33, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 408 done after 150 steps, reward Average: -186.83, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 409 done after 200 steps, reward Average: -186.83, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 410 done after 200 steps, reward Average: -186.83, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 411 done after 200 steps, reward Average: -186.83, up to now: minReward: -138.0, minAverage: -186.37\n",
      "Episode 412 done after 150 steps, reward Average: -186.33, up to now: minReward: -138.0, minAverage: -186.33\n",
      "Episode 413 done after 149 steps, reward Average: -185.82, up to now: minReward: -138.0, minAverage: -185.82\n",
      "Episode 414 done after 146 steps, reward Average: -185.28, up to now: minReward: -138.0, minAverage: -185.28\n",
      "Episode 415 done after 148 steps, reward Average: -184.76, up to now: minReward: -138.0, minAverage: -184.76\n",
      "Episode 416 done after 148 steps, reward Average: -184.24, up to now: minReward: -138.0, minAverage: -184.24\n",
      "Episode 417 done after 200 steps, reward Average: -184.24, up to now: minReward: -138.0, minAverage: -184.24\n",
      "Episode 418 done after 200 steps, reward Average: -184.24, up to now: minReward: -138.0, minAverage: -184.24\n",
      "Episode 419 done after 200 steps, reward Average: -184.24, up to now: minReward: -138.0, minAverage: -184.24\n",
      "Episode 420 done after 145 steps, reward Average: -184.19, up to now: minReward: -138.0, minAverage: -184.19\n",
      "Episode 421 done after 149 steps, reward Average: -183.68, up to now: minReward: -138.0, minAverage: -183.68\n",
      "Episode 422 done after 143 steps, reward Average: -183.11, up to now: minReward: -138.0, minAverage: -183.11\n",
      "Episode 423 done after 200 steps, reward Average: -183.11, up to now: minReward: -138.0, minAverage: -183.11\n",
      "Episode 424 done after 200 steps, reward Average: -183.11, up to now: minReward: -138.0, minAverage: -183.11\n",
      "Episode 425 done after 157 steps, reward Average: -182.68, up to now: minReward: -138.0, minAverage: -182.68\n",
      "Episode 426 done after 154 steps, reward Average: -182.22, up to now: minReward: -138.0, minAverage: -182.22\n",
      "Episode 427 done after 200 steps, reward Average: -182.22, up to now: minReward: -138.0, minAverage: -182.22\n",
      "Episode 428 done after 154 steps, reward Average: -181.76, up to now: minReward: -138.0, minAverage: -181.76\n",
      "Episode 429 done after 200 steps, reward Average: -181.76, up to now: minReward: -138.0, minAverage: -181.76\n",
      "Episode 430 done after 155 steps, reward Average: -181.31, up to now: minReward: -138.0, minAverage: -181.31\n",
      "Episode 431 done after 160 steps, reward Average: -180.91, up to now: minReward: -138.0, minAverage: -180.91\n",
      "Episode 432 done after 154 steps, reward Average: -180.45, up to now: minReward: -138.0, minAverage: -180.45\n",
      "Episode 433 done after 153 steps, reward Average: -179.98, up to now: minReward: -138.0, minAverage: -179.98\n",
      "Episode 434 done after 157 steps, reward Average: -179.55, up to now: minReward: -138.0, minAverage: -179.55\n",
      "Episode 435 done after 200 steps, reward Average: -179.55, up to now: minReward: -138.0, minAverage: -179.55\n",
      "Episode 436 done after 146 steps, reward Average: -179.01, up to now: minReward: -138.0, minAverage: -179.01\n",
      "Episode 437 done after 152 steps, reward Average: -178.53, up to now: minReward: -138.0, minAverage: -178.53\n",
      "Episode 438 done after 200 steps, reward Average: -178.53, up to now: minReward: -138.0, minAverage: -178.53\n",
      "Episode 439 done after 149 steps, reward Average: -178.02, up to now: minReward: -138.0, minAverage: -178.02\n",
      "Episode 440 done after 145 steps, reward Average: -177.47, up to now: minReward: -138.0, minAverage: -177.47\n",
      "Episode 441 done after 150 steps, reward Average: -176.97, up to now: minReward: -138.0, minAverage: -176.97\n",
      "Episode 442 done after 144 steps, reward Average: -176.41, up to now: minReward: -138.0, minAverage: -176.41\n",
      "Episode 443 done after 200 steps, reward Average: -176.41, up to now: minReward: -138.0, minAverage: -176.41\n",
      "Episode 444 done after 145 steps, reward Average: -175.86, up to now: minReward: -138.0, minAverage: -175.86\n",
      "Episode 445 done after 200 steps, reward Average: -175.86, up to now: minReward: -138.0, minAverage: -175.86\n",
      "Episode 446 done after 147 steps, reward Average: -175.33, up to now: minReward: -138.0, minAverage: -175.33\n",
      "Episode 447 done after 146 steps, reward Average: -174.79, up to now: minReward: -138.0, minAverage: -174.79\n",
      "Episode 448 done after 153 steps, reward Average: -174.32, up to now: minReward: -138.0, minAverage: -174.32\n",
      "Episode 449 done after 155 steps, reward Average: -173.87, up to now: minReward: -138.0, minAverage: -173.87\n",
      "Episode 450 done after 200 steps, reward Average: -173.87, up to now: minReward: -138.0, minAverage: -173.87\n",
      "Episode 451 done after 200 steps, reward Average: -173.87, up to now: minReward: -138.0, minAverage: -173.87\n",
      "Episode 452 done after 200 steps, reward Average: -173.87, up to now: minReward: -138.0, minAverage: -173.87\n",
      "Episode 453 done after 200 steps, reward Average: -173.87, up to now: minReward: -138.0, minAverage: -173.87\n",
      "Episode 454 done after 200 steps, reward Average: -173.87, up to now: minReward: -138.0, minAverage: -173.87\n",
      "Episode 455 done after 152 steps, reward Average: -173.53, up to now: minReward: -138.0, minAverage: -173.53\n",
      "Episode 456 done after 148 steps, reward Average: -173.01, up to now: minReward: -138.0, minAverage: -173.01\n",
      "Episode 457 done after 149 steps, reward Average: -172.5, up to now: minReward: -138.0, minAverage: -172.5\n",
      "Episode 458 done after 148 steps, reward Average: -171.98, up to now: minReward: -138.0, minAverage: -171.98\n",
      "Episode 459 done after 148 steps, reward Average: -171.46, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 460 done after 200 steps, reward Average: -171.88, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 461 done after 153 steps, reward Average: -171.99, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 462 done after 151 steps, reward Average: -171.95, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 463 done after 200 steps, reward Average: -172.39, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 464 done after 200 steps, reward Average: -172.91, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 465 done after 151 steps, reward Average: -172.42, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 466 done after 143 steps, reward Average: -171.85, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 467 done after 150 steps, reward Average: -171.85, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 468 done after 156 steps, reward Average: -171.93, up to now: minReward: -138.0, minAverage: -171.46\n",
      "Episode 469 done after 152 steps, reward Average: -171.45, up to now: minReward: -138.0, minAverage: -171.45\n",
      "Episode 470 done after 147 steps, reward Average: -170.92, up to now: minReward: -138.0, minAverage: -170.92\n",
      "Episode 471 done after 147 steps, reward Average: -170.39, up to now: minReward: -138.0, minAverage: -170.39\n",
      "Episode 472 done after 149 steps, reward Average: -169.88, up to now: minReward: -138.0, minAverage: -169.88\n",
      "Episode 473 done after 200 steps, reward Average: -169.88, up to now: minReward: -138.0, minAverage: -169.88\n",
      "Episode 474 done after 152 steps, reward Average: -169.4, up to now: minReward: -138.0, minAverage: -169.4\n",
      "Episode 475 done after 146 steps, reward Average: -169.34, up to now: minReward: -138.0, minAverage: -169.34\n",
      "Episode 476 done after 146 steps, reward Average: -168.8, up to now: minReward: -138.0, minAverage: -168.8\n",
      "Episode 477 done after 153 steps, reward Average: -168.33, up to now: minReward: -138.0, minAverage: -168.33\n",
      "Episode 478 done after 153 steps, reward Average: -167.86, up to now: minReward: -138.0, minAverage: -167.86\n",
      "Episode 479 done after 200 steps, reward Average: -168.36, up to now: minReward: -138.0, minAverage: -167.86\n",
      "Episode 480 done after 200 steps, reward Average: -168.36, up to now: minReward: -138.0, minAverage: -167.86\n",
      "Episode 481 done after 200 steps, reward Average: -168.36, up to now: minReward: -138.0, minAverage: -167.86\n",
      "Episode 482 done after 160 steps, reward Average: -167.96, up to now: minReward: -138.0, minAverage: -167.86\n",
      "Episode 483 done after 179 steps, reward Average: -167.75, up to now: minReward: -138.0, minAverage: -167.75\n",
      "Episode 484 done after 178 steps, reward Average: -167.53, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 485 done after 160 steps, reward Average: -167.66, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 486 done after 158 steps, reward Average: -167.77, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 487 done after 200 steps, reward Average: -167.77, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 488 done after 147 steps, reward Average: -167.79, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 489 done after 159 steps, reward Average: -167.81, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 490 done after 200 steps, reward Average: -167.81, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 491 done after 156 steps, reward Average: -167.91, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 492 done after 159 steps, reward Average: -168.04, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 493 done after 200 steps, reward Average: -168.58, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 494 done after 156 steps, reward Average: -168.68, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 495 done after 156 steps, reward Average: -168.24, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 496 done after 200 steps, reward Average: -168.72, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 497 done after 200 steps, reward Average: -168.72, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 498 done after 153 steps, reward Average: -168.69, up to now: minReward: -138.0, minAverage: -167.53\n",
      "Episode 499 done after 200 steps, reward Average: -169.17, up to now: minReward: -138.0, minAverage: -167.53\n",
      "final result: \n",
      "134 times arrived in 500 episodes, first time in episode 22\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njDFYMjDGGIMlA2OMMcD/B6QwkR4k0pVJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x229876096d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        interval=nbEpisodes\n",
    "        if interval<decayInterval:\n",
    "            interval=decayInterval\n",
    "        result=decayStart-episode/interval\n",
    "        if result < decayBase:\n",
    "            result = decayBase\n",
    "        return result\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    ##gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    ##alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) +  (R +  Q(S, A)  Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
