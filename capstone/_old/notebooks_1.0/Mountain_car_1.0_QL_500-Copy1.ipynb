{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Try QLearning - 500 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.2)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 22:38:18,837] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.005\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.2\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=False\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 116 steps, reward Average: -195.05882352941177, up to now: minReward: -116.0, minAverage: -195.05882352941177\n",
      "Episode 17 done after 200 steps, reward Average: -195.33333333333334, up to now: minReward: -116.0, minAverage: -195.05882352941177\n",
      "Episode 18 done after 195 steps, reward Average: -195.31578947368422, up to now: minReward: -116.0, minAverage: -195.05882352941177\n",
      "Episode 19 done after 109 steps, reward Average: -191.0, up to now: minReward: -109.0, minAverage: -191.0\n",
      "Episode 20 done after 121 steps, reward Average: -187.66666666666666, up to now: minReward: -109.0, minAverage: -187.66666666666666\n",
      "Episode 21 done after 113 steps, reward Average: -184.27272727272728, up to now: minReward: -109.0, minAverage: -184.27272727272728\n",
      "Episode 22 done after 111 steps, reward Average: -181.08695652173913, up to now: minReward: -109.0, minAverage: -181.08695652173913\n",
      "Episode 23 done after 110 steps, reward Average: -178.125, up to now: minReward: -109.0, minAverage: -178.125\n",
      "Episode 24 done after 108 steps, reward Average: -175.32, up to now: minReward: -108.0, minAverage: -175.32\n",
      "Episode 25 done after 109 steps, reward Average: -172.76923076923077, up to now: minReward: -108.0, minAverage: -172.76923076923077\n",
      "Episode 26 done after 112 steps, reward Average: -170.5185185185185, up to now: minReward: -108.0, minAverage: -170.5185185185185\n",
      "Episode 27 done after 107 steps, reward Average: -168.25, up to now: minReward: -107.0, minAverage: -168.25\n",
      "Episode 28 done after 102 steps, reward Average: -165.9655172413793, up to now: minReward: -102.0, minAverage: -165.9655172413793\n",
      "Episode 29 done after 108 steps, reward Average: -164.03333333333333, up to now: minReward: -102.0, minAverage: -164.03333333333333\n",
      "Episode 30 done after 97 steps, reward Average: -161.8709677419355, up to now: minReward: -97.0, minAverage: -161.8709677419355\n",
      "Episode 31 done after 104 steps, reward Average: -160.0625, up to now: minReward: -97.0, minAverage: -160.0625\n",
      "Episode 32 done after 99 steps, reward Average: -158.21212121212122, up to now: minReward: -97.0, minAverage: -158.21212121212122\n",
      "Episode 33 done after 99 steps, reward Average: -156.47058823529412, up to now: minReward: -97.0, minAverage: -156.47058823529412\n",
      "Episode 34 done after 88 steps, reward Average: -154.5142857142857, up to now: minReward: -88.0, minAverage: -154.5142857142857\n",
      "Episode 35 done after 105 steps, reward Average: -153.13888888888889, up to now: minReward: -88.0, minAverage: -153.13888888888889\n",
      "Episode 36 done after 106 steps, reward Average: -151.86486486486487, up to now: minReward: -88.0, minAverage: -151.86486486486487\n",
      "Episode 37 done after 104 steps, reward Average: -150.60526315789474, up to now: minReward: -88.0, minAverage: -150.60526315789474\n",
      "Episode 38 done after 91 steps, reward Average: -149.07692307692307, up to now: minReward: -88.0, minAverage: -149.07692307692307\n",
      "Episode 39 done after 106 steps, reward Average: -148.0, up to now: minReward: -88.0, minAverage: -148.0\n",
      "Episode 40 done after 100 steps, reward Average: -146.82926829268294, up to now: minReward: -88.0, minAverage: -146.82926829268294\n",
      "Episode 41 done after 105 steps, reward Average: -145.83333333333334, up to now: minReward: -88.0, minAverage: -145.83333333333334\n",
      "Episode 42 done after 105 steps, reward Average: -144.88372093023256, up to now: minReward: -88.0, minAverage: -144.88372093023256\n",
      "Episode 43 done after 104 steps, reward Average: -143.95454545454547, up to now: minReward: -88.0, minAverage: -143.95454545454547\n",
      "Episode 44 done after 101 steps, reward Average: -143.0, up to now: minReward: -88.0, minAverage: -143.0\n",
      "Episode 45 done after 105 steps, reward Average: -142.17391304347825, up to now: minReward: -88.0, minAverage: -142.17391304347825\n",
      "Episode 46 done after 93 steps, reward Average: -141.12765957446808, up to now: minReward: -88.0, minAverage: -141.12765957446808\n",
      "Episode 47 done after 105 steps, reward Average: -140.375, up to now: minReward: -88.0, minAverage: -140.375\n",
      "Episode 48 done after 104 steps, reward Average: -139.6326530612245, up to now: minReward: -88.0, minAverage: -139.6326530612245\n",
      "Episode 49 done after 104 steps, reward Average: -138.92, up to now: minReward: -88.0, minAverage: -138.92\n",
      "Episode 50 done after 104 steps, reward Average: -138.23529411764707, up to now: minReward: -88.0, minAverage: -138.23529411764707\n",
      "Episode 51 done after 96 steps, reward Average: -137.42307692307693, up to now: minReward: -88.0, minAverage: -137.42307692307693\n",
      "Episode 52 done after 101 steps, reward Average: -136.73584905660377, up to now: minReward: -88.0, minAverage: -136.73584905660377\n",
      "Episode 53 done after 101 steps, reward Average: -136.07407407407408, up to now: minReward: -88.0, minAverage: -136.07407407407408\n",
      "Episode 54 done after 99 steps, reward Average: -135.4, up to now: minReward: -88.0, minAverage: -135.4\n",
      "Episode 55 done after 97 steps, reward Average: -134.71428571428572, up to now: minReward: -88.0, minAverage: -134.71428571428572\n",
      "Episode 56 done after 102 steps, reward Average: -134.140350877193, up to now: minReward: -88.0, minAverage: -134.140350877193\n",
      "Episode 57 done after 84 steps, reward Average: -133.27586206896552, up to now: minReward: -84.0, minAverage: -133.27586206896552\n",
      "Episode 58 done after 101 steps, reward Average: -132.72881355932202, up to now: minReward: -84.0, minAverage: -132.72881355932202\n",
      "Episode 59 done after 102 steps, reward Average: -132.21666666666667, up to now: minReward: -84.0, minAverage: -132.21666666666667\n",
      "Episode 60 done after 85 steps, reward Average: -131.44262295081967, up to now: minReward: -84.0, minAverage: -131.44262295081967\n",
      "Episode 61 done after 91 steps, reward Average: -130.79032258064515, up to now: minReward: -84.0, minAverage: -130.79032258064515\n",
      "Episode 62 done after 102 steps, reward Average: -130.33333333333334, up to now: minReward: -84.0, minAverage: -130.33333333333334\n",
      "Episode 63 done after 99 steps, reward Average: -129.84375, up to now: minReward: -84.0, minAverage: -129.84375\n",
      "Episode 64 done after 101 steps, reward Average: -129.4, up to now: minReward: -84.0, minAverage: -129.4\n",
      "Episode 65 done after 106 steps, reward Average: -129.04545454545453, up to now: minReward: -84.0, minAverage: -129.04545454545453\n",
      "Episode 66 done after 103 steps, reward Average: -128.65671641791045, up to now: minReward: -84.0, minAverage: -128.65671641791045\n",
      "Episode 67 done after 106 steps, reward Average: -128.3235294117647, up to now: minReward: -84.0, minAverage: -128.3235294117647\n",
      "Episode 68 done after 105 steps, reward Average: -127.98550724637681, up to now: minReward: -84.0, minAverage: -127.98550724637681\n",
      "Episode 69 done after 85 steps, reward Average: -127.37142857142857, up to now: minReward: -84.0, minAverage: -127.37142857142857\n",
      "Episode 70 done after 99 steps, reward Average: -126.97183098591549, up to now: minReward: -84.0, minAverage: -126.97183098591549\n",
      "Episode 71 done after 88 steps, reward Average: -126.43055555555556, up to now: minReward: -84.0, minAverage: -126.43055555555556\n",
      "Episode 72 done after 90 steps, reward Average: -125.93150684931507, up to now: minReward: -84.0, minAverage: -125.93150684931507\n",
      "Episode 73 done after 104 steps, reward Average: -125.63513513513513, up to now: minReward: -84.0, minAverage: -125.63513513513513\n",
      "Episode 74 done after 104 steps, reward Average: -125.34666666666666, up to now: minReward: -84.0, minAverage: -125.34666666666666\n",
      "Episode 75 done after 101 steps, reward Average: -125.02631578947368, up to now: minReward: -84.0, minAverage: -125.02631578947368\n",
      "Episode 76 done after 84 steps, reward Average: -124.49350649350649, up to now: minReward: -84.0, minAverage: -124.49350649350649\n",
      "Episode 77 done after 105 steps, reward Average: -124.24358974358974, up to now: minReward: -84.0, minAverage: -124.24358974358974\n",
      "Episode 78 done after 104 steps, reward Average: -123.9873417721519, up to now: minReward: -84.0, minAverage: -123.9873417721519\n",
      "Episode 79 done after 104 steps, reward Average: -123.7375, up to now: minReward: -84.0, minAverage: -123.7375\n",
      "Episode 80 done after 105 steps, reward Average: -123.50617283950618, up to now: minReward: -84.0, minAverage: -123.50617283950618\n",
      "Episode 81 done after 104 steps, reward Average: -123.26829268292683, up to now: minReward: -84.0, minAverage: -123.26829268292683\n",
      "Episode 82 done after 105 steps, reward Average: -123.04819277108433, up to now: minReward: -84.0, minAverage: -123.04819277108433\n",
      "Episode 83 done after 87 steps, reward Average: -122.61904761904762, up to now: minReward: -84.0, minAverage: -122.61904761904762\n",
      "Episode 84 done after 102 steps, reward Average: -122.37647058823529, up to now: minReward: -84.0, minAverage: -122.37647058823529\n",
      "Episode 85 done after 97 steps, reward Average: -122.0813953488372, up to now: minReward: -84.0, minAverage: -122.0813953488372\n",
      "Episode 86 done after 106 steps, reward Average: -121.89655172413794, up to now: minReward: -84.0, minAverage: -121.89655172413794\n",
      "Episode 87 done after 106 steps, reward Average: -121.7159090909091, up to now: minReward: -84.0, minAverage: -121.7159090909091\n",
      "Episode 88 done after 101 steps, reward Average: -121.48314606741573, up to now: minReward: -84.0, minAverage: -121.48314606741573\n",
      "Episode 89 done after 104 steps, reward Average: -121.28888888888889, up to now: minReward: -84.0, minAverage: -121.28888888888889\n",
      "Episode 90 done after 105 steps, reward Average: -121.10989010989012, up to now: minReward: -84.0, minAverage: -121.10989010989012\n",
      "Episode 91 done after 101 steps, reward Average: -120.8913043478261, up to now: minReward: -84.0, minAverage: -120.8913043478261\n",
      "Episode 92 done after 111 steps, reward Average: -120.78494623655914, up to now: minReward: -84.0, minAverage: -120.78494623655914\n",
      "Episode 93 done after 105 steps, reward Average: -120.61702127659575, up to now: minReward: -84.0, minAverage: -120.61702127659575\n",
      "Episode 94 done after 102 steps, reward Average: -120.42105263157895, up to now: minReward: -84.0, minAverage: -120.42105263157895\n",
      "Episode 95 done after 87 steps, reward Average: -120.07291666666667, up to now: minReward: -84.0, minAverage: -120.07291666666667\n",
      "Episode 96 done after 98 steps, reward Average: -119.84536082474227, up to now: minReward: -84.0, minAverage: -119.84536082474227\n",
      "Episode 97 done after 106 steps, reward Average: -119.70408163265306, up to now: minReward: -84.0, minAverage: -119.70408163265306\n",
      "Episode 98 done after 106 steps, reward Average: -119.56565656565657, up to now: minReward: -84.0, minAverage: -119.56565656565657\n",
      "Episode 99 done after 84 steps, reward Average: -119.21, up to now: minReward: -84.0, minAverage: -119.21\n",
      "Episode 100 done after 104 steps, reward Average: -118.25, up to now: minReward: -84.0, minAverage: -118.25\n",
      "Episode 101 done after 106 steps, reward Average: -117.31, up to now: minReward: -84.0, minAverage: -117.31\n",
      "Episode 102 done after 104 steps, reward Average: -116.35, up to now: minReward: -84.0, minAverage: -116.35\n",
      "Episode 103 done after 88 steps, reward Average: -115.23, up to now: minReward: -84.0, minAverage: -115.23\n",
      "Episode 104 done after 97 steps, reward Average: -114.2, up to now: minReward: -84.0, minAverage: -114.2\n",
      "Episode 105 done after 103 steps, reward Average: -113.23, up to now: minReward: -84.0, minAverage: -113.23\n",
      "Episode 106 done after 103 steps, reward Average: -112.26, up to now: minReward: -84.0, minAverage: -112.26\n",
      "Episode 107 done after 104 steps, reward Average: -111.3, up to now: minReward: -84.0, minAverage: -111.3\n",
      "Episode 108 done after 102 steps, reward Average: -110.32, up to now: minReward: -84.0, minAverage: -110.32\n",
      "Episode 109 done after 104 steps, reward Average: -109.36, up to now: minReward: -84.0, minAverage: -109.36\n",
      "Episode 110 done after 102 steps, reward Average: -108.38, up to now: minReward: -84.0, minAverage: -108.38\n",
      "Episode 111 done after 106 steps, reward Average: -107.44, up to now: minReward: -84.0, minAverage: -107.44\n",
      "Episode 112 done after 105 steps, reward Average: -106.49, up to now: minReward: -84.0, minAverage: -106.49\n",
      "Episode 113 done after 106 steps, reward Average: -105.55, up to now: minReward: -84.0, minAverage: -105.55\n",
      "Episode 114 done after 100 steps, reward Average: -104.55, up to now: minReward: -84.0, minAverage: -104.55\n",
      "Episode 115 done after 105 steps, reward Average: -103.6, up to now: minReward: -84.0, minAverage: -103.6\n",
      "Episode 116 done after 104 steps, reward Average: -103.48, up to now: minReward: -84.0, minAverage: -103.48\n",
      "Episode 117 done after 106 steps, reward Average: -102.54, up to now: minReward: -84.0, minAverage: -102.54\n",
      "Episode 118 done after 104 steps, reward Average: -101.63, up to now: minReward: -84.0, minAverage: -101.63\n",
      "Episode 119 done after 106 steps, reward Average: -101.6, up to now: minReward: -84.0, minAverage: -101.6\n",
      "Episode 120 done after 84 steps, reward Average: -101.23, up to now: minReward: -84.0, minAverage: -101.23\n",
      "Episode 121 done after 105 steps, reward Average: -101.15, up to now: minReward: -84.0, minAverage: -101.15\n",
      "Episode 122 done after 106 steps, reward Average: -101.1, up to now: minReward: -84.0, minAverage: -101.1\n",
      "Episode 123 done after 98 steps, reward Average: -100.98, up to now: minReward: -84.0, minAverage: -100.98\n",
      "Episode 124 done after 101 steps, reward Average: -100.91, up to now: minReward: -84.0, minAverage: -100.91\n",
      "Episode 125 done after 92 steps, reward Average: -100.74, up to now: minReward: -84.0, minAverage: -100.74\n",
      "Episode 126 done after 105 steps, reward Average: -100.67, up to now: minReward: -84.0, minAverage: -100.67\n",
      "Episode 127 done after 99 steps, reward Average: -100.59, up to now: minReward: -84.0, minAverage: -100.59\n",
      "Episode 128 done after 103 steps, reward Average: -100.6, up to now: minReward: -84.0, minAverage: -100.59\n",
      "Episode 129 done after 102 steps, reward Average: -100.54, up to now: minReward: -84.0, minAverage: -100.54\n",
      "Episode 130 done after 104 steps, reward Average: -100.61, up to now: minReward: -84.0, minAverage: -100.54\n",
      "Episode 131 done after 103 steps, reward Average: -100.6, up to now: minReward: -84.0, minAverage: -100.54\n",
      "Episode 132 done after 93 steps, reward Average: -100.54, up to now: minReward: -84.0, minAverage: -100.54\n",
      "Episode 133 done after 106 steps, reward Average: -100.61, up to now: minReward: -84.0, minAverage: -100.54\n",
      "Episode 134 done after 106 steps, reward Average: -100.79, up to now: minReward: -84.0, minAverage: -100.54\n",
      "Episode 135 done after 84 steps, reward Average: -100.58, up to now: minReward: -84.0, minAverage: -100.54\n",
      "Episode 136 done after 105 steps, reward Average: -100.57, up to now: minReward: -84.0, minAverage: -100.54\n",
      "Episode 137 done after 88 steps, reward Average: -100.41, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 138 done after 105 steps, reward Average: -100.55, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 139 done after 105 steps, reward Average: -100.54, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 140 done after 105 steps, reward Average: -100.59, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 141 done after 105 steps, reward Average: -100.59, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 142 done after 105 steps, reward Average: -100.59, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 143 done after 99 steps, reward Average: -100.54, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 144 done after 108 steps, reward Average: -100.61, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 145 done after 104 steps, reward Average: -100.6, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 146 done after 96 steps, reward Average: -100.63, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 147 done after 105 steps, reward Average: -100.63, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 148 done after 101 steps, reward Average: -100.6, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 149 done after 107 steps, reward Average: -100.63, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 150 done after 105 steps, reward Average: -100.64, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 151 done after 98 steps, reward Average: -100.66, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 152 done after 104 steps, reward Average: -100.69, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 153 done after 106 steps, reward Average: -100.74, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 154 done after 105 steps, reward Average: -100.8, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 155 done after 103 steps, reward Average: -100.86, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 156 done after 103 steps, reward Average: -100.87, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 157 done after 104 steps, reward Average: -101.07, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 158 done after 88 steps, reward Average: -100.94, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 159 done after 106 steps, reward Average: -100.98, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 160 done after 105 steps, reward Average: -101.18, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 161 done after 105 steps, reward Average: -101.32, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 162 done after 93 steps, reward Average: -101.23, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 163 done after 96 steps, reward Average: -101.2, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 164 done after 105 steps, reward Average: -101.24, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 165 done after 105 steps, reward Average: -101.23, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 166 done after 107 steps, reward Average: -101.27, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 167 done after 104 steps, reward Average: -101.25, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 168 done after 106 steps, reward Average: -101.26, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 169 done after 102 steps, reward Average: -101.43, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 170 done after 105 steps, reward Average: -101.49, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 171 done after 101 steps, reward Average: -101.62, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 172 done after 108 steps, reward Average: -101.8, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 173 done after 117 steps, reward Average: -101.93, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 174 done after 88 steps, reward Average: -101.77, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 175 done after 113 steps, reward Average: -101.89, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 176 done after 106 steps, reward Average: -102.11, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 177 done after 105 steps, reward Average: -102.11, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 178 done after 108 steps, reward Average: -102.15, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 179 done after 107 steps, reward Average: -102.18, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 180 done after 107 steps, reward Average: -102.2, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 181 done after 114 steps, reward Average: -102.3, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 182 done after 97 steps, reward Average: -102.22, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 183 done after 106 steps, reward Average: -102.41, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 184 done after 105 steps, reward Average: -102.44, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 185 done after 106 steps, reward Average: -102.53, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 186 done after 107 steps, reward Average: -102.54, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 187 done after 105 steps, reward Average: -102.53, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 188 done after 111 steps, reward Average: -102.63, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 189 done after 107 steps, reward Average: -102.66, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 190 done after 99 steps, reward Average: -102.6, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 191 done after 101 steps, reward Average: -102.6, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 192 done after 102 steps, reward Average: -102.51, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 193 done after 106 steps, reward Average: -102.52, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 194 done after 99 steps, reward Average: -102.49, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 195 done after 106 steps, reward Average: -102.68, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 196 done after 106 steps, reward Average: -102.76, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 197 done after 107 steps, reward Average: -102.77, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 198 done after 105 steps, reward Average: -102.76, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 199 done after 107 steps, reward Average: -102.99, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 200 done after 99 steps, reward Average: -102.94, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 201 done after 86 steps, reward Average: -102.74, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 202 done after 112 steps, reward Average: -102.82, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 203 done after 106 steps, reward Average: -103.0, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 204 done after 106 steps, reward Average: -103.09, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 205 done after 106 steps, reward Average: -103.12, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 206 done after 104 steps, reward Average: -103.13, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 207 done after 101 steps, reward Average: -103.1, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 208 done after 107 steps, reward Average: -103.15, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 209 done after 104 steps, reward Average: -103.15, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 210 done after 103 steps, reward Average: -103.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 211 done after 106 steps, reward Average: -103.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 212 done after 105 steps, reward Average: -103.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 213 done after 107 steps, reward Average: -103.17, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 214 done after 104 steps, reward Average: -103.21, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 215 done after 102 steps, reward Average: -103.18, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 216 done after 107 steps, reward Average: -103.21, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 217 done after 102 steps, reward Average: -103.17, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 218 done after 106 steps, reward Average: -103.19, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 219 done after 103 steps, reward Average: -103.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 220 done after 105 steps, reward Average: -103.37, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 221 done after 106 steps, reward Average: -103.38, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 222 done after 104 steps, reward Average: -103.36, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 223 done after 103 steps, reward Average: -103.41, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 224 done after 106 steps, reward Average: -103.46, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 225 done after 106 steps, reward Average: -103.6, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 226 done after 104 steps, reward Average: -103.59, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 227 done after 86 steps, reward Average: -103.46, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 228 done after 107 steps, reward Average: -103.5, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 229 done after 106 steps, reward Average: -103.54, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 230 done after 103 steps, reward Average: -103.53, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 231 done after 106 steps, reward Average: -103.56, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 232 done after 104 steps, reward Average: -103.67, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 233 done after 111 steps, reward Average: -103.72, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 234 done after 90 steps, reward Average: -103.56, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 235 done after 102 steps, reward Average: -103.74, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 236 done after 107 steps, reward Average: -103.76, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 237 done after 107 steps, reward Average: -103.95, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 238 done after 107 steps, reward Average: -103.97, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 239 done after 86 steps, reward Average: -103.78, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 240 done after 102 steps, reward Average: -103.75, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 241 done after 105 steps, reward Average: -103.75, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 242 done after 108 steps, reward Average: -103.78, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 243 done after 107 steps, reward Average: -103.86, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 244 done after 86 steps, reward Average: -103.64, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 245 done after 85 steps, reward Average: -103.45, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 246 done after 92 steps, reward Average: -103.41, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 247 done after 107 steps, reward Average: -103.43, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 248 done after 88 steps, reward Average: -103.3, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 249 done after 106 steps, reward Average: -103.29, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 250 done after 200 steps, reward Average: -104.24, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 251 done after 106 steps, reward Average: -104.32, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 252 done after 111 steps, reward Average: -104.39, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 253 done after 125 steps, reward Average: -104.58, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 254 done after 113 steps, reward Average: -104.66, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 255 done after 107 steps, reward Average: -104.7, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 256 done after 111 steps, reward Average: -104.78, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 257 done after 108 steps, reward Average: -104.82, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 258 done after 128 steps, reward Average: -105.22, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 259 done after 105 steps, reward Average: -105.21, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 260 done after 107 steps, reward Average: -105.23, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 261 done after 109 steps, reward Average: -105.27, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 262 done after 108 steps, reward Average: -105.42, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 263 done after 89 steps, reward Average: -105.35, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 264 done after 100 steps, reward Average: -105.3, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 265 done after 107 steps, reward Average: -105.32, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 266 done after 108 steps, reward Average: -105.33, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 267 done after 98 steps, reward Average: -105.27, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 268 done after 88 steps, reward Average: -105.09, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 269 done after 102 steps, reward Average: -105.09, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 270 done after 109 steps, reward Average: -105.13, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 271 done after 107 steps, reward Average: -105.19, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 272 done after 128 steps, reward Average: -105.39, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 273 done after 200 steps, reward Average: -106.22, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 274 done after 183 steps, reward Average: -107.17, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 275 done after 144 steps, reward Average: -107.48, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 276 done after 113 steps, reward Average: -107.55, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 277 done after 90 steps, reward Average: -107.4, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 278 done after 122 steps, reward Average: -107.54, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 279 done after 110 steps, reward Average: -107.57, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 280 done after 107 steps, reward Average: -107.57, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 281 done after 137 steps, reward Average: -107.8, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 282 done after 124 steps, reward Average: -108.07, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 283 done after 117 steps, reward Average: -108.18, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 284 done after 107 steps, reward Average: -108.2, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 285 done after 106 steps, reward Average: -108.2, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 286 done after 146 steps, reward Average: -108.59, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 287 done after 110 steps, reward Average: -108.64, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 288 done after 99 steps, reward Average: -108.52, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 289 done after 139 steps, reward Average: -108.84, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 290 done after 109 steps, reward Average: -108.94, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 291 done after 108 steps, reward Average: -109.01, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 292 done after 110 steps, reward Average: -109.09, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 293 done after 88 steps, reward Average: -108.91, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 294 done after 111 steps, reward Average: -109.03, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 295 done after 105 steps, reward Average: -109.02, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 296 done after 96 steps, reward Average: -108.92, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 297 done after 121 steps, reward Average: -109.06, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 298 done after 107 steps, reward Average: -109.08, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 299 done after 121 steps, reward Average: -109.22, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 300 done after 108 steps, reward Average: -109.31, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 301 done after 139 steps, reward Average: -109.84, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 302 done after 88 steps, reward Average: -109.6, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 303 done after 108 steps, reward Average: -109.62, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 304 done after 107 steps, reward Average: -109.63, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 305 done after 108 steps, reward Average: -109.65, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 306 done after 135 steps, reward Average: -109.96, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 307 done after 131 steps, reward Average: -110.26, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 308 done after 113 steps, reward Average: -110.32, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 309 done after 132 steps, reward Average: -110.6, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 310 done after 106 steps, reward Average: -110.63, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 311 done after 111 steps, reward Average: -110.68, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 312 done after 100 steps, reward Average: -110.63, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 313 done after 144 steps, reward Average: -111.0, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 314 done after 111 steps, reward Average: -111.07, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 315 done after 98 steps, reward Average: -111.03, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 316 done after 122 steps, reward Average: -111.18, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 317 done after 112 steps, reward Average: -111.28, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 318 done after 109 steps, reward Average: -111.31, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 319 done after 147 steps, reward Average: -111.75, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 320 done after 107 steps, reward Average: -111.77, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 321 done after 200 steps, reward Average: -112.71, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 322 done after 110 steps, reward Average: -112.77, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 323 done after 110 steps, reward Average: -112.84, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 324 done after 93 steps, reward Average: -112.71, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 325 done after 106 steps, reward Average: -112.71, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 326 done after 105 steps, reward Average: -112.72, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 327 done after 131 steps, reward Average: -113.17, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 328 done after 131 steps, reward Average: -113.41, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 329 done after 95 steps, reward Average: -113.3, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 330 done after 134 steps, reward Average: -113.61, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 331 done after 200 steps, reward Average: -114.55, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 332 done after 187 steps, reward Average: -115.38, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 333 done after 134 steps, reward Average: -115.61, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 334 done after 108 steps, reward Average: -115.79, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 335 done after 137 steps, reward Average: -116.14, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 336 done after 139 steps, reward Average: -116.46, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 337 done after 111 steps, reward Average: -116.5, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 338 done after 108 steps, reward Average: -116.51, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 339 done after 119 steps, reward Average: -116.84, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 340 done after 200 steps, reward Average: -117.82, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 341 done after 123 steps, reward Average: -118.0, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 342 done after 107 steps, reward Average: -117.99, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 343 done after 107 steps, reward Average: -117.99, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 344 done after 114 steps, reward Average: -118.27, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 345 done after 115 steps, reward Average: -118.57, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 346 done after 200 steps, reward Average: -119.65, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 347 done after 112 steps, reward Average: -119.7, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 348 done after 132 steps, reward Average: -120.14, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 349 done after 120 steps, reward Average: -120.28, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 350 done after 110 steps, reward Average: -119.38, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 351 done after 106 steps, reward Average: -119.38, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 352 done after 117 steps, reward Average: -119.44, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 353 done after 110 steps, reward Average: -119.29, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 354 done after 127 steps, reward Average: -119.43, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 355 done after 108 steps, reward Average: -119.44, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 356 done after 107 steps, reward Average: -119.4, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 357 done after 107 steps, reward Average: -119.39, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 358 done after 114 steps, reward Average: -119.25, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 359 done after 107 steps, reward Average: -119.27, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 360 done after 117 steps, reward Average: -119.37, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 361 done after 112 steps, reward Average: -119.4, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 362 done after 117 steps, reward Average: -119.49, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 363 done after 120 steps, reward Average: -119.8, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 364 done after 108 steps, reward Average: -119.88, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 365 done after 93 steps, reward Average: -119.74, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 366 done after 106 steps, reward Average: -119.72, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 367 done after 90 steps, reward Average: -119.64, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 368 done after 115 steps, reward Average: -119.91, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 369 done after 127 steps, reward Average: -120.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 370 done after 133 steps, reward Average: -120.4, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 371 done after 119 steps, reward Average: -120.52, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 372 done after 118 steps, reward Average: -120.42, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 373 done after 122 steps, reward Average: -119.64, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 374 done after 111 steps, reward Average: -118.92, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 375 done after 100 steps, reward Average: -118.48, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 376 done after 106 steps, reward Average: -118.41, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 377 done after 119 steps, reward Average: -118.7, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 378 done after 104 steps, reward Average: -118.52, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 379 done after 114 steps, reward Average: -118.56, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 380 done after 115 steps, reward Average: -118.64, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 381 done after 119 steps, reward Average: -118.46, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 382 done after 114 steps, reward Average: -118.36, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 383 done after 108 steps, reward Average: -118.27, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 384 done after 107 steps, reward Average: -118.27, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 385 done after 107 steps, reward Average: -118.28, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 386 done after 111 steps, reward Average: -117.93, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 387 done after 148 steps, reward Average: -118.31, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 388 done after 118 steps, reward Average: -118.5, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 389 done after 111 steps, reward Average: -118.22, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 390 done after 116 steps, reward Average: -118.29, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 391 done after 109 steps, reward Average: -118.3, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 392 done after 108 steps, reward Average: -118.28, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 393 done after 181 steps, reward Average: -119.21, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 394 done after 111 steps, reward Average: -119.21, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 395 done after 119 steps, reward Average: -119.35, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 396 done after 125 steps, reward Average: -119.64, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 397 done after 107 steps, reward Average: -119.5, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 398 done after 109 steps, reward Average: -119.52, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 399 done after 117 steps, reward Average: -119.48, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 400 done after 113 steps, reward Average: -119.53, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 401 done after 107 steps, reward Average: -119.21, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 402 done after 109 steps, reward Average: -119.42, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 403 done after 110 steps, reward Average: -119.44, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 404 done after 107 steps, reward Average: -119.44, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 405 done after 107 steps, reward Average: -119.43, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 406 done after 111 steps, reward Average: -119.19, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 407 done after 114 steps, reward Average: -119.02, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 408 done after 112 steps, reward Average: -119.01, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 409 done after 111 steps, reward Average: -118.8, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 410 done after 92 steps, reward Average: -118.66, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 411 done after 115 steps, reward Average: -118.7, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 412 done after 104 steps, reward Average: -118.74, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 413 done after 111 steps, reward Average: -118.41, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 414 done after 112 steps, reward Average: -118.42, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 415 done after 107 steps, reward Average: -118.51, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 416 done after 116 steps, reward Average: -118.45, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 417 done after 121 steps, reward Average: -118.54, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 418 done after 108 steps, reward Average: -118.53, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 419 done after 95 steps, reward Average: -118.01, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 420 done after 111 steps, reward Average: -118.05, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 421 done after 110 steps, reward Average: -117.15, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 422 done after 113 steps, reward Average: -117.18, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 423 done after 116 steps, reward Average: -117.24, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 424 done after 109 steps, reward Average: -117.4, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 425 done after 111 steps, reward Average: -117.45, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 426 done after 124 steps, reward Average: -117.64, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 427 done after 124 steps, reward Average: -117.57, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 428 done after 200 steps, reward Average: -118.26, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 429 done after 111 steps, reward Average: -118.42, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 430 done after 112 steps, reward Average: -118.2, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 431 done after 108 steps, reward Average: -117.28, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 432 done after 113 steps, reward Average: -116.54, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 433 done after 109 steps, reward Average: -116.29, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 434 done after 115 steps, reward Average: -116.36, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 435 done after 90 steps, reward Average: -115.89, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 436 done after 123 steps, reward Average: -115.73, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 437 done after 117 steps, reward Average: -115.79, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 438 done after 114 steps, reward Average: -115.85, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 439 done after 132 steps, reward Average: -115.98, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 440 done after 126 steps, reward Average: -115.24, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 441 done after 115 steps, reward Average: -115.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 442 done after 110 steps, reward Average: -115.19, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 443 done after 118 steps, reward Average: -115.3, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 444 done after 200 steps, reward Average: -116.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 445 done after 131 steps, reward Average: -116.32, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 446 done after 127 steps, reward Average: -115.59, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 447 done after 114 steps, reward Average: -115.61, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 448 done after 133 steps, reward Average: -115.62, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 449 done after 126 steps, reward Average: -115.68, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 450 done after 114 steps, reward Average: -115.72, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 451 done after 189 steps, reward Average: -116.55, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 452 done after 122 steps, reward Average: -116.6, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 453 done after 97 steps, reward Average: -116.47, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 454 done after 187 steps, reward Average: -117.07, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 455 done after 188 steps, reward Average: -117.87, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 456 done after 176 steps, reward Average: -118.56, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 457 done after 181 steps, reward Average: -119.3, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 458 done after 174 steps, reward Average: -119.9, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 459 done after 170 steps, reward Average: -120.53, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 460 done after 200 steps, reward Average: -121.36, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 461 done after 173 steps, reward Average: -121.97, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 462 done after 168 steps, reward Average: -122.48, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 463 done after 200 steps, reward Average: -123.28, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 464 done after 200 steps, reward Average: -124.2, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 465 done after 200 steps, reward Average: -125.27, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 466 done after 200 steps, reward Average: -126.21, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 467 done after 183 steps, reward Average: -127.14, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 468 done after 196 steps, reward Average: -127.95, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 469 done after 200 steps, reward Average: -128.68, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 470 done after 200 steps, reward Average: -129.35, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 471 done after 200 steps, reward Average: -130.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 472 done after 177 steps, reward Average: -130.75, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 473 done after 119 steps, reward Average: -130.72, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 474 done after 178 steps, reward Average: -131.39, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 475 done after 187 steps, reward Average: -132.26, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 476 done after 180 steps, reward Average: -133.0, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 477 done after 190 steps, reward Average: -133.71, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 478 done after 190 steps, reward Average: -134.57, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 479 done after 200 steps, reward Average: -135.43, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 480 done after 185 steps, reward Average: -136.13, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 481 done after 173 steps, reward Average: -136.67, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 482 done after 188 steps, reward Average: -137.41, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 483 done after 173 steps, reward Average: -138.06, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 484 done after 179 steps, reward Average: -138.78, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 485 done after 176 steps, reward Average: -139.47, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 486 done after 180 steps, reward Average: -140.16, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 487 done after 179 steps, reward Average: -140.47, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 488 done after 179 steps, reward Average: -141.08, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 489 done after 178 steps, reward Average: -141.75, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 490 done after 200 steps, reward Average: -142.59, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 491 done after 179 steps, reward Average: -143.29, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 492 done after 187 steps, reward Average: -144.08, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 493 done after 177 steps, reward Average: -144.04, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 494 done after 193 steps, reward Average: -144.86, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 495 done after 179 steps, reward Average: -145.46, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 496 done after 186 steps, reward Average: -146.07, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 497 done after 190 steps, reward Average: -146.9, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 498 done after 199 steps, reward Average: -147.8, up to now: minReward: -84.0, minAverage: -100.41\n",
      "Episode 499 done after 175 steps, reward Average: -148.38, up to now: minReward: -84.0, minAverage: -100.41\n",
      "final result: \n",
      "465 times arrived in 500 episodes, first time in episode 16\n",
      "problem solved?:True\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25df0debbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
