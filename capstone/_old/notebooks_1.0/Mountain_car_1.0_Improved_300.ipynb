{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling 888, Alpha was further tunes, 300 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 25:\n",
    "            return 0.65\n",
    "        elif episode < 35:\n",
    "            return 0.6\n",
    "        elif episode < 45:\n",
    "            return 0.55\n",
    "        elif episode < 55:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.4\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 13:31:15,440] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 170 steps, reward Average: -197.85714285714286, up to now: minReward: -170.0, minAverage: -197.85714285714286\n",
      "Episode 14 done after 164 steps, reward Average: -195.6, up to now: minReward: -164.0, minAverage: -195.6\n",
      "Episode 15 done after 200 steps, reward Average: -195.875, up to now: minReward: -164.0, minAverage: -195.6\n",
      "Episode 16 done after 156 steps, reward Average: -193.52941176470588, up to now: minReward: -156.0, minAverage: -193.52941176470588\n",
      "Episode 17 done after 182 steps, reward Average: -192.88888888888889, up to now: minReward: -156.0, minAverage: -192.88888888888889\n",
      "Episode 18 done after 155 steps, reward Average: -190.89473684210526, up to now: minReward: -155.0, minAverage: -190.89473684210526\n",
      "Episode 19 done after 162 steps, reward Average: -189.45, up to now: minReward: -155.0, minAverage: -189.45\n",
      "Episode 20 done after 150 steps, reward Average: -187.57142857142858, up to now: minReward: -150.0, minAverage: -187.57142857142858\n",
      "Episode 21 done after 138 steps, reward Average: -185.3181818181818, up to now: minReward: -138.0, minAverage: -185.3181818181818\n",
      "Episode 22 done after 146 steps, reward Average: -183.6086956521739, up to now: minReward: -138.0, minAverage: -183.6086956521739\n",
      "Episode 23 done after 144 steps, reward Average: -181.95833333333334, up to now: minReward: -138.0, minAverage: -181.95833333333334\n",
      "Episode 24 done after 141 steps, reward Average: -180.32, up to now: minReward: -138.0, minAverage: -180.32\n",
      "Episode 25 done after 144 steps, reward Average: -178.92307692307693, up to now: minReward: -138.0, minAverage: -178.92307692307693\n",
      "Episode 26 done after 144 steps, reward Average: -177.62962962962962, up to now: minReward: -138.0, minAverage: -177.62962962962962\n",
      "Episode 27 done after 142 steps, reward Average: -176.35714285714286, up to now: minReward: -138.0, minAverage: -176.35714285714286\n",
      "Episode 28 done after 139 steps, reward Average: -175.06896551724137, up to now: minReward: -138.0, minAverage: -175.06896551724137\n",
      "Episode 29 done after 150 steps, reward Average: -174.23333333333332, up to now: minReward: -138.0, minAverage: -174.23333333333332\n",
      "Episode 30 done after 139 steps, reward Average: -173.09677419354838, up to now: minReward: -138.0, minAverage: -173.09677419354838\n",
      "Episode 31 done after 141 steps, reward Average: -172.09375, up to now: minReward: -138.0, minAverage: -172.09375\n",
      "Episode 32 done after 140 steps, reward Average: -171.12121212121212, up to now: minReward: -138.0, minAverage: -171.12121212121212\n",
      "Episode 33 done after 142 steps, reward Average: -170.26470588235293, up to now: minReward: -138.0, minAverage: -170.26470588235293\n",
      "Episode 34 done after 139 steps, reward Average: -169.37142857142857, up to now: minReward: -138.0, minAverage: -169.37142857142857\n",
      "Episode 35 done after 139 steps, reward Average: -168.52777777777777, up to now: minReward: -138.0, minAverage: -168.52777777777777\n",
      "Episode 36 done after 142 steps, reward Average: -167.8108108108108, up to now: minReward: -138.0, minAverage: -167.8108108108108\n",
      "Episode 37 done after 138 steps, reward Average: -167.02631578947367, up to now: minReward: -138.0, minAverage: -167.02631578947367\n",
      "Episode 38 done after 142 steps, reward Average: -166.3846153846154, up to now: minReward: -138.0, minAverage: -166.3846153846154\n",
      "Episode 39 done after 138 steps, reward Average: -165.675, up to now: minReward: -138.0, minAverage: -165.675\n",
      "Episode 40 done after 138 steps, reward Average: -165.0, up to now: minReward: -138.0, minAverage: -165.0\n",
      "Episode 41 done after 138 steps, reward Average: -164.35714285714286, up to now: minReward: -138.0, minAverage: -164.35714285714286\n",
      "Episode 42 done after 138 steps, reward Average: -163.74418604651163, up to now: minReward: -138.0, minAverage: -163.74418604651163\n",
      "Episode 43 done after 139 steps, reward Average: -163.1818181818182, up to now: minReward: -138.0, minAverage: -163.1818181818182\n",
      "Episode 44 done after 140 steps, reward Average: -162.66666666666666, up to now: minReward: -138.0, minAverage: -162.66666666666666\n",
      "Episode 45 done after 138 steps, reward Average: -162.1304347826087, up to now: minReward: -138.0, minAverage: -162.1304347826087\n",
      "Episode 46 done after 138 steps, reward Average: -161.61702127659575, up to now: minReward: -138.0, minAverage: -161.61702127659575\n",
      "Episode 47 done after 138 steps, reward Average: -161.125, up to now: minReward: -138.0, minAverage: -161.125\n",
      "Episode 48 done after 138 steps, reward Average: -160.6530612244898, up to now: minReward: -138.0, minAverage: -160.6530612244898\n",
      "Episode 49 done after 138 steps, reward Average: -160.2, up to now: minReward: -138.0, minAverage: -160.2\n",
      "Episode 50 done after 140 steps, reward Average: -159.80392156862746, up to now: minReward: -138.0, minAverage: -159.80392156862746\n",
      "Episode 51 done after 138 steps, reward Average: -159.3846153846154, up to now: minReward: -138.0, minAverage: -159.3846153846154\n",
      "Episode 52 done after 138 steps, reward Average: -158.9811320754717, up to now: minReward: -138.0, minAverage: -158.9811320754717\n",
      "Episode 53 done after 139 steps, reward Average: -158.61111111111111, up to now: minReward: -138.0, minAverage: -158.61111111111111\n",
      "Episode 54 done after 139 steps, reward Average: -158.25454545454545, up to now: minReward: -138.0, minAverage: -158.25454545454545\n",
      "Episode 55 done after 138 steps, reward Average: -157.89285714285714, up to now: minReward: -138.0, minAverage: -157.89285714285714\n",
      "Episode 56 done after 139 steps, reward Average: -157.56140350877192, up to now: minReward: -138.0, minAverage: -157.56140350877192\n",
      "Episode 57 done after 142 steps, reward Average: -157.29310344827587, up to now: minReward: -138.0, minAverage: -157.29310344827587\n",
      "Episode 58 done after 137 steps, reward Average: -156.94915254237287, up to now: minReward: -137.0, minAverage: -156.94915254237287\n",
      "Episode 59 done after 138 steps, reward Average: -156.63333333333333, up to now: minReward: -137.0, minAverage: -156.63333333333333\n",
      "Episode 60 done after 139 steps, reward Average: -156.34426229508196, up to now: minReward: -137.0, minAverage: -156.34426229508196\n",
      "Episode 61 done after 138 steps, reward Average: -156.0483870967742, up to now: minReward: -137.0, minAverage: -156.0483870967742\n",
      "Episode 62 done after 138 steps, reward Average: -155.76190476190476, up to now: minReward: -137.0, minAverage: -155.76190476190476\n",
      "Episode 63 done after 138 steps, reward Average: -155.484375, up to now: minReward: -137.0, minAverage: -155.484375\n",
      "Episode 64 done after 138 steps, reward Average: -155.2153846153846, up to now: minReward: -137.0, minAverage: -155.2153846153846\n",
      "Episode 65 done after 138 steps, reward Average: -154.95454545454547, up to now: minReward: -137.0, minAverage: -154.95454545454547\n",
      "Episode 66 done after 137 steps, reward Average: -154.6865671641791, up to now: minReward: -137.0, minAverage: -154.6865671641791\n",
      "Episode 67 done after 138 steps, reward Average: -154.44117647058823, up to now: minReward: -137.0, minAverage: -154.44117647058823\n",
      "Episode 68 done after 137 steps, reward Average: -154.18840579710144, up to now: minReward: -137.0, minAverage: -154.18840579710144\n",
      "Episode 69 done after 139 steps, reward Average: -153.97142857142856, up to now: minReward: -137.0, minAverage: -153.97142857142856\n",
      "Episode 70 done after 137 steps, reward Average: -153.73239436619718, up to now: minReward: -137.0, minAverage: -153.73239436619718\n",
      "Episode 71 done after 138 steps, reward Average: -153.51388888888889, up to now: minReward: -137.0, minAverage: -153.51388888888889\n",
      "Episode 72 done after 138 steps, reward Average: -153.3013698630137, up to now: minReward: -137.0, minAverage: -153.3013698630137\n",
      "Episode 73 done after 138 steps, reward Average: -153.09459459459458, up to now: minReward: -137.0, minAverage: -153.09459459459458\n",
      "Episode 74 done after 140 steps, reward Average: -152.92, up to now: minReward: -137.0, minAverage: -152.92\n",
      "Episode 75 done after 137 steps, reward Average: -152.71052631578948, up to now: minReward: -137.0, minAverage: -152.71052631578948\n",
      "Episode 76 done after 136 steps, reward Average: -152.4935064935065, up to now: minReward: -136.0, minAverage: -152.4935064935065\n",
      "Episode 77 done after 137 steps, reward Average: -152.2948717948718, up to now: minReward: -136.0, minAverage: -152.2948717948718\n",
      "Episode 78 done after 138 steps, reward Average: -152.1139240506329, up to now: minReward: -136.0, minAverage: -152.1139240506329\n",
      "Episode 79 done after 139 steps, reward Average: -151.95, up to now: minReward: -136.0, minAverage: -151.95\n",
      "Episode 80 done after 143 steps, reward Average: -151.8395061728395, up to now: minReward: -136.0, minAverage: -151.8395061728395\n",
      "Episode 81 done after 137 steps, reward Average: -151.65853658536585, up to now: minReward: -136.0, minAverage: -151.65853658536585\n",
      "Episode 82 done after 141 steps, reward Average: -151.53012048192772, up to now: minReward: -136.0, minAverage: -151.53012048192772\n",
      "Episode 83 done after 138 steps, reward Average: -151.36904761904762, up to now: minReward: -136.0, minAverage: -151.36904761904762\n",
      "Episode 84 done after 137 steps, reward Average: -151.2, up to now: minReward: -136.0, minAverage: -151.2\n",
      "Episode 85 done after 140 steps, reward Average: -151.06976744186048, up to now: minReward: -136.0, minAverage: -151.06976744186048\n",
      "Episode 86 done after 137 steps, reward Average: -150.90804597701148, up to now: minReward: -136.0, minAverage: -150.90804597701148\n",
      "Episode 87 done after 138 steps, reward Average: -150.76136363636363, up to now: minReward: -136.0, minAverage: -150.76136363636363\n",
      "Episode 88 done after 138 steps, reward Average: -150.61797752808988, up to now: minReward: -136.0, minAverage: -150.61797752808988\n",
      "Episode 89 done after 137 steps, reward Average: -150.46666666666667, up to now: minReward: -136.0, minAverage: -150.46666666666667\n",
      "Episode 90 done after 139 steps, reward Average: -150.34065934065933, up to now: minReward: -136.0, minAverage: -150.34065934065933\n",
      "Episode 91 done after 139 steps, reward Average: -150.2173913043478, up to now: minReward: -136.0, minAverage: -150.2173913043478\n",
      "Episode 92 done after 137 steps, reward Average: -150.0752688172043, up to now: minReward: -136.0, minAverage: -150.0752688172043\n",
      "Episode 93 done after 139 steps, reward Average: -149.95744680851064, up to now: minReward: -136.0, minAverage: -149.95744680851064\n",
      "Episode 94 done after 137 steps, reward Average: -149.82105263157894, up to now: minReward: -136.0, minAverage: -149.82105263157894\n",
      "Episode 95 done after 137 steps, reward Average: -149.6875, up to now: minReward: -136.0, minAverage: -149.6875\n",
      "Episode 96 done after 140 steps, reward Average: -149.58762886597938, up to now: minReward: -136.0, minAverage: -149.58762886597938\n",
      "Episode 97 done after 140 steps, reward Average: -149.48979591836735, up to now: minReward: -136.0, minAverage: -149.48979591836735\n",
      "Episode 98 done after 137 steps, reward Average: -149.36363636363637, up to now: minReward: -136.0, minAverage: -149.36363636363637\n",
      "Episode 99 done after 137 steps, reward Average: -149.24, up to now: minReward: -136.0, minAverage: -149.24\n",
      "Episode 100 done after 138 steps, reward Average: -148.62, up to now: minReward: -136.0, minAverage: -148.62\n",
      "Episode 101 done after 137 steps, reward Average: -147.99, up to now: minReward: -136.0, minAverage: -147.99\n",
      "Episode 102 done after 137 steps, reward Average: -147.36, up to now: minReward: -136.0, minAverage: -147.36\n",
      "Episode 103 done after 138 steps, reward Average: -146.74, up to now: minReward: -136.0, minAverage: -146.74\n",
      "Episode 104 done after 136 steps, reward Average: -146.1, up to now: minReward: -136.0, minAverage: -146.1\n",
      "Episode 105 done after 139 steps, reward Average: -145.49, up to now: minReward: -136.0, minAverage: -145.49\n",
      "Episode 106 done after 137 steps, reward Average: -144.86, up to now: minReward: -136.0, minAverage: -144.86\n",
      "Episode 107 done after 139 steps, reward Average: -144.25, up to now: minReward: -136.0, minAverage: -144.25\n",
      "Episode 108 done after 138 steps, reward Average: -143.63, up to now: minReward: -136.0, minAverage: -143.63\n",
      "Episode 109 done after 137 steps, reward Average: -143.0, up to now: minReward: -136.0, minAverage: -143.0\n",
      "Episode 110 done after 137 steps, reward Average: -142.37, up to now: minReward: -136.0, minAverage: -142.37\n",
      "Episode 111 done after 139 steps, reward Average: -141.76, up to now: minReward: -136.0, minAverage: -141.76\n",
      "Episode 112 done after 137 steps, reward Average: -141.13, up to now: minReward: -136.0, minAverage: -141.13\n",
      "Episode 113 done after 137 steps, reward Average: -140.8, up to now: minReward: -136.0, minAverage: -140.8\n",
      "Episode 114 done after 137 steps, reward Average: -140.53, up to now: minReward: -136.0, minAverage: -140.53\n",
      "Episode 115 done after 137 steps, reward Average: -139.9, up to now: minReward: -136.0, minAverage: -139.9\n",
      "Episode 116 done after 138 steps, reward Average: -139.72, up to now: minReward: -136.0, minAverage: -139.72\n",
      "Episode 117 done after 137 steps, reward Average: -139.27, up to now: minReward: -136.0, minAverage: -139.27\n",
      "Episode 118 done after 137 steps, reward Average: -139.09, up to now: minReward: -136.0, minAverage: -139.09\n",
      "Episode 119 done after 136 steps, reward Average: -138.83, up to now: minReward: -136.0, minAverage: -138.83\n",
      "Episode 120 done after 136 steps, reward Average: -138.69, up to now: minReward: -136.0, minAverage: -138.69\n",
      "Episode 121 done after 138 steps, reward Average: -138.69, up to now: minReward: -136.0, minAverage: -138.69\n",
      "Episode 122 done after 137 steps, reward Average: -138.6, up to now: minReward: -136.0, minAverage: -138.6\n",
      "Episode 123 done after 138 steps, reward Average: -138.54, up to now: minReward: -136.0, minAverage: -138.54\n",
      "Episode 124 done after 138 steps, reward Average: -138.51, up to now: minReward: -136.0, minAverage: -138.51\n",
      "Episode 125 done after 138 steps, reward Average: -138.45, up to now: minReward: -136.0, minAverage: -138.45\n",
      "Episode 126 done after 137 steps, reward Average: -138.38, up to now: minReward: -136.0, minAverage: -138.38\n",
      "Episode 127 done after 138 steps, reward Average: -138.34, up to now: minReward: -136.0, minAverage: -138.34\n",
      "Episode 128 done after 137 steps, reward Average: -138.32, up to now: minReward: -136.0, minAverage: -138.32\n",
      "Episode 129 done after 137 steps, reward Average: -138.19, up to now: minReward: -136.0, minAverage: -138.19\n",
      "Episode 130 done after 137 steps, reward Average: -138.17, up to now: minReward: -136.0, minAverage: -138.17\n",
      "Episode 131 done after 138 steps, reward Average: -138.14, up to now: minReward: -136.0, minAverage: -138.14\n",
      "Episode 132 done after 137 steps, reward Average: -138.11, up to now: minReward: -136.0, minAverage: -138.11\n",
      "Episode 133 done after 138 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -138.07\n",
      "Episode 134 done after 143 steps, reward Average: -138.11, up to now: minReward: -136.0, minAverage: -138.07\n",
      "Episode 135 done after 137 steps, reward Average: -138.09, up to now: minReward: -136.0, minAverage: -138.07\n",
      "Episode 136 done after 137 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -138.04\n",
      "Episode 137 done after 141 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -138.04\n",
      "Episode 138 done after 138 steps, reward Average: -138.03, up to now: minReward: -136.0, minAverage: -138.03\n",
      "Episode 139 done after 142 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -138.03\n",
      "Episode 140 done after 141 steps, reward Average: -138.1, up to now: minReward: -136.0, minAverage: -138.03\n",
      "Episode 141 done after 137 steps, reward Average: -138.09, up to now: minReward: -136.0, minAverage: -138.03\n",
      "Episode 142 done after 136 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -138.03\n",
      "Episode 143 done after 137 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -138.03\n",
      "Episode 144 done after 137 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -138.02\n",
      "Episode 145 done after 139 steps, reward Average: -138.03, up to now: minReward: -136.0, minAverage: -138.02\n",
      "Episode 146 done after 139 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -138.02\n",
      "Episode 147 done after 141 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -138.02\n",
      "Episode 148 done after 137 steps, reward Average: -138.06, up to now: minReward: -136.0, minAverage: -138.02\n",
      "Episode 149 done after 137 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -138.02\n",
      "Episode 150 done after 137 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -138.02\n",
      "Episode 151 done after 138 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -138.02\n",
      "Episode 152 done after 137 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -138.01\n",
      "Episode 153 done after 143 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -138.01\n",
      "Episode 154 done after 137 steps, reward Average: -138.03, up to now: minReward: -136.0, minAverage: -138.01\n",
      "Episode 155 done after 137 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -138.01\n",
      "Episode 156 done after 137 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -138.0\n",
      "Episode 157 done after 139 steps, reward Average: -137.97, up to now: minReward: -136.0, minAverage: -137.97\n",
      "Episode 158 done after 137 steps, reward Average: -137.97, up to now: minReward: -136.0, minAverage: -137.97\n",
      "Episode 159 done after 136 steps, reward Average: -137.95, up to now: minReward: -136.0, minAverage: -137.95\n",
      "Episode 160 done after 138 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.94\n",
      "Episode 161 done after 137 steps, reward Average: -137.93, up to now: minReward: -136.0, minAverage: -137.93\n",
      "Episode 162 done after 137 steps, reward Average: -137.92, up to now: minReward: -136.0, minAverage: -137.92\n",
      "Episode 163 done after 139 steps, reward Average: -137.93, up to now: minReward: -136.0, minAverage: -137.92\n",
      "Episode 164 done after 137 steps, reward Average: -137.92, up to now: minReward: -136.0, minAverage: -137.92\n",
      "Episode 165 done after 139 steps, reward Average: -137.93, up to now: minReward: -136.0, minAverage: -137.92\n",
      "Episode 166 done after 137 steps, reward Average: -137.93, up to now: minReward: -136.0, minAverage: -137.92\n",
      "Episode 167 done after 137 steps, reward Average: -137.92, up to now: minReward: -136.0, minAverage: -137.92\n",
      "Episode 168 done after 138 steps, reward Average: -137.93, up to now: minReward: -136.0, minAverage: -137.92\n",
      "Episode 169 done after 137 steps, reward Average: -137.91, up to now: minReward: -136.0, minAverage: -137.91\n",
      "Episode 170 done after 136 steps, reward Average: -137.9, up to now: minReward: -136.0, minAverage: -137.9\n",
      "Episode 171 done after 138 steps, reward Average: -137.9, up to now: minReward: -136.0, minAverage: -137.9\n",
      "Episode 172 done after 136 steps, reward Average: -137.88, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 173 done after 144 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 174 done after 138 steps, reward Average: -137.92, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 175 done after 138 steps, reward Average: -137.93, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 176 done after 138 steps, reward Average: -137.95, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 177 done after 143 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 178 done after 138 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 179 done after 147 steps, reward Average: -138.09, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 180 done after 139 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 181 done after 142 steps, reward Average: -138.1, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 182 done after 137 steps, reward Average: -138.06, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 183 done after 139 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 184 done after 138 steps, reward Average: -138.08, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 185 done after 137 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 186 done after 137 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 187 done after 137 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 188 done after 138 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 189 done after 137 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 190 done after 137 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 191 done after 138 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 192 done after 137 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 193 done after 136 steps, reward Average: -137.98, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 194 done after 138 steps, reward Average: -137.99, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 195 done after 137 steps, reward Average: -137.99, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 196 done after 138 steps, reward Average: -137.97, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 197 done after 137 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 198 done after 137 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 199 done after 137 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 200 done after 138 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 201 done after 137 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 202 done after 143 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 203 done after 137 steps, reward Average: -137.99, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 204 done after 144 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 205 done after 137 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 206 done after 137 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 207 done after 139 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 208 done after 137 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 209 done after 137 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 210 done after 138 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 211 done after 137 steps, reward Average: -138.03, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 212 done after 137 steps, reward Average: -138.03, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 213 done after 138 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 214 done after 137 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 215 done after 138 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 216 done after 138 steps, reward Average: -138.05, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 217 done after 138 steps, reward Average: -138.06, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 218 done after 138 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 219 done after 139 steps, reward Average: -138.1, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 220 done after 139 steps, reward Average: -138.13, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 221 done after 137 steps, reward Average: -138.12, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 222 done after 137 steps, reward Average: -138.12, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 223 done after 137 steps, reward Average: -138.11, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 224 done after 137 steps, reward Average: -138.1, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 225 done after 137 steps, reward Average: -138.09, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 226 done after 137 steps, reward Average: -138.09, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 227 done after 136 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 228 done after 137 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 229 done after 139 steps, reward Average: -138.09, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 230 done after 137 steps, reward Average: -138.09, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 231 done after 136 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 232 done after 137 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 233 done after 138 steps, reward Average: -138.07, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 234 done after 136 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 235 done after 139 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 236 done after 137 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 237 done after 139 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 238 done after 138 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 239 done after 138 steps, reward Average: -137.96, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 240 done after 153 steps, reward Average: -138.08, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 241 done after 137 steps, reward Average: -138.08, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 242 done after 138 steps, reward Average: -138.1, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 243 done after 140 steps, reward Average: -138.13, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 244 done after 137 steps, reward Average: -138.13, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 245 done after 137 steps, reward Average: -138.11, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 246 done after 136 steps, reward Average: -138.08, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 247 done after 137 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 248 done after 137 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 249 done after 136 steps, reward Average: -138.03, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 250 done after 138 steps, reward Average: -138.04, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 251 done after 136 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 252 done after 137 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 253 done after 137 steps, reward Average: -137.96, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 254 done after 140 steps, reward Average: -137.99, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 255 done after 138 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 256 done after 137 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 257 done after 139 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 258 done after 136 steps, reward Average: -137.99, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 259 done after 138 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 260 done after 137 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 261 done after 138 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 262 done after 137 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 263 done after 136 steps, reward Average: -137.98, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 264 done after 139 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 265 done after 137 steps, reward Average: -137.98, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 266 done after 137 steps, reward Average: -137.98, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 267 done after 138 steps, reward Average: -137.99, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 268 done after 140 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 269 done after 137 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 270 done after 136 steps, reward Average: -138.01, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 271 done after 137 steps, reward Average: -138.0, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 272 done after 138 steps, reward Average: -138.02, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 273 done after 138 steps, reward Average: -137.96, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 274 done after 137 steps, reward Average: -137.95, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 275 done after 137 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 276 done after 138 steps, reward Average: -137.94, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 277 done after 140 steps, reward Average: -137.91, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 278 done after 137 steps, reward Average: -137.9, up to now: minReward: -136.0, minAverage: -137.88\n",
      "Episode 279 done after 137 steps, reward Average: -137.8, up to now: minReward: -136.0, minAverage: -137.8\n",
      "Episode 280 done after 137 steps, reward Average: -137.78, up to now: minReward: -136.0, minAverage: -137.78\n",
      "Episode 281 done after 138 steps, reward Average: -137.74, up to now: minReward: -136.0, minAverage: -137.74\n",
      "Episode 282 done after 136 steps, reward Average: -137.73, up to now: minReward: -136.0, minAverage: -137.73\n",
      "Episode 283 done after 138 steps, reward Average: -137.72, up to now: minReward: -136.0, minAverage: -137.72\n",
      "Episode 284 done after 137 steps, reward Average: -137.71, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 285 done after 137 steps, reward Average: -137.71, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 286 done after 138 steps, reward Average: -137.72, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 287 done after 137 steps, reward Average: -137.72, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 288 done after 137 steps, reward Average: -137.71, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 289 done after 137 steps, reward Average: -137.71, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 290 done after 137 steps, reward Average: -137.71, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 291 done after 139 steps, reward Average: -137.72, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 292 done after 137 steps, reward Average: -137.72, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 293 done after 140 steps, reward Average: -137.76, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 294 done after 141 steps, reward Average: -137.79, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 295 done after 137 steps, reward Average: -137.79, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 296 done after 137 steps, reward Average: -137.78, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 297 done after 138 steps, reward Average: -137.79, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 298 done after 139 steps, reward Average: -137.81, up to now: minReward: -136.0, minAverage: -137.71\n",
      "Episode 299 done after 137 steps, reward Average: -137.81, up to now: minReward: -136.0, minAverage: -137.71\n",
      "final result: \n",
      "286 times arrived in 300 episodes, first time in episode 13\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24742beb860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 25:\n",
    "            return 0.65\n",
    "        elif episode < 35:\n",
    "            return 0.6\n",
    "        elif episode < 45:\n",
    "            return 0.55\n",
    "        elif episode < 55:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.4\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) +  (R +  Q(S, A)  Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
