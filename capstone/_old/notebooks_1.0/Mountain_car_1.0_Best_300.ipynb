{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling 888, Best version, 300 Episodes #\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 11:42:54,098] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 123 steps, reward Average: -195.72222222222223, up to now: minReward: -123.0, minAverage: -195.72222222222223\n",
      "Episode 18 done after 117 steps, reward Average: -191.57894736842104, up to now: minReward: -117.0, minAverage: -191.57894736842104\n",
      "Episode 19 done after 136 steps, reward Average: -188.8, up to now: minReward: -117.0, minAverage: -188.8\n",
      "Episode 20 done after 124 steps, reward Average: -185.71428571428572, up to now: minReward: -117.0, minAverage: -185.71428571428572\n",
      "Episode 21 done after 115 steps, reward Average: -182.5, up to now: minReward: -115.0, minAverage: -182.5\n",
      "Episode 22 done after 116 steps, reward Average: -179.6086956521739, up to now: minReward: -115.0, minAverage: -179.6086956521739\n",
      "Episode 23 done after 117 steps, reward Average: -177.0, up to now: minReward: -115.0, minAverage: -177.0\n",
      "Episode 24 done after 114 steps, reward Average: -174.48, up to now: minReward: -114.0, minAverage: -174.48\n",
      "Episode 25 done after 117 steps, reward Average: -172.26923076923077, up to now: minReward: -114.0, minAverage: -172.26923076923077\n",
      "Episode 26 done after 119 steps, reward Average: -170.2962962962963, up to now: minReward: -114.0, minAverage: -170.2962962962963\n",
      "Episode 27 done after 119 steps, reward Average: -168.46428571428572, up to now: minReward: -114.0, minAverage: -168.46428571428572\n",
      "Episode 28 done after 119 steps, reward Average: -166.75862068965517, up to now: minReward: -114.0, minAverage: -166.75862068965517\n",
      "Episode 29 done after 113 steps, reward Average: -164.96666666666667, up to now: minReward: -113.0, minAverage: -164.96666666666667\n",
      "Episode 30 done after 110 steps, reward Average: -163.19354838709677, up to now: minReward: -110.0, minAverage: -163.19354838709677\n",
      "Episode 31 done after 114 steps, reward Average: -161.65625, up to now: minReward: -110.0, minAverage: -161.65625\n",
      "Episode 32 done after 135 steps, reward Average: -160.84848484848484, up to now: minReward: -110.0, minAverage: -160.84848484848484\n",
      "Episode 33 done after 109 steps, reward Average: -159.3235294117647, up to now: minReward: -109.0, minAverage: -159.3235294117647\n",
      "Episode 34 done after 119 steps, reward Average: -158.17142857142858, up to now: minReward: -109.0, minAverage: -158.17142857142858\n",
      "Episode 35 done after 108 steps, reward Average: -156.77777777777777, up to now: minReward: -108.0, minAverage: -156.77777777777777\n",
      "Episode 36 done after 113 steps, reward Average: -155.59459459459458, up to now: minReward: -108.0, minAverage: -155.59459459459458\n",
      "Episode 37 done after 110 steps, reward Average: -154.39473684210526, up to now: minReward: -108.0, minAverage: -154.39473684210526\n",
      "Episode 38 done after 111 steps, reward Average: -153.28205128205127, up to now: minReward: -108.0, minAverage: -153.28205128205127\n",
      "Episode 39 done after 111 steps, reward Average: -152.225, up to now: minReward: -108.0, minAverage: -152.225\n",
      "Episode 40 done after 110 steps, reward Average: -151.1951219512195, up to now: minReward: -108.0, minAverage: -151.1951219512195\n",
      "Episode 41 done after 110 steps, reward Average: -150.21428571428572, up to now: minReward: -108.0, minAverage: -150.21428571428572\n",
      "Episode 42 done after 107 steps, reward Average: -149.2093023255814, up to now: minReward: -107.0, minAverage: -149.2093023255814\n",
      "Episode 43 done after 109 steps, reward Average: -148.29545454545453, up to now: minReward: -107.0, minAverage: -148.29545454545453\n",
      "Episode 44 done after 180 steps, reward Average: -149.0, up to now: minReward: -107.0, minAverage: -148.29545454545453\n",
      "Episode 45 done after 106 steps, reward Average: -148.06521739130434, up to now: minReward: -106.0, minAverage: -148.06521739130434\n",
      "Episode 46 done after 106 steps, reward Average: -147.17021276595744, up to now: minReward: -106.0, minAverage: -147.17021276595744\n",
      "Episode 47 done after 122 steps, reward Average: -146.64583333333334, up to now: minReward: -106.0, minAverage: -146.64583333333334\n",
      "Episode 48 done after 107 steps, reward Average: -145.83673469387756, up to now: minReward: -106.0, minAverage: -145.83673469387756\n",
      "Episode 49 done after 108 steps, reward Average: -145.08, up to now: minReward: -106.0, minAverage: -145.08\n",
      "Episode 50 done after 200 steps, reward Average: -146.15686274509804, up to now: minReward: -106.0, minAverage: -145.08\n",
      "Episode 51 done after 200 steps, reward Average: -147.19230769230768, up to now: minReward: -106.0, minAverage: -145.08\n",
      "Episode 52 done after 131 steps, reward Average: -146.88679245283018, up to now: minReward: -106.0, minAverage: -145.08\n",
      "Episode 53 done after 107 steps, reward Average: -146.14814814814815, up to now: minReward: -106.0, minAverage: -145.08\n",
      "Episode 54 done after 108 steps, reward Average: -145.45454545454547, up to now: minReward: -106.0, minAverage: -145.08\n",
      "Episode 55 done after 110 steps, reward Average: -144.82142857142858, up to now: minReward: -106.0, minAverage: -144.82142857142858\n",
      "Episode 56 done after 107 steps, reward Average: -144.1578947368421, up to now: minReward: -106.0, minAverage: -144.1578947368421\n",
      "Episode 57 done after 110 steps, reward Average: -143.56896551724137, up to now: minReward: -106.0, minAverage: -143.56896551724137\n",
      "Episode 58 done after 109 steps, reward Average: -142.98305084745763, up to now: minReward: -106.0, minAverage: -142.98305084745763\n",
      "Episode 59 done after 109 steps, reward Average: -142.41666666666666, up to now: minReward: -106.0, minAverage: -142.41666666666666\n",
      "Episode 60 done after 118 steps, reward Average: -142.01639344262296, up to now: minReward: -106.0, minAverage: -142.01639344262296\n",
      "Episode 61 done after 130 steps, reward Average: -141.82258064516128, up to now: minReward: -106.0, minAverage: -141.82258064516128\n",
      "Episode 62 done after 107 steps, reward Average: -141.26984126984127, up to now: minReward: -106.0, minAverage: -141.26984126984127\n",
      "Episode 63 done after 108 steps, reward Average: -140.75, up to now: minReward: -106.0, minAverage: -140.75\n",
      "Episode 64 done after 113 steps, reward Average: -140.3230769230769, up to now: minReward: -106.0, minAverage: -140.3230769230769\n",
      "Episode 65 done after 108 steps, reward Average: -139.83333333333334, up to now: minReward: -106.0, minAverage: -139.83333333333334\n",
      "Episode 66 done after 108 steps, reward Average: -139.3582089552239, up to now: minReward: -106.0, minAverage: -139.3582089552239\n",
      "Episode 67 done after 124 steps, reward Average: -139.13235294117646, up to now: minReward: -106.0, minAverage: -139.13235294117646\n",
      "Episode 68 done after 130 steps, reward Average: -139.0, up to now: minReward: -106.0, minAverage: -139.0\n",
      "Episode 69 done after 108 steps, reward Average: -138.55714285714285, up to now: minReward: -106.0, minAverage: -138.55714285714285\n",
      "Episode 70 done after 107 steps, reward Average: -138.11267605633802, up to now: minReward: -106.0, minAverage: -138.11267605633802\n",
      "Episode 71 done after 107 steps, reward Average: -137.68055555555554, up to now: minReward: -106.0, minAverage: -137.68055555555554\n",
      "Episode 72 done after 108 steps, reward Average: -137.27397260273972, up to now: minReward: -106.0, minAverage: -137.27397260273972\n",
      "Episode 73 done after 113 steps, reward Average: -136.94594594594594, up to now: minReward: -106.0, minAverage: -136.94594594594594\n",
      "Episode 74 done after 113 steps, reward Average: -136.62666666666667, up to now: minReward: -106.0, minAverage: -136.62666666666667\n",
      "Episode 75 done after 109 steps, reward Average: -136.26315789473685, up to now: minReward: -106.0, minAverage: -136.26315789473685\n",
      "Episode 76 done after 108 steps, reward Average: -135.8961038961039, up to now: minReward: -106.0, minAverage: -135.8961038961039\n",
      "Episode 77 done after 132 steps, reward Average: -135.84615384615384, up to now: minReward: -106.0, minAverage: -135.84615384615384\n",
      "Episode 78 done after 125 steps, reward Average: -135.70886075949366, up to now: minReward: -106.0, minAverage: -135.70886075949366\n",
      "Episode 79 done after 107 steps, reward Average: -135.35, up to now: minReward: -106.0, minAverage: -135.35\n",
      "Episode 80 done after 127 steps, reward Average: -135.2469135802469, up to now: minReward: -106.0, minAverage: -135.2469135802469\n",
      "Episode 81 done after 108 steps, reward Average: -134.91463414634146, up to now: minReward: -106.0, minAverage: -134.91463414634146\n",
      "Episode 82 done after 118 steps, reward Average: -134.710843373494, up to now: minReward: -106.0, minAverage: -134.710843373494\n",
      "Episode 83 done after 108 steps, reward Average: -134.39285714285714, up to now: minReward: -106.0, minAverage: -134.39285714285714\n",
      "Episode 84 done after 108 steps, reward Average: -134.08235294117648, up to now: minReward: -106.0, minAverage: -134.08235294117648\n",
      "Episode 85 done after 108 steps, reward Average: -133.77906976744185, up to now: minReward: -106.0, minAverage: -133.77906976744185\n",
      "Episode 86 done after 200 steps, reward Average: -134.54022988505747, up to now: minReward: -106.0, minAverage: -133.77906976744185\n",
      "Episode 87 done after 113 steps, reward Average: -134.29545454545453, up to now: minReward: -106.0, minAverage: -133.77906976744185\n",
      "Episode 88 done after 107 steps, reward Average: -133.98876404494382, up to now: minReward: -106.0, minAverage: -133.77906976744185\n",
      "Episode 89 done after 110 steps, reward Average: -133.72222222222223, up to now: minReward: -106.0, minAverage: -133.72222222222223\n",
      "Episode 90 done after 107 steps, reward Average: -133.42857142857142, up to now: minReward: -106.0, minAverage: -133.42857142857142\n",
      "Episode 91 done after 108 steps, reward Average: -133.15217391304347, up to now: minReward: -106.0, minAverage: -133.15217391304347\n",
      "Episode 92 done after 125 steps, reward Average: -133.06451612903226, up to now: minReward: -106.0, minAverage: -133.06451612903226\n",
      "Episode 93 done after 107 steps, reward Average: -132.7872340425532, up to now: minReward: -106.0, minAverage: -132.7872340425532\n",
      "Episode 94 done after 126 steps, reward Average: -132.7157894736842, up to now: minReward: -106.0, minAverage: -132.7157894736842\n",
      "Episode 95 done after 125 steps, reward Average: -132.63541666666666, up to now: minReward: -106.0, minAverage: -132.63541666666666\n",
      "Episode 96 done after 109 steps, reward Average: -132.39175257731958, up to now: minReward: -106.0, minAverage: -132.39175257731958\n",
      "Episode 97 done after 200 steps, reward Average: -133.08163265306123, up to now: minReward: -106.0, minAverage: -132.39175257731958\n",
      "Episode 98 done after 108 steps, reward Average: -132.82828282828282, up to now: minReward: -106.0, minAverage: -132.39175257731958\n",
      "Episode 99 done after 118 steps, reward Average: -132.68, up to now: minReward: -106.0, minAverage: -132.39175257731958\n",
      "Episode 100 done after 108 steps, reward Average: -131.76, up to now: minReward: -106.0, minAverage: -131.76\n",
      "Episode 101 done after 108 steps, reward Average: -130.84, up to now: minReward: -106.0, minAverage: -130.84\n",
      "Episode 102 done after 118 steps, reward Average: -130.02, up to now: minReward: -106.0, minAverage: -130.02\n",
      "Episode 103 done after 200 steps, reward Average: -130.02, up to now: minReward: -106.0, minAverage: -130.02\n",
      "Episode 104 done after 109 steps, reward Average: -129.11, up to now: minReward: -106.0, minAverage: -129.11\n",
      "Episode 105 done after 109 steps, reward Average: -128.2, up to now: minReward: -106.0, minAverage: -128.2\n",
      "Episode 106 done after 128 steps, reward Average: -127.48, up to now: minReward: -106.0, minAverage: -127.48\n",
      "Episode 107 done after 108 steps, reward Average: -126.56, up to now: minReward: -106.0, minAverage: -126.56\n",
      "Episode 108 done after 135 steps, reward Average: -125.91, up to now: minReward: -106.0, minAverage: -125.91\n",
      "Episode 109 done after 110 steps, reward Average: -125.01, up to now: minReward: -106.0, minAverage: -125.01\n",
      "Episode 110 done after 108 steps, reward Average: -124.09, up to now: minReward: -106.0, minAverage: -124.09\n",
      "Episode 111 done after 130 steps, reward Average: -123.39, up to now: minReward: -106.0, minAverage: -123.39\n",
      "Episode 112 done after 125 steps, reward Average: -122.64, up to now: minReward: -106.0, minAverage: -122.64\n",
      "Episode 113 done after 108 steps, reward Average: -121.72, up to now: minReward: -106.0, minAverage: -121.72\n",
      "Episode 114 done after 107 steps, reward Average: -120.79, up to now: minReward: -106.0, minAverage: -120.79\n",
      "Episode 115 done after 125 steps, reward Average: -120.04, up to now: minReward: -106.0, minAverage: -120.04\n",
      "Episode 116 done after 118 steps, reward Average: -119.22, up to now: minReward: -106.0, minAverage: -119.22\n",
      "Episode 117 done after 108 steps, reward Average: -119.07, up to now: minReward: -106.0, minAverage: -119.07\n",
      "Episode 118 done after 114 steps, reward Average: -119.04, up to now: minReward: -106.0, minAverage: -119.04\n",
      "Episode 119 done after 108 steps, reward Average: -118.76, up to now: minReward: -106.0, minAverage: -118.76\n",
      "Episode 120 done after 108 steps, reward Average: -118.6, up to now: minReward: -106.0, minAverage: -118.6\n",
      "Episode 121 done after 107 steps, reward Average: -118.52, up to now: minReward: -106.0, minAverage: -118.52\n",
      "Episode 122 done after 108 steps, reward Average: -118.44, up to now: minReward: -106.0, minAverage: -118.44\n",
      "Episode 123 done after 110 steps, reward Average: -118.37, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 124 done after 200 steps, reward Average: -119.23, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 125 done after 108 steps, reward Average: -119.14, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 126 done after 130 steps, reward Average: -119.25, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 127 done after 109 steps, reward Average: -119.15, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 128 done after 108 steps, reward Average: -119.04, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 129 done after 200 steps, reward Average: -119.91, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 130 done after 108 steps, reward Average: -119.89, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 131 done after 110 steps, reward Average: -119.85, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 132 done after 106 steps, reward Average: -119.56, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 133 done after 131 steps, reward Average: -119.78, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 134 done after 107 steps, reward Average: -119.66, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 135 done after 108 steps, reward Average: -119.66, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 136 done after 112 steps, reward Average: -119.65, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 137 done after 110 steps, reward Average: -119.65, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 138 done after 111 steps, reward Average: -119.65, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 139 done after 125 steps, reward Average: -119.79, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 140 done after 108 steps, reward Average: -119.77, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 141 done after 131 steps, reward Average: -119.98, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 142 done after 124 steps, reward Average: -120.15, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 143 done after 118 steps, reward Average: -120.24, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 144 done after 200 steps, reward Average: -120.44, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 145 done after 108 steps, reward Average: -120.46, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 146 done after 108 steps, reward Average: -120.48, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 147 done after 109 steps, reward Average: -120.35, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 148 done after 107 steps, reward Average: -120.35, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 149 done after 108 steps, reward Average: -120.35, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 150 done after 114 steps, reward Average: -119.49, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 151 done after 108 steps, reward Average: -118.57, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 152 done after 129 steps, reward Average: -118.55, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 153 done after 108 steps, reward Average: -118.56, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 154 done after 109 steps, reward Average: -118.57, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 155 done after 108 steps, reward Average: -118.55, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 156 done after 108 steps, reward Average: -118.56, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 157 done after 180 steps, reward Average: -119.26, up to now: minReward: -106.0, minAverage: -118.37\n",
      "Episode 158 done after 84 steps, reward Average: -119.01, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 159 done after 107 steps, reward Average: -118.99, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 160 done after 108 steps, reward Average: -118.89, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 161 done after 108 steps, reward Average: -118.67, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 162 done after 200 steps, reward Average: -119.6, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 163 done after 109 steps, reward Average: -119.61, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 164 done after 109 steps, reward Average: -119.57, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 165 done after 113 steps, reward Average: -119.62, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 166 done after 200 steps, reward Average: -120.54, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 167 done after 109 steps, reward Average: -120.39, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 168 done after 200 steps, reward Average: -121.09, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 169 done after 108 steps, reward Average: -121.09, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 170 done after 109 steps, reward Average: -121.11, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 171 done after 108 steps, reward Average: -121.12, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 172 done after 111 steps, reward Average: -121.15, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 173 done after 108 steps, reward Average: -121.1, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 174 done after 108 steps, reward Average: -121.05, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 175 done after 110 steps, reward Average: -121.06, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 176 done after 107 steps, reward Average: -121.05, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 177 done after 107 steps, reward Average: -120.8, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 178 done after 108 steps, reward Average: -120.63, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 179 done after 107 steps, reward Average: -120.63, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 180 done after 123 steps, reward Average: -120.59, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 181 done after 131 steps, reward Average: -120.82, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 182 done after 108 steps, reward Average: -120.72, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 183 done after 112 steps, reward Average: -120.76, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 184 done after 105 steps, reward Average: -120.73, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 185 done after 110 steps, reward Average: -120.75, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 186 done after 108 steps, reward Average: -119.83, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 187 done after 106 steps, reward Average: -119.76, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 188 done after 107 steps, reward Average: -119.76, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 189 done after 132 steps, reward Average: -119.98, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 190 done after 108 steps, reward Average: -119.99, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 191 done after 107 steps, reward Average: -119.98, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 192 done after 111 steps, reward Average: -119.84, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 193 done after 105 steps, reward Average: -119.82, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 194 done after 200 steps, reward Average: -120.56, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 195 done after 105 steps, reward Average: -120.36, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 196 done after 106 steps, reward Average: -120.33, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 197 done after 107 steps, reward Average: -119.4, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 198 done after 107 steps, reward Average: -119.39, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 199 done after 110 steps, reward Average: -119.31, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 200 done after 105 steps, reward Average: -119.28, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 201 done after 109 steps, reward Average: -119.29, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 202 done after 106 steps, reward Average: -119.17, up to now: minReward: -84.0, minAverage: -118.37\n",
      "Episode 203 done after 105 steps, reward Average: -118.22, up to now: minReward: -84.0, minAverage: -118.22\n",
      "Episode 204 done after 108 steps, reward Average: -118.21, up to now: minReward: -84.0, minAverage: -118.21\n",
      "Episode 205 done after 105 steps, reward Average: -118.17, up to now: minReward: -84.0, minAverage: -118.17\n",
      "Episode 206 done after 200 steps, reward Average: -118.89, up to now: minReward: -84.0, minAverage: -118.17\n",
      "Episode 207 done after 106 steps, reward Average: -118.87, up to now: minReward: -84.0, minAverage: -118.17\n",
      "Episode 208 done after 106 steps, reward Average: -118.58, up to now: minReward: -84.0, minAverage: -118.17\n",
      "Episode 209 done after 106 steps, reward Average: -118.54, up to now: minReward: -84.0, minAverage: -118.17\n",
      "Episode 210 done after 108 steps, reward Average: -118.54, up to now: minReward: -84.0, minAverage: -118.17\n",
      "Episode 211 done after 106 steps, reward Average: -118.3, up to now: minReward: -84.0, minAverage: -118.17\n",
      "Episode 212 done after 105 steps, reward Average: -118.1, up to now: minReward: -84.0, minAverage: -118.1\n",
      "Episode 213 done after 105 steps, reward Average: -118.07, up to now: minReward: -84.0, minAverage: -118.07\n",
      "Episode 214 done after 106 steps, reward Average: -118.06, up to now: minReward: -84.0, minAverage: -118.06\n",
      "Episode 215 done after 106 steps, reward Average: -117.87, up to now: minReward: -84.0, minAverage: -117.87\n",
      "Episode 216 done after 110 steps, reward Average: -117.79, up to now: minReward: -84.0, minAverage: -117.79\n",
      "Episode 217 done after 106 steps, reward Average: -117.77, up to now: minReward: -84.0, minAverage: -117.77\n",
      "Episode 218 done after 108 steps, reward Average: -117.71, up to now: minReward: -84.0, minAverage: -117.71\n",
      "Episode 219 done after 106 steps, reward Average: -117.69, up to now: minReward: -84.0, minAverage: -117.69\n",
      "Episode 220 done after 126 steps, reward Average: -117.87, up to now: minReward: -84.0, minAverage: -117.69\n",
      "Episode 221 done after 106 steps, reward Average: -117.86, up to now: minReward: -84.0, minAverage: -117.69\n",
      "Episode 222 done after 110 steps, reward Average: -117.88, up to now: minReward: -84.0, minAverage: -117.69\n",
      "Episode 223 done after 108 steps, reward Average: -117.86, up to now: minReward: -84.0, minAverage: -117.69\n",
      "Episode 224 done after 106 steps, reward Average: -116.92, up to now: minReward: -84.0, minAverage: -116.92\n",
      "Episode 225 done after 106 steps, reward Average: -116.9, up to now: minReward: -84.0, minAverage: -116.9\n",
      "Episode 226 done after 106 steps, reward Average: -116.66, up to now: minReward: -84.0, minAverage: -116.66\n",
      "Episode 227 done after 106 steps, reward Average: -116.63, up to now: minReward: -84.0, minAverage: -116.63\n",
      "Episode 228 done after 107 steps, reward Average: -116.62, up to now: minReward: -84.0, minAverage: -116.62\n",
      "Episode 229 done after 106 steps, reward Average: -115.68, up to now: minReward: -84.0, minAverage: -115.68\n",
      "Episode 230 done after 107 steps, reward Average: -115.67, up to now: minReward: -84.0, minAverage: -115.67\n",
      "Episode 231 done after 108 steps, reward Average: -115.65, up to now: minReward: -84.0, minAverage: -115.65\n",
      "Episode 232 done after 106 steps, reward Average: -115.65, up to now: minReward: -84.0, minAverage: -115.65\n",
      "Episode 233 done after 108 steps, reward Average: -115.42, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 234 done after 133 steps, reward Average: -115.68, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 235 done after 107 steps, reward Average: -115.67, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 236 done after 106 steps, reward Average: -115.61, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 237 done after 200 steps, reward Average: -116.51, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 238 done after 200 steps, reward Average: -117.4, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 239 done after 132 steps, reward Average: -117.47, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 240 done after 200 steps, reward Average: -118.39, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 241 done after 108 steps, reward Average: -118.16, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 242 done after 109 steps, reward Average: -118.01, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 243 done after 109 steps, reward Average: -117.92, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 244 done after 107 steps, reward Average: -116.99, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 245 done after 106 steps, reward Average: -116.97, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 246 done after 106 steps, reward Average: -116.95, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 247 done after 108 steps, reward Average: -116.94, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 248 done after 106 steps, reward Average: -116.93, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 249 done after 109 steps, reward Average: -116.94, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 250 done after 105 steps, reward Average: -116.85, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 251 done after 200 steps, reward Average: -117.77, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 252 done after 106 steps, reward Average: -117.54, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 253 done after 108 steps, reward Average: -117.54, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 254 done after 106 steps, reward Average: -117.51, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 255 done after 110 steps, reward Average: -117.53, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 256 done after 107 steps, reward Average: -117.52, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 257 done after 200 steps, reward Average: -117.72, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 258 done after 108 steps, reward Average: -117.96, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 259 done after 107 steps, reward Average: -117.96, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 260 done after 106 steps, reward Average: -117.94, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 261 done after 107 steps, reward Average: -117.93, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 262 done after 106 steps, reward Average: -116.99, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 263 done after 106 steps, reward Average: -116.96, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 264 done after 107 steps, reward Average: -116.94, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 265 done after 108 steps, reward Average: -116.89, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 266 done after 106 steps, reward Average: -115.95, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 267 done after 111 steps, reward Average: -115.97, up to now: minReward: -84.0, minAverage: -115.42\n",
      "Episode 268 done after 106 steps, reward Average: -115.03, up to now: minReward: -84.0, minAverage: -115.03\n",
      "Episode 269 done after 106 steps, reward Average: -115.01, up to now: minReward: -84.0, minAverage: -115.01\n",
      "Episode 270 done after 106 steps, reward Average: -114.98, up to now: minReward: -84.0, minAverage: -114.98\n",
      "Episode 271 done after 108 steps, reward Average: -114.98, up to now: minReward: -84.0, minAverage: -114.98\n",
      "Episode 272 done after 108 steps, reward Average: -114.95, up to now: minReward: -84.0, minAverage: -114.95\n",
      "Episode 273 done after 108 steps, reward Average: -114.95, up to now: minReward: -84.0, minAverage: -114.95\n",
      "Episode 274 done after 133 steps, reward Average: -115.2, up to now: minReward: -84.0, minAverage: -114.95\n",
      "Episode 275 done after 106 steps, reward Average: -115.16, up to now: minReward: -84.0, minAverage: -114.95\n",
      "Episode 276 done after 108 steps, reward Average: -115.17, up to now: minReward: -84.0, minAverage: -114.95\n",
      "Episode 277 done after 106 steps, reward Average: -115.16, up to now: minReward: -84.0, minAverage: -114.95\n",
      "Episode 278 done after 105 steps, reward Average: -115.13, up to now: minReward: -84.0, minAverage: -114.95\n",
      "Episode 279 done after 106 steps, reward Average: -115.12, up to now: minReward: -84.0, minAverage: -114.95\n",
      "Episode 280 done after 105 steps, reward Average: -114.94, up to now: minReward: -84.0, minAverage: -114.94\n",
      "Episode 281 done after 106 steps, reward Average: -114.69, up to now: minReward: -84.0, minAverage: -114.69\n",
      "Episode 282 done after 106 steps, reward Average: -114.67, up to now: minReward: -84.0, minAverage: -114.67\n",
      "Episode 283 done after 106 steps, reward Average: -114.61, up to now: minReward: -84.0, minAverage: -114.61\n",
      "Episode 284 done after 107 steps, reward Average: -114.63, up to now: minReward: -84.0, minAverage: -114.61\n",
      "Episode 285 done after 108 steps, reward Average: -114.61, up to now: minReward: -84.0, minAverage: -114.61\n",
      "Episode 286 done after 106 steps, reward Average: -114.59, up to now: minReward: -84.0, minAverage: -114.59\n",
      "Episode 287 done after 200 steps, reward Average: -115.53, up to now: minReward: -84.0, minAverage: -114.59\n",
      "Episode 288 done after 107 steps, reward Average: -115.53, up to now: minReward: -84.0, minAverage: -114.59\n",
      "Episode 289 done after 106 steps, reward Average: -115.27, up to now: minReward: -84.0, minAverage: -114.59\n",
      "Episode 290 done after 107 steps, reward Average: -115.26, up to now: minReward: -84.0, minAverage: -114.59\n",
      "Episode 291 done after 120 steps, reward Average: -115.39, up to now: minReward: -84.0, minAverage: -114.59\n",
      "Episode 292 done after 106 steps, reward Average: -115.34, up to now: minReward: -84.0, minAverage: -114.59\n",
      "Episode 293 done after 108 steps, reward Average: -115.37, up to now: minReward: -84.0, minAverage: -114.59\n",
      "Episode 294 done after 106 steps, reward Average: -114.43, up to now: minReward: -84.0, minAverage: -114.43\n",
      "Episode 295 done after 106 steps, reward Average: -114.44, up to now: minReward: -84.0, minAverage: -114.43\n",
      "Episode 296 done after 106 steps, reward Average: -114.44, up to now: minReward: -84.0, minAverage: -114.43\n",
      "Episode 297 done after 106 steps, reward Average: -114.43, up to now: minReward: -84.0, minAverage: -114.43\n",
      "Episode 298 done after 106 steps, reward Average: -114.42, up to now: minReward: -84.0, minAverage: -114.42\n",
      "Episode 299 done after 106 steps, reward Average: -114.38, up to now: minReward: -84.0, minAverage: -114.38\n",
      "final result: \n",
      "264 times arrived in 300 episodes, first time in episode 17\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERQGIiICAoDERFBYSAiIsD/BznzlgbNwGxCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c2ccb3160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) +  (R +  Q(S, A)  Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
