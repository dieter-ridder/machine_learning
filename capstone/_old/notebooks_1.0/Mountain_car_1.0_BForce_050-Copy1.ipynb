{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Brute Force version, 50 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.7\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 13:09:34,774] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 150\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=50\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 195 steps, reward Average: -199.72222222222223, up to now: minReward: -195.0, minAverage: -199.72222222222223\n",
      "Episode 18 done after 116 steps, reward Average: -195.31578947368422, up to now: minReward: -116.0, minAverage: -195.31578947368422\n",
      "Episode 19 done after 112 steps, reward Average: -191.15, up to now: minReward: -112.0, minAverage: -191.15\n",
      "Episode 20 done after 108 steps, reward Average: -187.1904761904762, up to now: minReward: -108.0, minAverage: -187.1904761904762\n",
      "Episode 21 done after 200 steps, reward Average: -187.77272727272728, up to now: minReward: -108.0, minAverage: -187.1904761904762\n",
      "Episode 22 done after 107 steps, reward Average: -184.2608695652174, up to now: minReward: -107.0, minAverage: -184.2608695652174\n",
      "Episode 23 done after 118 steps, reward Average: -181.5, up to now: minReward: -107.0, minAverage: -181.5\n",
      "Episode 24 done after 108 steps, reward Average: -178.56, up to now: minReward: -107.0, minAverage: -178.56\n",
      "Episode 25 done after 105 steps, reward Average: -175.73076923076923, up to now: minReward: -105.0, minAverage: -175.73076923076923\n",
      "Episode 26 done after 105 steps, reward Average: -173.11111111111111, up to now: minReward: -105.0, minAverage: -173.11111111111111\n",
      "Episode 27 done after 105 steps, reward Average: -170.67857142857142, up to now: minReward: -105.0, minAverage: -170.67857142857142\n",
      "Episode 28 done after 200 steps, reward Average: -171.68965517241378, up to now: minReward: -105.0, minAverage: -170.67857142857142\n",
      "Episode 29 done after 117 steps, reward Average: -169.86666666666667, up to now: minReward: -105.0, minAverage: -169.86666666666667\n",
      "Episode 30 done after 110 steps, reward Average: -167.93548387096774, up to now: minReward: -105.0, minAverage: -167.93548387096774\n",
      "Episode 31 done after 200 steps, reward Average: -168.9375, up to now: minReward: -105.0, minAverage: -167.93548387096774\n",
      "Episode 32 done after 130 steps, reward Average: -167.75757575757575, up to now: minReward: -105.0, minAverage: -167.75757575757575\n",
      "Episode 33 done after 128 steps, reward Average: -166.58823529411765, up to now: minReward: -105.0, minAverage: -166.58823529411765\n",
      "Episode 34 done after 129 steps, reward Average: -165.5142857142857, up to now: minReward: -105.0, minAverage: -165.5142857142857\n",
      "Episode 35 done after 129 steps, reward Average: -164.5, up to now: minReward: -105.0, minAverage: -164.5\n",
      "Episode 36 done after 200 steps, reward Average: -165.45945945945945, up to now: minReward: -105.0, minAverage: -164.5\n",
      "Episode 37 done after 122 steps, reward Average: -164.31578947368422, up to now: minReward: -105.0, minAverage: -164.31578947368422\n",
      "Episode 38 done after 126 steps, reward Average: -163.33333333333334, up to now: minReward: -105.0, minAverage: -163.33333333333334\n",
      "Episode 39 done after 108 steps, reward Average: -161.95, up to now: minReward: -105.0, minAverage: -161.95\n",
      "Episode 40 done after 105 steps, reward Average: -160.5609756097561, up to now: minReward: -105.0, minAverage: -160.5609756097561\n",
      "Episode 41 done after 105 steps, reward Average: -159.23809523809524, up to now: minReward: -105.0, minAverage: -159.23809523809524\n",
      "Episode 42 done after 106 steps, reward Average: -158.0, up to now: minReward: -105.0, minAverage: -158.0\n",
      "Episode 43 done after 109 steps, reward Average: -156.88636363636363, up to now: minReward: -105.0, minAverage: -156.88636363636363\n",
      "Episode 44 done after 123 steps, reward Average: -156.13333333333333, up to now: minReward: -105.0, minAverage: -156.13333333333333\n",
      "Episode 45 done after 107 steps, reward Average: -155.06521739130434, up to now: minReward: -105.0, minAverage: -155.06521739130434\n",
      "Episode 46 done after 105 steps, reward Average: -154.0, up to now: minReward: -105.0, minAverage: -154.0\n",
      "Episode 47 done after 127 steps, reward Average: -153.4375, up to now: minReward: -105.0, minAverage: -153.4375\n",
      "Episode 48 done after 108 steps, reward Average: -152.51020408163265, up to now: minReward: -105.0, minAverage: -152.51020408163265\n",
      "Episode 49 done after 133 steps, reward Average: -152.12, up to now: minReward: -105.0, minAverage: -152.12\n",
      "Episode 50 done after 200 steps, reward Average: -152.12, up to now: minReward: -105.0, minAverage: -152.12\n",
      "Episode 51 done after 200 steps, reward Average: -152.12, up to now: minReward: -105.0, minAverage: -152.12\n",
      "Episode 52 done after 105 steps, reward Average: -150.22, up to now: minReward: -105.0, minAverage: -150.22\n",
      "Episode 53 done after 109 steps, reward Average: -148.4, up to now: minReward: -105.0, minAverage: -148.4\n",
      "Episode 54 done after 126 steps, reward Average: -146.92, up to now: minReward: -105.0, minAverage: -146.92\n",
      "Episode 55 done after 105 steps, reward Average: -145.02, up to now: minReward: -105.0, minAverage: -145.02\n",
      "Episode 56 done after 105 steps, reward Average: -143.12, up to now: minReward: -105.0, minAverage: -143.12\n",
      "Episode 57 done after 107 steps, reward Average: -141.26, up to now: minReward: -105.0, minAverage: -141.26\n",
      "Episode 58 done after 107 steps, reward Average: -139.4, up to now: minReward: -105.0, minAverage: -139.4\n",
      "Episode 59 done after 106 steps, reward Average: -137.52, up to now: minReward: -105.0, minAverage: -137.52\n",
      "Episode 60 done after 105 steps, reward Average: -135.62, up to now: minReward: -105.0, minAverage: -135.62\n",
      "Episode 61 done after 105 steps, reward Average: -133.72, up to now: minReward: -105.0, minAverage: -133.72\n",
      "Episode 62 done after 126 steps, reward Average: -132.24, up to now: minReward: -105.0, minAverage: -132.24\n",
      "Episode 63 done after 198 steps, reward Average: -132.2, up to now: minReward: -105.0, minAverage: -132.2\n",
      "Episode 64 done after 122 steps, reward Average: -130.64, up to now: minReward: -105.0, minAverage: -130.64\n",
      "Episode 65 done after 108 steps, reward Average: -128.8, up to now: minReward: -105.0, minAverage: -128.8\n",
      "Episode 66 done after 105 steps, reward Average: -126.9, up to now: minReward: -105.0, minAverage: -126.9\n",
      "Episode 67 done after 111 steps, reward Average: -125.22, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 68 done after 200 steps, reward Average: -126.9, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 69 done after 105 steps, reward Average: -126.76, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 70 done after 105 steps, reward Average: -126.7, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 71 done after 200 steps, reward Average: -126.7, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 72 done after 200 steps, reward Average: -128.56, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 73 done after 106 steps, reward Average: -128.32, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 74 done after 105 steps, reward Average: -128.26, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 75 done after 123 steps, reward Average: -128.62, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 76 done after 106 steps, reward Average: -128.64, up to now: minReward: -105.0, minAverage: -125.22\n",
      "Episode 77 done after 104 steps, reward Average: -128.62, up to now: minReward: -104.0, minAverage: -125.22\n",
      "Episode 78 done after 105 steps, reward Average: -126.72, up to now: minReward: -104.0, minAverage: -125.22\n",
      "Episode 79 done after 108 steps, reward Average: -126.54, up to now: minReward: -104.0, minAverage: -125.22\n",
      "Episode 80 done after 108 steps, reward Average: -126.5, up to now: minReward: -104.0, minAverage: -125.22\n",
      "Episode 81 done after 106 steps, reward Average: -124.62, up to now: minReward: -104.0, minAverage: -124.62\n",
      "Episode 82 done after 105 steps, reward Average: -124.12, up to now: minReward: -104.0, minAverage: -124.12\n",
      "Episode 83 done after 105 steps, reward Average: -123.66, up to now: minReward: -104.0, minAverage: -123.66\n",
      "Episode 84 done after 104 steps, reward Average: -123.16, up to now: minReward: -104.0, minAverage: -123.16\n",
      "Episode 85 done after 117 steps, reward Average: -122.92, up to now: minReward: -104.0, minAverage: -122.92\n",
      "Episode 86 done after 126 steps, reward Average: -121.44, up to now: minReward: -104.0, minAverage: -121.44\n",
      "Episode 87 done after 124 steps, reward Average: -121.48, up to now: minReward: -104.0, minAverage: -121.44\n",
      "Episode 88 done after 105 steps, reward Average: -121.06, up to now: minReward: -104.0, minAverage: -121.06\n",
      "Episode 89 done after 106 steps, reward Average: -121.02, up to now: minReward: -104.0, minAverage: -121.02\n",
      "Episode 90 done after 105 steps, reward Average: -121.02, up to now: minReward: -104.0, minAverage: -121.02\n",
      "Episode 91 done after 104 steps, reward Average: -121.0, up to now: minReward: -104.0, minAverage: -121.0\n",
      "Episode 92 done after 124 steps, reward Average: -121.36, up to now: minReward: -104.0, minAverage: -121.0\n",
      "Episode 93 done after 128 steps, reward Average: -121.74, up to now: minReward: -104.0, minAverage: -121.0\n",
      "Episode 94 done after 123 steps, reward Average: -121.74, up to now: minReward: -104.0, minAverage: -121.0\n",
      "Episode 95 done after 105 steps, reward Average: -121.7, up to now: minReward: -104.0, minAverage: -121.0\n",
      "Episode 96 done after 107 steps, reward Average: -121.74, up to now: minReward: -104.0, minAverage: -121.0\n",
      "Episode 97 done after 105 steps, reward Average: -121.3, up to now: minReward: -104.0, minAverage: -121.0\n",
      "Episode 98 done after 107 steps, reward Average: -121.28, up to now: minReward: -104.0, minAverage: -121.0\n",
      "Episode 99 done after 106 steps, reward Average: -120.74, up to now: minReward: -104.0, minAverage: -120.74\n",
      "Episode 100 done after 107 steps, reward Average: -118.88, up to now: minReward: -104.0, minAverage: -118.88\n",
      "Episode 101 done after 108 steps, reward Average: -117.04, up to now: minReward: -104.0, minAverage: -117.04\n",
      "Episode 102 done after 105 steps, reward Average: -117.04, up to now: minReward: -104.0, minAverage: -117.04\n",
      "Episode 103 done after 109 steps, reward Average: -117.04, up to now: minReward: -104.0, minAverage: -117.04\n",
      "Episode 104 done after 108 steps, reward Average: -116.68, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 105 done after 105 steps, reward Average: -116.68, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 106 done after 105 steps, reward Average: -116.68, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 107 done after 111 steps, reward Average: -116.76, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 108 done after 105 steps, reward Average: -116.72, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 109 done after 105 steps, reward Average: -116.7, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 110 done after 123 steps, reward Average: -117.06, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 111 done after 124 steps, reward Average: -117.44, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 112 done after 105 steps, reward Average: -117.02, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 113 done after 200 steps, reward Average: -117.06, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 114 done after 105 steps, reward Average: -116.72, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 115 done after 127 steps, reward Average: -117.1, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 116 done after 105 steps, reward Average: -117.1, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 117 done after 125 steps, reward Average: -117.38, up to now: minReward: -104.0, minAverage: -116.68\n",
      "Episode 118 done after 122 steps, reward Average: -115.82, up to now: minReward: -104.0, minAverage: -115.82\n",
      "Episode 119 done after 105 steps, reward Average: -115.82, up to now: minReward: -104.0, minAverage: -115.82\n",
      "Episode 120 done after 108 steps, reward Average: -115.88, up to now: minReward: -104.0, minAverage: -115.82\n",
      "Episode 121 done after 107 steps, reward Average: -114.02, up to now: minReward: -104.0, minAverage: -114.02\n",
      "Episode 122 done after 105 steps, reward Average: -112.12, up to now: minReward: -104.0, minAverage: -112.12\n",
      "Episode 123 done after 105 steps, reward Average: -112.1, up to now: minReward: -104.0, minAverage: -112.1\n",
      "Episode 124 done after 108 steps, reward Average: -112.16, up to now: minReward: -104.0, minAverage: -112.1\n",
      "Episode 125 done after 105 steps, reward Average: -111.8, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 126 done after 106 steps, reward Average: -111.8, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 127 done after 200 steps, reward Average: -113.72, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 128 done after 112 steps, reward Average: -113.86, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 129 done after 105 steps, reward Average: -113.8, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 130 done after 105 steps, reward Average: -113.74, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 131 done after 107 steps, reward Average: -113.76, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 132 done after 105 steps, reward Average: -113.76, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 133 done after 123 steps, reward Average: -114.12, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 134 done after 106 steps, reward Average: -114.16, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 135 done after 106 steps, reward Average: -113.94, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 136 done after 107 steps, reward Average: -113.56, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 137 done after 105 steps, reward Average: -113.18, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 138 done after 105 steps, reward Average: -113.18, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 139 done after 105 steps, reward Average: -113.16, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 140 done after 112 steps, reward Average: -113.3, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 141 done after 105 steps, reward Average: -113.32, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 142 done after 105 steps, reward Average: -112.94, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 143 done after 111 steps, reward Average: -112.6, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 144 done after 200 steps, reward Average: -114.14, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 145 done after 105 steps, reward Average: -114.14, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 146 done after 105 steps, reward Average: -114.1, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 147 done after 105 steps, reward Average: -114.1, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 148 done after 126 steps, reward Average: -114.48, up to now: minReward: -104.0, minAverage: -111.8\n",
      "Episode 149 done after 106 steps, reward Average: -114.48, up to now: minReward: -104.0, minAverage: -111.8\n",
      "final result: \n",
      "121 times arrived in 150 episodes, first time in episode 17\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cb52da3240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 25:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
