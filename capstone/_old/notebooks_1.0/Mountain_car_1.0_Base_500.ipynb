{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile Model: #\n",
    "\n",
    "In this class there can be found both the tile coding, as well as the state model to learn.\n",
    "\n",
    "Tile coding is implemented straight forward:\n",
    "The observation, in out case position and velocity, is first shifted to avoid negative coordinated, and later scaled to the grid size. \n",
    "\n",
    "If we expect a grid of 8*8, after scaling and shifting, the range of the input is between [0,8] in both dimensions.\n",
    "\n",
    "For tiling the grids of each tiling level are shifted to its neighbour by 1/numberOfTilings. Before being copied into the coordinates grid (tiling\\*gridsize\\*gridsize)the observation is shifted accordingly and casted to an int.\n",
    "\n",
    "Now we have a lot of discrete points in the space (tiling\\*gridsize\\*gridsize). In a second step this 3D address of a point is mapped to point to a 1D-array to store the states. Mapping is done straigthforward: \n",
    "\n",
    "Tiling 0: mapped to 0...63\n",
    "Tiling 1: mapped to 64 ... 127 and so on.\n",
    "\n",
    "Inside one tiling level it is simiiar:\n",
    "(position,velocity) is mapped to position+8*velocity.\n",
    "\n",
    "As result we get an index vector, containing the index for each tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 09:07:45,301] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 500\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 18 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 19 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 20 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 21 done after 172 steps, reward Average: -198.72727272727272, up to now: minReward: -172.0, minAverage: -198.72727272727272\n",
      "Episode 22 done after 200 steps, reward Average: -198.7826086956522, up to now: minReward: -172.0, minAverage: -198.72727272727272\n",
      "Episode 23 done after 161 steps, reward Average: -197.20833333333334, up to now: minReward: -161.0, minAverage: -197.20833333333334\n",
      "Episode 24 done after 200 steps, reward Average: -197.32, up to now: minReward: -161.0, minAverage: -197.20833333333334\n",
      "Episode 25 done after 200 steps, reward Average: -197.42307692307693, up to now: minReward: -161.0, minAverage: -197.20833333333334\n",
      "Episode 26 done after 157 steps, reward Average: -195.92592592592592, up to now: minReward: -157.0, minAverage: -195.92592592592592\n",
      "Episode 27 done after 155 steps, reward Average: -194.46428571428572, up to now: minReward: -155.0, minAverage: -194.46428571428572\n",
      "Episode 28 done after 200 steps, reward Average: -194.6551724137931, up to now: minReward: -155.0, minAverage: -194.46428571428572\n",
      "Episode 29 done after 200 steps, reward Average: -194.83333333333334, up to now: minReward: -155.0, minAverage: -194.46428571428572\n",
      "Episode 30 done after 157 steps, reward Average: -193.61290322580646, up to now: minReward: -155.0, minAverage: -193.61290322580646\n",
      "Episode 31 done after 200 steps, reward Average: -193.8125, up to now: minReward: -155.0, minAverage: -193.61290322580646\n",
      "Episode 32 done after 200 steps, reward Average: -194.0, up to now: minReward: -155.0, minAverage: -193.61290322580646\n",
      "Episode 33 done after 200 steps, reward Average: -194.1764705882353, up to now: minReward: -155.0, minAverage: -193.61290322580646\n",
      "Episode 34 done after 152 steps, reward Average: -192.97142857142856, up to now: minReward: -152.0, minAverage: -192.97142857142856\n",
      "Episode 35 done after 156 steps, reward Average: -191.94444444444446, up to now: minReward: -152.0, minAverage: -191.94444444444446\n",
      "Episode 36 done after 153 steps, reward Average: -190.8918918918919, up to now: minReward: -152.0, minAverage: -190.8918918918919\n",
      "Episode 37 done after 150 steps, reward Average: -189.81578947368422, up to now: minReward: -150.0, minAverage: -189.81578947368422\n",
      "Episode 38 done after 169 steps, reward Average: -189.28205128205127, up to now: minReward: -150.0, minAverage: -189.28205128205127\n",
      "Episode 39 done after 160 steps, reward Average: -188.55, up to now: minReward: -150.0, minAverage: -188.55\n",
      "Episode 40 done after 150 steps, reward Average: -187.609756097561, up to now: minReward: -150.0, minAverage: -187.609756097561\n",
      "Episode 41 done after 150 steps, reward Average: -186.71428571428572, up to now: minReward: -150.0, minAverage: -186.71428571428572\n",
      "Episode 42 done after 154 steps, reward Average: -185.95348837209303, up to now: minReward: -150.0, minAverage: -185.95348837209303\n",
      "Episode 43 done after 167 steps, reward Average: -185.52272727272728, up to now: minReward: -150.0, minAverage: -185.52272727272728\n",
      "Episode 44 done after 152 steps, reward Average: -184.77777777777777, up to now: minReward: -150.0, minAverage: -184.77777777777777\n",
      "Episode 45 done after 152 steps, reward Average: -184.06521739130434, up to now: minReward: -150.0, minAverage: -184.06521739130434\n",
      "Episode 46 done after 149 steps, reward Average: -183.31914893617022, up to now: minReward: -149.0, minAverage: -183.31914893617022\n",
      "Episode 47 done after 151 steps, reward Average: -182.64583333333334, up to now: minReward: -149.0, minAverage: -182.64583333333334\n",
      "Episode 48 done after 149 steps, reward Average: -181.9591836734694, up to now: minReward: -149.0, minAverage: -181.9591836734694\n",
      "Episode 49 done after 150 steps, reward Average: -181.32, up to now: minReward: -149.0, minAverage: -181.32\n",
      "Episode 50 done after 153 steps, reward Average: -180.76470588235293, up to now: minReward: -149.0, minAverage: -180.76470588235293\n",
      "Episode 51 done after 154 steps, reward Average: -180.25, up to now: minReward: -149.0, minAverage: -180.25\n",
      "Episode 52 done after 155 steps, reward Average: -179.77358490566039, up to now: minReward: -149.0, minAverage: -179.77358490566039\n",
      "Episode 53 done after 153 steps, reward Average: -179.27777777777777, up to now: minReward: -149.0, minAverage: -179.27777777777777\n",
      "Episode 54 done after 156 steps, reward Average: -178.85454545454544, up to now: minReward: -149.0, minAverage: -178.85454545454544\n",
      "Episode 55 done after 155 steps, reward Average: -178.42857142857142, up to now: minReward: -149.0, minAverage: -178.42857142857142\n",
      "Episode 56 done after 155 steps, reward Average: -178.01754385964912, up to now: minReward: -149.0, minAverage: -178.01754385964912\n",
      "Episode 57 done after 148 steps, reward Average: -177.5, up to now: minReward: -148.0, minAverage: -177.5\n",
      "Episode 58 done after 153 steps, reward Average: -177.08474576271186, up to now: minReward: -148.0, minAverage: -177.08474576271186\n",
      "Episode 59 done after 158 steps, reward Average: -176.76666666666668, up to now: minReward: -148.0, minAverage: -176.76666666666668\n",
      "Episode 60 done after 154 steps, reward Average: -176.39344262295083, up to now: minReward: -148.0, minAverage: -176.39344262295083\n",
      "Episode 61 done after 150 steps, reward Average: -175.96774193548387, up to now: minReward: -148.0, minAverage: -175.96774193548387\n",
      "Episode 62 done after 157 steps, reward Average: -175.66666666666666, up to now: minReward: -148.0, minAverage: -175.66666666666666\n",
      "Episode 63 done after 154 steps, reward Average: -175.328125, up to now: minReward: -148.0, minAverage: -175.328125\n",
      "Episode 64 done after 153 steps, reward Average: -174.98461538461538, up to now: minReward: -148.0, minAverage: -174.98461538461538\n",
      "Episode 65 done after 149 steps, reward Average: -174.5909090909091, up to now: minReward: -148.0, minAverage: -174.5909090909091\n",
      "Episode 66 done after 154 steps, reward Average: -174.28358208955223, up to now: minReward: -148.0, minAverage: -174.28358208955223\n",
      "Episode 67 done after 163 steps, reward Average: -174.11764705882354, up to now: minReward: -148.0, minAverage: -174.11764705882354\n",
      "Episode 68 done after 144 steps, reward Average: -173.68115942028984, up to now: minReward: -144.0, minAverage: -173.68115942028984\n",
      "Episode 69 done after 138 steps, reward Average: -173.17142857142858, up to now: minReward: -138.0, minAverage: -173.17142857142858\n",
      "Episode 70 done after 137 steps, reward Average: -172.66197183098592, up to now: minReward: -137.0, minAverage: -172.66197183098592\n",
      "Episode 71 done after 136 steps, reward Average: -172.15277777777777, up to now: minReward: -136.0, minAverage: -172.15277777777777\n",
      "Episode 72 done after 149 steps, reward Average: -171.83561643835617, up to now: minReward: -136.0, minAverage: -171.83561643835617\n",
      "Episode 73 done after 141 steps, reward Average: -171.4189189189189, up to now: minReward: -136.0, minAverage: -171.4189189189189\n",
      "Episode 74 done after 145 steps, reward Average: -171.06666666666666, up to now: minReward: -136.0, minAverage: -171.06666666666666\n",
      "Episode 75 done after 147 steps, reward Average: -170.75, up to now: minReward: -136.0, minAverage: -170.75\n",
      "Episode 76 done after 146 steps, reward Average: -170.42857142857142, up to now: minReward: -136.0, minAverage: -170.42857142857142\n",
      "Episode 77 done after 138 steps, reward Average: -170.01282051282053, up to now: minReward: -136.0, minAverage: -170.01282051282053\n",
      "Episode 78 done after 145 steps, reward Average: -169.69620253164558, up to now: minReward: -136.0, minAverage: -169.69620253164558\n",
      "Episode 79 done after 146 steps, reward Average: -169.4, up to now: minReward: -136.0, minAverage: -169.4\n",
      "Episode 80 done after 145 steps, reward Average: -169.09876543209876, up to now: minReward: -136.0, minAverage: -169.09876543209876\n",
      "Episode 81 done after 147 steps, reward Average: -168.82926829268294, up to now: minReward: -136.0, minAverage: -168.82926829268294\n",
      "Episode 82 done after 147 steps, reward Average: -168.56626506024097, up to now: minReward: -136.0, minAverage: -168.56626506024097\n",
      "Episode 83 done after 153 steps, reward Average: -168.38095238095238, up to now: minReward: -136.0, minAverage: -168.38095238095238\n",
      "Episode 84 done after 181 steps, reward Average: -168.52941176470588, up to now: minReward: -136.0, minAverage: -168.38095238095238\n",
      "Episode 85 done after 160 steps, reward Average: -168.43023255813952, up to now: minReward: -136.0, minAverage: -168.38095238095238\n",
      "Episode 86 done after 144 steps, reward Average: -168.1494252873563, up to now: minReward: -136.0, minAverage: -168.1494252873563\n",
      "Episode 87 done after 145 steps, reward Average: -167.88636363636363, up to now: minReward: -136.0, minAverage: -167.88636363636363\n",
      "Episode 88 done after 146 steps, reward Average: -167.64044943820224, up to now: minReward: -136.0, minAverage: -167.64044943820224\n",
      "Episode 89 done after 138 steps, reward Average: -167.3111111111111, up to now: minReward: -136.0, minAverage: -167.3111111111111\n",
      "Episode 90 done after 142 steps, reward Average: -167.03296703296704, up to now: minReward: -136.0, minAverage: -167.03296703296704\n",
      "Episode 91 done after 141 steps, reward Average: -166.75, up to now: minReward: -136.0, minAverage: -166.75\n",
      "Episode 92 done after 147 steps, reward Average: -166.53763440860214, up to now: minReward: -136.0, minAverage: -166.53763440860214\n",
      "Episode 93 done after 147 steps, reward Average: -166.32978723404256, up to now: minReward: -136.0, minAverage: -166.32978723404256\n",
      "Episode 94 done after 137 steps, reward Average: -166.02105263157895, up to now: minReward: -136.0, minAverage: -166.02105263157895\n",
      "Episode 95 done after 129 steps, reward Average: -165.63541666666666, up to now: minReward: -129.0, minAverage: -165.63541666666666\n",
      "Episode 96 done after 135 steps, reward Average: -165.31958762886597, up to now: minReward: -129.0, minAverage: -165.31958762886597\n",
      "Episode 97 done after 141 steps, reward Average: -165.07142857142858, up to now: minReward: -129.0, minAverage: -165.07142857142858\n",
      "Episode 98 done after 88 steps, reward Average: -164.2929292929293, up to now: minReward: -88.0, minAverage: -164.2929292929293\n",
      "Episode 99 done after 147 steps, reward Average: -164.12, up to now: minReward: -88.0, minAverage: -164.12\n",
      "Episode 100 done after 143 steps, reward Average: -163.55, up to now: minReward: -88.0, minAverage: -163.55\n",
      "Episode 101 done after 139 steps, reward Average: -162.94, up to now: minReward: -88.0, minAverage: -162.94\n",
      "Episode 102 done after 122 steps, reward Average: -162.16, up to now: minReward: -88.0, minAverage: -162.16\n",
      "Episode 103 done after 137 steps, reward Average: -161.53, up to now: minReward: -88.0, minAverage: -161.53\n",
      "Episode 104 done after 140 steps, reward Average: -160.93, up to now: minReward: -88.0, minAverage: -160.93\n",
      "Episode 105 done after 140 steps, reward Average: -160.33, up to now: minReward: -88.0, minAverage: -160.33\n",
      "Episode 106 done after 146 steps, reward Average: -159.79, up to now: minReward: -88.0, minAverage: -159.79\n",
      "Episode 107 done after 138 steps, reward Average: -159.17, up to now: minReward: -88.0, minAverage: -159.17\n",
      "Episode 108 done after 136 steps, reward Average: -158.53, up to now: minReward: -88.0, minAverage: -158.53\n",
      "Episode 109 done after 142 steps, reward Average: -157.95, up to now: minReward: -88.0, minAverage: -157.95\n",
      "Episode 110 done after 144 steps, reward Average: -157.39, up to now: minReward: -88.0, minAverage: -157.39\n",
      "Episode 111 done after 145 steps, reward Average: -156.84, up to now: minReward: -88.0, minAverage: -156.84\n",
      "Episode 112 done after 138 steps, reward Average: -156.22, up to now: minReward: -88.0, minAverage: -156.22\n",
      "Episode 113 done after 136 steps, reward Average: -155.58, up to now: minReward: -88.0, minAverage: -155.58\n",
      "Episode 114 done after 132 steps, reward Average: -154.9, up to now: minReward: -88.0, minAverage: -154.9\n",
      "Episode 115 done after 139 steps, reward Average: -154.29, up to now: minReward: -88.0, minAverage: -154.29\n",
      "Episode 116 done after 149 steps, reward Average: -153.78, up to now: minReward: -88.0, minAverage: -153.78\n",
      "Episode 117 done after 153 steps, reward Average: -153.31, up to now: minReward: -88.0, minAverage: -153.31\n",
      "Episode 118 done after 129 steps, reward Average: -152.6, up to now: minReward: -88.0, minAverage: -152.6\n",
      "Episode 119 done after 156 steps, reward Average: -152.16, up to now: minReward: -88.0, minAverage: -152.16\n",
      "Episode 120 done after 154 steps, reward Average: -151.7, up to now: minReward: -88.0, minAverage: -151.7\n",
      "Episode 121 done after 140 steps, reward Average: -151.38, up to now: minReward: -88.0, minAverage: -151.38\n",
      "Episode 122 done after 129 steps, reward Average: -150.67, up to now: minReward: -88.0, minAverage: -150.67\n",
      "Episode 123 done after 154 steps, reward Average: -150.6, up to now: minReward: -88.0, minAverage: -150.6\n",
      "Episode 124 done after 137 steps, reward Average: -149.97, up to now: minReward: -88.0, minAverage: -149.97\n",
      "Episode 125 done after 129 steps, reward Average: -149.26, up to now: minReward: -88.0, minAverage: -149.26\n",
      "Episode 126 done after 149 steps, reward Average: -149.18, up to now: minReward: -88.0, minAverage: -149.18\n",
      "Episode 127 done after 151 steps, reward Average: -149.14, up to now: minReward: -88.0, minAverage: -149.14\n",
      "Episode 128 done after 153 steps, reward Average: -148.67, up to now: minReward: -88.0, minAverage: -148.67\n",
      "Episode 129 done after 96 steps, reward Average: -147.63, up to now: minReward: -88.0, minAverage: -147.63\n",
      "Episode 130 done after 157 steps, reward Average: -147.63, up to now: minReward: -88.0, minAverage: -147.63\n",
      "Episode 131 done after 153 steps, reward Average: -147.16, up to now: minReward: -88.0, minAverage: -147.16\n",
      "Episode 132 done after 141 steps, reward Average: -146.57, up to now: minReward: -88.0, minAverage: -146.57\n",
      "Episode 133 done after 140 steps, reward Average: -145.97, up to now: minReward: -88.0, minAverage: -145.97\n",
      "Episode 134 done after 130 steps, reward Average: -145.75, up to now: minReward: -88.0, minAverage: -145.75\n",
      "Episode 135 done after 122 steps, reward Average: -145.41, up to now: minReward: -88.0, minAverage: -145.41\n",
      "Episode 136 done after 130 steps, reward Average: -145.18, up to now: minReward: -88.0, minAverage: -145.18\n",
      "Episode 137 done after 149 steps, reward Average: -145.17, up to now: minReward: -88.0, minAverage: -145.17\n",
      "Episode 138 done after 141 steps, reward Average: -144.89, up to now: minReward: -88.0, minAverage: -144.89\n",
      "Episode 139 done after 131 steps, reward Average: -144.6, up to now: minReward: -88.0, minAverage: -144.6\n",
      "Episode 140 done after 137 steps, reward Average: -144.47, up to now: minReward: -88.0, minAverage: -144.47\n",
      "Episode 141 done after 146 steps, reward Average: -144.43, up to now: minReward: -88.0, minAverage: -144.43\n",
      "Episode 142 done after 134 steps, reward Average: -144.23, up to now: minReward: -88.0, minAverage: -144.23\n",
      "Episode 143 done after 134 steps, reward Average: -143.9, up to now: minReward: -88.0, minAverage: -143.9\n",
      "Episode 144 done after 146 steps, reward Average: -143.84, up to now: minReward: -88.0, minAverage: -143.84\n",
      "Episode 145 done after 139 steps, reward Average: -143.71, up to now: minReward: -88.0, minAverage: -143.71\n",
      "Episode 146 done after 137 steps, reward Average: -143.59, up to now: minReward: -88.0, minAverage: -143.59\n",
      "Episode 147 done after 138 steps, reward Average: -143.46, up to now: minReward: -88.0, minAverage: -143.46\n",
      "Episode 148 done after 133 steps, reward Average: -143.3, up to now: minReward: -88.0, minAverage: -143.3\n",
      "Episode 149 done after 140 steps, reward Average: -143.2, up to now: minReward: -88.0, minAverage: -143.2\n",
      "Episode 150 done after 139 steps, reward Average: -143.06, up to now: minReward: -88.0, minAverage: -143.06\n",
      "Episode 151 done after 138 steps, reward Average: -142.9, up to now: minReward: -88.0, minAverage: -142.9\n",
      "Episode 152 done after 127 steps, reward Average: -142.62, up to now: minReward: -88.0, minAverage: -142.62\n",
      "Episode 153 done after 135 steps, reward Average: -142.44, up to now: minReward: -88.0, minAverage: -142.44\n",
      "Episode 154 done after 138 steps, reward Average: -142.26, up to now: minReward: -88.0, minAverage: -142.26\n",
      "Episode 155 done after 133 steps, reward Average: -142.04, up to now: minReward: -88.0, minAverage: -142.04\n",
      "Episode 156 done after 125 steps, reward Average: -141.74, up to now: minReward: -88.0, minAverage: -141.74\n",
      "Episode 157 done after 129 steps, reward Average: -141.55, up to now: minReward: -88.0, minAverage: -141.55\n",
      "Episode 158 done after 121 steps, reward Average: -141.23, up to now: minReward: -88.0, minAverage: -141.23\n",
      "Episode 159 done after 137 steps, reward Average: -141.02, up to now: minReward: -88.0, minAverage: -141.02\n",
      "Episode 160 done after 170 steps, reward Average: -141.18, up to now: minReward: -88.0, minAverage: -141.02\n",
      "Episode 161 done after 134 steps, reward Average: -141.02, up to now: minReward: -88.0, minAverage: -141.02\n",
      "Episode 162 done after 139 steps, reward Average: -140.84, up to now: minReward: -88.0, minAverage: -140.84\n",
      "Episode 163 done after 145 steps, reward Average: -140.75, up to now: minReward: -88.0, minAverage: -140.75\n",
      "Episode 164 done after 142 steps, reward Average: -140.64, up to now: minReward: -88.0, minAverage: -140.64\n",
      "Episode 165 done after 143 steps, reward Average: -140.58, up to now: minReward: -88.0, minAverage: -140.58\n",
      "Episode 166 done after 131 steps, reward Average: -140.35, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 167 done after 200 steps, reward Average: -140.72, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 168 done after 142 steps, reward Average: -140.7, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 169 done after 128 steps, reward Average: -140.6, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 170 done after 142 steps, reward Average: -140.65, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 171 done after 145 steps, reward Average: -140.74, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 172 done after 132 steps, reward Average: -140.57, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 173 done after 200 steps, reward Average: -141.16, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 174 done after 140 steps, reward Average: -141.11, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 175 done after 200 steps, reward Average: -141.64, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 176 done after 144 steps, reward Average: -141.62, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 177 done after 141 steps, reward Average: -141.65, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 178 done after 142 steps, reward Average: -141.62, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 179 done after 137 steps, reward Average: -141.53, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 180 done after 143 steps, reward Average: -141.51, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 181 done after 134 steps, reward Average: -141.38, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 182 done after 143 steps, reward Average: -141.34, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 183 done after 140 steps, reward Average: -141.21, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 184 done after 139 steps, reward Average: -140.79, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 185 done after 200 steps, reward Average: -141.19, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 186 done after 200 steps, reward Average: -141.75, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 187 done after 200 steps, reward Average: -142.3, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 188 done after 200 steps, reward Average: -142.84, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 189 done after 200 steps, reward Average: -143.46, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 190 done after 200 steps, reward Average: -144.04, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 191 done after 200 steps, reward Average: -144.63, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 192 done after 200 steps, reward Average: -145.16, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 193 done after 200 steps, reward Average: -145.69, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 194 done after 200 steps, reward Average: -146.32, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 195 done after 200 steps, reward Average: -147.03, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 196 done after 200 steps, reward Average: -147.68, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 197 done after 200 steps, reward Average: -148.27, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 198 done after 200 steps, reward Average: -149.39, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 199 done after 200 steps, reward Average: -149.92, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 200 done after 200 steps, reward Average: -150.49, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 201 done after 200 steps, reward Average: -151.1, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 202 done after 200 steps, reward Average: -151.88, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 203 done after 200 steps, reward Average: -152.51, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 204 done after 200 steps, reward Average: -153.11, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 205 done after 200 steps, reward Average: -153.71, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 206 done after 200 steps, reward Average: -154.25, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 207 done after 200 steps, reward Average: -154.87, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 208 done after 200 steps, reward Average: -155.51, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 209 done after 200 steps, reward Average: -156.09, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 210 done after 200 steps, reward Average: -156.65, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 211 done after 200 steps, reward Average: -157.2, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 212 done after 200 steps, reward Average: -157.82, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 213 done after 200 steps, reward Average: -158.46, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 214 done after 200 steps, reward Average: -159.14, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 215 done after 200 steps, reward Average: -159.75, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 216 done after 200 steps, reward Average: -160.26, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 217 done after 200 steps, reward Average: -160.73, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 218 done after 200 steps, reward Average: -161.44, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 219 done after 200 steps, reward Average: -161.88, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 220 done after 200 steps, reward Average: -162.34, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 221 done after 200 steps, reward Average: -162.94, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 222 done after 200 steps, reward Average: -163.65, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 223 done after 200 steps, reward Average: -164.11, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 224 done after 200 steps, reward Average: -164.74, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 225 done after 200 steps, reward Average: -165.45, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 226 done after 200 steps, reward Average: -165.96, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 227 done after 200 steps, reward Average: -166.45, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 228 done after 200 steps, reward Average: -166.92, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 229 done after 200 steps, reward Average: -167.96, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 230 done after 200 steps, reward Average: -168.39, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 231 done after 200 steps, reward Average: -168.86, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 232 done after 200 steps, reward Average: -169.45, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 233 done after 200 steps, reward Average: -170.05, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 234 done after 200 steps, reward Average: -170.75, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 235 done after 200 steps, reward Average: -171.53, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 236 done after 200 steps, reward Average: -172.23, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 237 done after 200 steps, reward Average: -172.74, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 238 done after 200 steps, reward Average: -173.33, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 239 done after 200 steps, reward Average: -174.02, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 240 done after 200 steps, reward Average: -174.65, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 241 done after 200 steps, reward Average: -175.19, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 242 done after 200 steps, reward Average: -175.85, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 243 done after 200 steps, reward Average: -176.51, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 244 done after 200 steps, reward Average: -177.05, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 245 done after 200 steps, reward Average: -177.66, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 246 done after 200 steps, reward Average: -178.29, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 247 done after 200 steps, reward Average: -178.91, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 248 done after 200 steps, reward Average: -179.58, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 249 done after 200 steps, reward Average: -180.18, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 250 done after 200 steps, reward Average: -180.79, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 251 done after 200 steps, reward Average: -181.41, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 252 done after 200 steps, reward Average: -182.14, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 253 done after 200 steps, reward Average: -182.79, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 254 done after 200 steps, reward Average: -183.41, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 255 done after 200 steps, reward Average: -184.08, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 256 done after 200 steps, reward Average: -184.83, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 257 done after 200 steps, reward Average: -185.54, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 258 done after 200 steps, reward Average: -186.33, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 259 done after 200 steps, reward Average: -186.96, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 260 done after 200 steps, reward Average: -187.26, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 261 done after 200 steps, reward Average: -187.92, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 262 done after 200 steps, reward Average: -188.53, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 263 done after 200 steps, reward Average: -189.08, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 264 done after 200 steps, reward Average: -189.66, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 265 done after 200 steps, reward Average: -190.23, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 266 done after 200 steps, reward Average: -190.92, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 267 done after 200 steps, reward Average: -190.92, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 268 done after 200 steps, reward Average: -191.5, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 269 done after 200 steps, reward Average: -192.22, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 270 done after 200 steps, reward Average: -192.8, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 271 done after 200 steps, reward Average: -193.35, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 272 done after 200 steps, reward Average: -194.03, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 273 done after 200 steps, reward Average: -194.03, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 274 done after 200 steps, reward Average: -194.63, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 275 done after 200 steps, reward Average: -194.63, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 276 done after 200 steps, reward Average: -195.19, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 277 done after 200 steps, reward Average: -195.78, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 278 done after 200 steps, reward Average: -196.36, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 279 done after 200 steps, reward Average: -196.99, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 280 done after 200 steps, reward Average: -197.56, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 281 done after 200 steps, reward Average: -198.22, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 282 done after 200 steps, reward Average: -198.79, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 283 done after 200 steps, reward Average: -199.39, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 284 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 285 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 286 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 287 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 288 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 289 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 290 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 291 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 292 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 293 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 294 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 295 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 296 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 297 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 298 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 299 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 300 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 301 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 302 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 303 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 304 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 305 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 306 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 307 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 308 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 309 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 310 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 311 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 312 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 313 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 314 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 315 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 316 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 317 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 318 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 319 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 320 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 321 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 322 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 323 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 324 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 325 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 326 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 327 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 328 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 329 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 330 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 331 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 332 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 333 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 334 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 335 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 336 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 337 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 338 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 339 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 340 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 341 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 342 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 343 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 344 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 345 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 346 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 347 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 348 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 349 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 350 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 351 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 352 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 353 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 354 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 355 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 356 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 357 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 358 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 359 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 360 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 361 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 362 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 363 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 364 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 365 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 366 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 367 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 368 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 369 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 370 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 371 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 372 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 373 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 374 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 375 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 376 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 377 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 378 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 379 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 380 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 381 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 382 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 383 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 384 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 385 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 386 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 387 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 388 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 389 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 390 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 391 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 392 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 393 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 394 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 395 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 396 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 397 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 398 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 399 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 400 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 401 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 402 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 403 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 404 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 405 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 406 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 407 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 408 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 409 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 410 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 411 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 412 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 413 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 414 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 415 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 416 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 417 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 418 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 419 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 420 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 421 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 422 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 423 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 424 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 425 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 426 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 427 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 428 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 429 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 430 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 431 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 432 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 433 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 434 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 435 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 436 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 437 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 438 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 439 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 440 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 441 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 442 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 443 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 444 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 445 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 446 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 447 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 448 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 449 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 450 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 451 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 452 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 453 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 454 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 455 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 456 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 457 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 458 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 459 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 460 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 461 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 462 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 463 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 464 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 465 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 466 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 467 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 468 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 469 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 470 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 471 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 472 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 473 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 474 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 475 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 476 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 477 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 478 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 479 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 480 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 481 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 482 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 483 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 484 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 485 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 486 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 487 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 488 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 489 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 490 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 491 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 492 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 493 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 494 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 495 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 496 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 497 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 498 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "Episode 499 done after 200 steps, reward Average: -200.0, up to now: minReward: -88.0, minAverage: -140.35\n",
      "final result: \n",
      "153 times arrived in 500 episodes, first time in episode 21\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eb4628c198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        interval=nbEpisodes\n",
    "        if interval<decayInterval:\n",
    "            interval=decayInterval\n",
    "        result=decayStart-episode/interval\n",
    "        if result < decayBase:\n",
    "            result = decayBase\n",
    "        return result\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    ##epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "    ##                      decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    ##gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    ##alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) +  (R +  Q(S, A)  Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
