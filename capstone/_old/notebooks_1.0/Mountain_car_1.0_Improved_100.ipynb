{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Alpha was further tunes, 100 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 25:\n",
    "            return 0.65\n",
    "        elif episode < 35:\n",
    "            return 0.6\n",
    "        elif episode < 45:\n",
    "            return 0.55\n",
    "        elif episode < 55:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.4\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.5)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 13:17:42,225] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 100\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=50\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 138 steps, reward Average: -196.35294117647058, up to now: minReward: -138.0, minAverage: -196.35294117647058\n",
      "Episode 17 done after 200 steps, reward Average: -196.55555555555554, up to now: minReward: -138.0, minAverage: -196.35294117647058\n",
      "Episode 18 done after 128 steps, reward Average: -192.94736842105263, up to now: minReward: -128.0, minAverage: -192.94736842105263\n",
      "Episode 19 done after 132 steps, reward Average: -189.9, up to now: minReward: -128.0, minAverage: -189.9\n",
      "Episode 20 done after 124 steps, reward Average: -186.76190476190476, up to now: minReward: -124.0, minAverage: -186.76190476190476\n",
      "Episode 21 done after 118 steps, reward Average: -183.63636363636363, up to now: minReward: -118.0, minAverage: -183.63636363636363\n",
      "Episode 22 done after 115 steps, reward Average: -180.65217391304347, up to now: minReward: -115.0, minAverage: -180.65217391304347\n",
      "Episode 23 done after 126 steps, reward Average: -178.375, up to now: minReward: -115.0, minAverage: -178.375\n",
      "Episode 24 done after 119 steps, reward Average: -176.0, up to now: minReward: -115.0, minAverage: -176.0\n",
      "Episode 25 done after 119 steps, reward Average: -173.80769230769232, up to now: minReward: -115.0, minAverage: -173.80769230769232\n",
      "Episode 26 done after 115 steps, reward Average: -171.62962962962962, up to now: minReward: -115.0, minAverage: -171.62962962962962\n",
      "Episode 27 done after 124 steps, reward Average: -169.92857142857142, up to now: minReward: -115.0, minAverage: -169.92857142857142\n",
      "Episode 28 done after 124 steps, reward Average: -168.3448275862069, up to now: minReward: -115.0, minAverage: -168.3448275862069\n",
      "Episode 29 done after 116 steps, reward Average: -166.6, up to now: minReward: -115.0, minAverage: -166.6\n",
      "Episode 30 done after 113 steps, reward Average: -164.8709677419355, up to now: minReward: -113.0, minAverage: -164.8709677419355\n",
      "Episode 31 done after 110 steps, reward Average: -163.15625, up to now: minReward: -110.0, minAverage: -163.15625\n",
      "Episode 32 done after 121 steps, reward Average: -161.87878787878788, up to now: minReward: -110.0, minAverage: -161.87878787878788\n",
      "Episode 33 done after 109 steps, reward Average: -160.3235294117647, up to now: minReward: -109.0, minAverage: -160.3235294117647\n",
      "Episode 34 done after 109 steps, reward Average: -158.85714285714286, up to now: minReward: -109.0, minAverage: -158.85714285714286\n",
      "Episode 35 done after 120 steps, reward Average: -157.77777777777777, up to now: minReward: -109.0, minAverage: -157.77777777777777\n",
      "Episode 36 done after 108 steps, reward Average: -156.43243243243242, up to now: minReward: -108.0, minAverage: -156.43243243243242\n",
      "Episode 37 done after 109 steps, reward Average: -155.18421052631578, up to now: minReward: -108.0, minAverage: -155.18421052631578\n",
      "Episode 38 done after 107 steps, reward Average: -153.94871794871796, up to now: minReward: -107.0, minAverage: -153.94871794871796\n",
      "Episode 39 done after 112 steps, reward Average: -152.9, up to now: minReward: -107.0, minAverage: -152.9\n",
      "Episode 40 done after 107 steps, reward Average: -151.78048780487805, up to now: minReward: -107.0, minAverage: -151.78048780487805\n",
      "Episode 41 done after 109 steps, reward Average: -150.76190476190476, up to now: minReward: -107.0, minAverage: -150.76190476190476\n",
      "Episode 42 done after 112 steps, reward Average: -149.86046511627907, up to now: minReward: -107.0, minAverage: -149.86046511627907\n",
      "Episode 43 done after 106 steps, reward Average: -148.86363636363637, up to now: minReward: -106.0, minAverage: -148.86363636363637\n",
      "Episode 44 done after 107 steps, reward Average: -147.93333333333334, up to now: minReward: -106.0, minAverage: -147.93333333333334\n",
      "Episode 45 done after 113 steps, reward Average: -147.17391304347825, up to now: minReward: -106.0, minAverage: -147.17391304347825\n",
      "Episode 46 done after 107 steps, reward Average: -146.31914893617022, up to now: minReward: -106.0, minAverage: -146.31914893617022\n",
      "Episode 47 done after 107 steps, reward Average: -145.5, up to now: minReward: -106.0, minAverage: -145.5\n",
      "Episode 48 done after 116 steps, reward Average: -144.89795918367346, up to now: minReward: -106.0, minAverage: -144.89795918367346\n",
      "Episode 49 done after 108 steps, reward Average: -144.16, up to now: minReward: -106.0, minAverage: -144.16\n",
      "Episode 50 done after 108 steps, reward Average: -142.32, up to now: minReward: -106.0, minAverage: -142.32\n",
      "Episode 51 done after 106 steps, reward Average: -140.44, up to now: minReward: -106.0, minAverage: -140.44\n",
      "Episode 52 done after 120 steps, reward Average: -138.84, up to now: minReward: -106.0, minAverage: -138.84\n",
      "Episode 53 done after 109 steps, reward Average: -137.02, up to now: minReward: -106.0, minAverage: -137.02\n",
      "Episode 54 done after 107 steps, reward Average: -135.16, up to now: minReward: -106.0, minAverage: -135.16\n",
      "Episode 55 done after 109 steps, reward Average: -133.34, up to now: minReward: -106.0, minAverage: -133.34\n",
      "Episode 56 done after 109 steps, reward Average: -131.52, up to now: minReward: -106.0, minAverage: -131.52\n",
      "Episode 57 done after 107 steps, reward Average: -129.66, up to now: minReward: -106.0, minAverage: -129.66\n",
      "Episode 58 done after 107 steps, reward Average: -127.8, up to now: minReward: -106.0, minAverage: -127.8\n",
      "Episode 59 done after 112 steps, reward Average: -126.04, up to now: minReward: -106.0, minAverage: -126.04\n",
      "Episode 60 done after 110 steps, reward Average: -124.24, up to now: minReward: -106.0, minAverage: -124.24\n",
      "Episode 61 done after 113 steps, reward Average: -122.5, up to now: minReward: -106.0, minAverage: -122.5\n",
      "Episode 62 done after 106 steps, reward Average: -120.62, up to now: minReward: -106.0, minAverage: -120.62\n",
      "Episode 63 done after 108 steps, reward Average: -118.78, up to now: minReward: -106.0, minAverage: -118.78\n",
      "Episode 64 done after 106 steps, reward Average: -116.9, up to now: minReward: -106.0, minAverage: -116.9\n",
      "Episode 65 done after 106 steps, reward Average: -115.02, up to now: minReward: -106.0, minAverage: -115.02\n",
      "Episode 66 done after 109 steps, reward Average: -114.44, up to now: minReward: -106.0, minAverage: -114.44\n",
      "Episode 67 done after 109 steps, reward Average: -112.62, up to now: minReward: -106.0, minAverage: -112.62\n",
      "Episode 68 done after 107 steps, reward Average: -112.2, up to now: minReward: -106.0, minAverage: -112.2\n",
      "Episode 69 done after 109 steps, reward Average: -111.74, up to now: minReward: -106.0, minAverage: -111.74\n",
      "Episode 70 done after 109 steps, reward Average: -111.44, up to now: minReward: -106.0, minAverage: -111.44\n",
      "Episode 71 done after 108 steps, reward Average: -111.24, up to now: minReward: -106.0, minAverage: -111.24\n",
      "Episode 72 done after 110 steps, reward Average: -111.14, up to now: minReward: -106.0, minAverage: -111.14\n",
      "Episode 73 done after 121 steps, reward Average: -111.04, up to now: minReward: -106.0, minAverage: -111.04\n",
      "Episode 74 done after 107 steps, reward Average: -110.8, up to now: minReward: -106.0, minAverage: -110.8\n",
      "Episode 75 done after 113 steps, reward Average: -110.68, up to now: minReward: -106.0, minAverage: -110.68\n",
      "Episode 76 done after 109 steps, reward Average: -110.56, up to now: minReward: -106.0, minAverage: -110.56\n",
      "Episode 77 done after 109 steps, reward Average: -110.26, up to now: minReward: -106.0, minAverage: -110.26\n",
      "Episode 78 done after 116 steps, reward Average: -110.1, up to now: minReward: -106.0, minAverage: -110.1\n",
      "Episode 79 done after 112 steps, reward Average: -110.02, up to now: minReward: -106.0, minAverage: -110.02\n",
      "Episode 80 done after 109 steps, reward Average: -109.94, up to now: minReward: -106.0, minAverage: -109.94\n",
      "Episode 81 done after 107 steps, reward Average: -109.88, up to now: minReward: -106.0, minAverage: -109.88\n",
      "Episode 82 done after 118 steps, reward Average: -109.82, up to now: minReward: -106.0, minAverage: -109.82\n",
      "Episode 83 done after 120 steps, reward Average: -110.04, up to now: minReward: -106.0, minAverage: -109.82\n",
      "Episode 84 done after 106 steps, reward Average: -109.98, up to now: minReward: -106.0, minAverage: -109.82\n",
      "Episode 85 done after 109 steps, reward Average: -109.76, up to now: minReward: -106.0, minAverage: -109.76\n",
      "Episode 86 done after 106 steps, reward Average: -109.72, up to now: minReward: -106.0, minAverage: -109.72\n",
      "Episode 87 done after 108 steps, reward Average: -109.7, up to now: minReward: -106.0, minAverage: -109.7\n",
      "Episode 88 done after 109 steps, reward Average: -109.74, up to now: minReward: -106.0, minAverage: -109.7\n",
      "Episode 89 done after 106 steps, reward Average: -109.62, up to now: minReward: -106.0, minAverage: -109.62\n",
      "Episode 90 done after 108 steps, reward Average: -109.64, up to now: minReward: -106.0, minAverage: -109.62\n",
      "Episode 91 done after 106 steps, reward Average: -109.58, up to now: minReward: -106.0, minAverage: -109.58\n",
      "Episode 92 done after 106 steps, reward Average: -109.46, up to now: minReward: -106.0, minAverage: -109.46\n",
      "Episode 93 done after 117 steps, reward Average: -109.68, up to now: minReward: -106.0, minAverage: -109.46\n",
      "Episode 94 done after 109 steps, reward Average: -109.72, up to now: minReward: -106.0, minAverage: -109.46\n",
      "Episode 95 done after 111 steps, reward Average: -109.68, up to now: minReward: -106.0, minAverage: -109.46\n",
      "Episode 96 done after 109 steps, reward Average: -109.72, up to now: minReward: -106.0, minAverage: -109.46\n",
      "Episode 97 done after 108 steps, reward Average: -109.74, up to now: minReward: -106.0, minAverage: -109.46\n",
      "Episode 98 done after 108 steps, reward Average: -109.58, up to now: minReward: -106.0, minAverage: -109.46\n",
      "Episode 99 done after 107 steps, reward Average: -109.56, up to now: minReward: -106.0, minAverage: -109.46\n",
      "final result: \n",
      "83 times arrived in 100 episodes, first time in episode 16\n",
      "problem solved?:True\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA2ExkXuEfIuKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21e020b2898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 25:\n",
    "            return 0.65\n",
    "        elif episode < 35:\n",
    "            return 0.6\n",
    "        elif episode < 45:\n",
    "            return 0.55\n",
    "        elif episode < 55:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.4\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
