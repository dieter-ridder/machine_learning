{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling 8*8*8, Decay Epsilon #\n",
    "\n",
    "Base parameters:\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 1/(epsiode+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 11:10:18,203] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 50\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=50\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=1           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 18 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 19 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 20 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 21 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 22 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 23 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 24 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 25 done after 187 steps, reward Average: -199.5, up to now: minReward: -187.0, minAverage: -199.5\n",
      "Episode 26 done after 200 steps, reward Average: -199.5185185185185, up to now: minReward: -187.0, minAverage: -199.5\n",
      "Episode 27 done after 200 steps, reward Average: -199.53571428571428, up to now: minReward: -187.0, minAverage: -199.5\n",
      "Episode 28 done after 200 steps, reward Average: -199.55172413793105, up to now: minReward: -187.0, minAverage: -199.5\n",
      "Episode 29 done after 200 steps, reward Average: -199.56666666666666, up to now: minReward: -187.0, minAverage: -199.5\n",
      "Episode 30 done after 200 steps, reward Average: -199.58064516129033, up to now: minReward: -187.0, minAverage: -199.5\n",
      "Episode 31 done after 172 steps, reward Average: -198.71875, up to now: minReward: -172.0, minAverage: -198.71875\n",
      "Episode 32 done after 200 steps, reward Average: -198.75757575757575, up to now: minReward: -172.0, minAverage: -198.71875\n",
      "Episode 33 done after 200 steps, reward Average: -198.7941176470588, up to now: minReward: -172.0, minAverage: -198.71875\n",
      "Episode 34 done after 169 steps, reward Average: -197.94285714285715, up to now: minReward: -169.0, minAverage: -197.94285714285715\n",
      "Episode 35 done after 171 steps, reward Average: -197.19444444444446, up to now: minReward: -169.0, minAverage: -197.19444444444446\n",
      "Episode 36 done after 170 steps, reward Average: -196.45945945945945, up to now: minReward: -169.0, minAverage: -196.45945945945945\n",
      "Episode 37 done after 157 steps, reward Average: -195.42105263157896, up to now: minReward: -157.0, minAverage: -195.42105263157896\n",
      "Episode 38 done after 156 steps, reward Average: -194.4102564102564, up to now: minReward: -156.0, minAverage: -194.4102564102564\n",
      "Episode 39 done after 164 steps, reward Average: -193.65, up to now: minReward: -156.0, minAverage: -193.65\n",
      "Episode 40 done after 170 steps, reward Average: -193.0731707317073, up to now: minReward: -156.0, minAverage: -193.0731707317073\n",
      "Episode 41 done after 158 steps, reward Average: -192.23809523809524, up to now: minReward: -156.0, minAverage: -192.23809523809524\n",
      "Episode 42 done after 155 steps, reward Average: -191.37209302325581, up to now: minReward: -155.0, minAverage: -191.37209302325581\n",
      "Episode 43 done after 173 steps, reward Average: -190.95454545454547, up to now: minReward: -155.0, minAverage: -190.95454545454547\n",
      "Episode 44 done after 198 steps, reward Average: -191.11111111111111, up to now: minReward: -155.0, minAverage: -190.95454545454547\n",
      "Episode 45 done after 154 steps, reward Average: -190.30434782608697, up to now: minReward: -154.0, minAverage: -190.30434782608697\n",
      "Episode 46 done after 153 steps, reward Average: -189.51063829787233, up to now: minReward: -153.0, minAverage: -189.51063829787233\n",
      "Episode 47 done after 158 steps, reward Average: -188.85416666666666, up to now: minReward: -153.0, minAverage: -188.85416666666666\n",
      "Episode 48 done after 156 steps, reward Average: -188.18367346938774, up to now: minReward: -153.0, minAverage: -188.18367346938774\n",
      "Episode 49 done after 158 steps, reward Average: -187.58, up to now: minReward: -153.0, minAverage: -187.58\n",
      "final result: \n",
      "18 times arrived in 50 episodes, first time in episode 25\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17d58a1b470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        interval=nbEpisodes\n",
    "        if interval<decayInterval:\n",
    "            interval=decayInterval\n",
    "        result=decayStart-episode/interval\n",
    "        if result < decayBase:\n",
    "            result = decayBase\n",
    "        return result\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    ##gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    ##alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
