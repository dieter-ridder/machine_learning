{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Improvement by decreasing gamma - 300 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.2)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 13:57:38,814] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.2\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 193 steps, reward Average: -199.53333333333333, up to now: minReward: -193.0, minAverage: -199.53333333333333\n",
      "Episode 15 done after 200 steps, reward Average: -199.5625, up to now: minReward: -193.0, minAverage: -199.53333333333333\n",
      "Episode 16 done after 196 steps, reward Average: -199.35294117647058, up to now: minReward: -193.0, minAverage: -199.35294117647058\n",
      "Episode 17 done after 200 steps, reward Average: -199.38888888888889, up to now: minReward: -193.0, minAverage: -199.35294117647058\n",
      "Episode 18 done after 193 steps, reward Average: -199.05263157894737, up to now: minReward: -193.0, minAverage: -199.05263157894737\n",
      "Episode 19 done after 106 steps, reward Average: -194.4, up to now: minReward: -106.0, minAverage: -194.4\n",
      "Episode 20 done after 192 steps, reward Average: -194.28571428571428, up to now: minReward: -106.0, minAverage: -194.28571428571428\n",
      "Episode 21 done after 121 steps, reward Average: -190.95454545454547, up to now: minReward: -106.0, minAverage: -190.95454545454547\n",
      "Episode 22 done after 110 steps, reward Average: -187.43478260869566, up to now: minReward: -106.0, minAverage: -187.43478260869566\n",
      "Episode 23 done after 111 steps, reward Average: -184.25, up to now: minReward: -106.0, minAverage: -184.25\n",
      "Episode 24 done after 110 steps, reward Average: -181.28, up to now: minReward: -106.0, minAverage: -181.28\n",
      "Episode 25 done after 118 steps, reward Average: -178.84615384615384, up to now: minReward: -106.0, minAverage: -178.84615384615384\n",
      "Episode 26 done after 107 steps, reward Average: -176.1851851851852, up to now: minReward: -106.0, minAverage: -176.1851851851852\n",
      "Episode 27 done after 121 steps, reward Average: -174.21428571428572, up to now: minReward: -106.0, minAverage: -174.21428571428572\n",
      "Episode 28 done after 107 steps, reward Average: -171.89655172413794, up to now: minReward: -106.0, minAverage: -171.89655172413794\n",
      "Episode 29 done after 106 steps, reward Average: -169.7, up to now: minReward: -106.0, minAverage: -169.7\n",
      "Episode 30 done after 106 steps, reward Average: -167.6451612903226, up to now: minReward: -106.0, minAverage: -167.6451612903226\n",
      "Episode 31 done after 107 steps, reward Average: -165.75, up to now: minReward: -106.0, minAverage: -165.75\n",
      "Episode 32 done after 110 steps, reward Average: -164.06060606060606, up to now: minReward: -106.0, minAverage: -164.06060606060606\n",
      "Episode 33 done after 108 steps, reward Average: -162.41176470588235, up to now: minReward: -106.0, minAverage: -162.41176470588235\n",
      "Episode 34 done after 108 steps, reward Average: -160.85714285714286, up to now: minReward: -106.0, minAverage: -160.85714285714286\n",
      "Episode 35 done after 200 steps, reward Average: -161.94444444444446, up to now: minReward: -106.0, minAverage: -160.85714285714286\n",
      "Episode 36 done after 200 steps, reward Average: -162.97297297297297, up to now: minReward: -106.0, minAverage: -160.85714285714286\n",
      "Episode 37 done after 200 steps, reward Average: -163.94736842105263, up to now: minReward: -106.0, minAverage: -160.85714285714286\n",
      "Episode 38 done after 107 steps, reward Average: -162.48717948717947, up to now: minReward: -106.0, minAverage: -160.85714285714286\n",
      "Episode 39 done after 111 steps, reward Average: -161.2, up to now: minReward: -106.0, minAverage: -160.85714285714286\n",
      "Episode 40 done after 111 steps, reward Average: -159.97560975609755, up to now: minReward: -106.0, minAverage: -159.97560975609755\n",
      "Episode 41 done after 200 steps, reward Average: -160.92857142857142, up to now: minReward: -106.0, minAverage: -159.97560975609755\n",
      "Episode 42 done after 106 steps, reward Average: -159.65116279069767, up to now: minReward: -106.0, minAverage: -159.65116279069767\n",
      "Episode 43 done after 105 steps, reward Average: -158.4090909090909, up to now: minReward: -105.0, minAverage: -158.4090909090909\n",
      "Episode 44 done after 107 steps, reward Average: -157.26666666666668, up to now: minReward: -105.0, minAverage: -157.26666666666668\n",
      "Episode 45 done after 107 steps, reward Average: -156.17391304347825, up to now: minReward: -105.0, minAverage: -156.17391304347825\n",
      "Episode 46 done after 104 steps, reward Average: -155.06382978723406, up to now: minReward: -104.0, minAverage: -155.06382978723406\n",
      "Episode 47 done after 105 steps, reward Average: -154.02083333333334, up to now: minReward: -104.0, minAverage: -154.02083333333334\n",
      "Episode 48 done after 107 steps, reward Average: -153.0612244897959, up to now: minReward: -104.0, minAverage: -153.0612244897959\n",
      "Episode 49 done after 107 steps, reward Average: -152.14, up to now: minReward: -104.0, minAverage: -152.14\n",
      "Episode 50 done after 104 steps, reward Average: -151.19607843137254, up to now: minReward: -104.0, minAverage: -151.19607843137254\n",
      "Episode 51 done after 104 steps, reward Average: -150.28846153846155, up to now: minReward: -104.0, minAverage: -150.28846153846155\n",
      "Episode 52 done after 104 steps, reward Average: -149.41509433962264, up to now: minReward: -104.0, minAverage: -149.41509433962264\n",
      "Episode 53 done after 104 steps, reward Average: -148.57407407407408, up to now: minReward: -104.0, minAverage: -148.57407407407408\n",
      "Episode 54 done after 105 steps, reward Average: -147.78181818181818, up to now: minReward: -104.0, minAverage: -147.78181818181818\n",
      "Episode 55 done after 103 steps, reward Average: -146.98214285714286, up to now: minReward: -103.0, minAverage: -146.98214285714286\n",
      "Episode 56 done after 107 steps, reward Average: -146.28070175438597, up to now: minReward: -103.0, minAverage: -146.28070175438597\n",
      "Episode 57 done after 104 steps, reward Average: -145.55172413793105, up to now: minReward: -103.0, minAverage: -145.55172413793105\n",
      "Episode 58 done after 105 steps, reward Average: -144.864406779661, up to now: minReward: -103.0, minAverage: -144.864406779661\n",
      "Episode 59 done after 106 steps, reward Average: -144.21666666666667, up to now: minReward: -103.0, minAverage: -144.21666666666667\n",
      "Episode 60 done after 104 steps, reward Average: -143.55737704918033, up to now: minReward: -103.0, minAverage: -143.55737704918033\n",
      "Episode 61 done after 107 steps, reward Average: -142.96774193548387, up to now: minReward: -103.0, minAverage: -142.96774193548387\n",
      "Episode 62 done after 106 steps, reward Average: -142.38095238095238, up to now: minReward: -103.0, minAverage: -142.38095238095238\n",
      "Episode 63 done after 104 steps, reward Average: -141.78125, up to now: minReward: -103.0, minAverage: -141.78125\n",
      "Episode 64 done after 107 steps, reward Average: -141.24615384615385, up to now: minReward: -103.0, minAverage: -141.24615384615385\n",
      "Episode 65 done after 104 steps, reward Average: -140.6818181818182, up to now: minReward: -103.0, minAverage: -140.6818181818182\n",
      "Episode 66 done after 104 steps, reward Average: -140.13432835820896, up to now: minReward: -103.0, minAverage: -140.13432835820896\n",
      "Episode 67 done after 104 steps, reward Average: -139.60294117647058, up to now: minReward: -103.0, minAverage: -139.60294117647058\n",
      "Episode 68 done after 186 steps, reward Average: -140.2753623188406, up to now: minReward: -103.0, minAverage: -139.60294117647058\n",
      "Episode 69 done after 105 steps, reward Average: -139.77142857142857, up to now: minReward: -103.0, minAverage: -139.60294117647058\n",
      "Episode 70 done after 106 steps, reward Average: -139.29577464788733, up to now: minReward: -103.0, minAverage: -139.29577464788733\n",
      "Episode 71 done after 108 steps, reward Average: -138.86111111111111, up to now: minReward: -103.0, minAverage: -138.86111111111111\n",
      "Episode 72 done after 104 steps, reward Average: -138.3835616438356, up to now: minReward: -103.0, minAverage: -138.3835616438356\n",
      "Episode 73 done after 106 steps, reward Average: -137.94594594594594, up to now: minReward: -103.0, minAverage: -137.94594594594594\n",
      "Episode 74 done after 107 steps, reward Average: -137.53333333333333, up to now: minReward: -103.0, minAverage: -137.53333333333333\n",
      "Episode 75 done after 104 steps, reward Average: -137.0921052631579, up to now: minReward: -103.0, minAverage: -137.0921052631579\n",
      "Episode 76 done after 192 steps, reward Average: -137.80519480519482, up to now: minReward: -103.0, minAverage: -137.0921052631579\n",
      "Episode 77 done after 108 steps, reward Average: -137.42307692307693, up to now: minReward: -103.0, minAverage: -137.0921052631579\n",
      "Episode 78 done after 108 steps, reward Average: -137.0506329113924, up to now: minReward: -103.0, minAverage: -137.0506329113924\n",
      "Episode 79 done after 109 steps, reward Average: -136.7, up to now: minReward: -103.0, minAverage: -136.7\n",
      "Episode 80 done after 200 steps, reward Average: -137.4814814814815, up to now: minReward: -103.0, minAverage: -136.7\n",
      "Episode 81 done after 104 steps, reward Average: -137.0731707317073, up to now: minReward: -103.0, minAverage: -136.7\n",
      "Episode 82 done after 107 steps, reward Average: -136.710843373494, up to now: minReward: -103.0, minAverage: -136.7\n",
      "Episode 83 done after 104 steps, reward Average: -136.32142857142858, up to now: minReward: -103.0, minAverage: -136.32142857142858\n",
      "Episode 84 done after 108 steps, reward Average: -135.98823529411766, up to now: minReward: -103.0, minAverage: -135.98823529411766\n",
      "Episode 85 done after 105 steps, reward Average: -135.62790697674419, up to now: minReward: -103.0, minAverage: -135.62790697674419\n",
      "Episode 86 done after 108 steps, reward Average: -135.31034482758622, up to now: minReward: -103.0, minAverage: -135.31034482758622\n",
      "Episode 87 done after 105 steps, reward Average: -134.9659090909091, up to now: minReward: -103.0, minAverage: -134.9659090909091\n",
      "Episode 88 done after 107 steps, reward Average: -134.65168539325842, up to now: minReward: -103.0, minAverage: -134.65168539325842\n",
      "Episode 89 done after 105 steps, reward Average: -134.32222222222222, up to now: minReward: -103.0, minAverage: -134.32222222222222\n",
      "Episode 90 done after 104 steps, reward Average: -133.98901098901098, up to now: minReward: -103.0, minAverage: -133.98901098901098\n",
      "Episode 91 done after 104 steps, reward Average: -133.66304347826087, up to now: minReward: -103.0, minAverage: -133.66304347826087\n",
      "Episode 92 done after 102 steps, reward Average: -133.32258064516128, up to now: minReward: -102.0, minAverage: -133.32258064516128\n",
      "Episode 93 done after 87 steps, reward Average: -132.82978723404256, up to now: minReward: -87.0, minAverage: -132.82978723404256\n",
      "Episode 94 done after 97 steps, reward Average: -132.45263157894738, up to now: minReward: -87.0, minAverage: -132.45263157894738\n",
      "Episode 95 done after 104 steps, reward Average: -132.15625, up to now: minReward: -87.0, minAverage: -132.15625\n",
      "Episode 96 done after 89 steps, reward Average: -131.71134020618555, up to now: minReward: -87.0, minAverage: -131.71134020618555\n",
      "Episode 97 done after 109 steps, reward Average: -131.4795918367347, up to now: minReward: -87.0, minAverage: -131.4795918367347\n",
      "Episode 98 done after 105 steps, reward Average: -131.21212121212122, up to now: minReward: -87.0, minAverage: -131.21212121212122\n",
      "Episode 99 done after 106 steps, reward Average: -130.96, up to now: minReward: -87.0, minAverage: -130.96\n",
      "Episode 100 done after 105 steps, reward Average: -130.01, up to now: minReward: -87.0, minAverage: -130.01\n",
      "Episode 101 done after 108 steps, reward Average: -129.09, up to now: minReward: -87.0, minAverage: -129.09\n",
      "Episode 102 done after 104 steps, reward Average: -128.13, up to now: minReward: -87.0, minAverage: -128.13\n",
      "Episode 103 done after 105 steps, reward Average: -127.18, up to now: minReward: -87.0, minAverage: -127.18\n",
      "Episode 104 done after 107 steps, reward Average: -126.25, up to now: minReward: -87.0, minAverage: -126.25\n",
      "Episode 105 done after 105 steps, reward Average: -125.3, up to now: minReward: -87.0, minAverage: -125.3\n",
      "Episode 106 done after 104 steps, reward Average: -124.34, up to now: minReward: -87.0, minAverage: -124.34\n",
      "Episode 107 done after 105 steps, reward Average: -123.39, up to now: minReward: -87.0, minAverage: -123.39\n",
      "Episode 108 done after 104 steps, reward Average: -122.43, up to now: minReward: -87.0, minAverage: -122.43\n",
      "Episode 109 done after 104 steps, reward Average: -121.47, up to now: minReward: -87.0, minAverage: -121.47\n",
      "Episode 110 done after 105 steps, reward Average: -120.52, up to now: minReward: -87.0, minAverage: -120.52\n",
      "Episode 111 done after 107 steps, reward Average: -119.59, up to now: minReward: -87.0, minAverage: -119.59\n",
      "Episode 112 done after 190 steps, reward Average: -119.49, up to now: minReward: -87.0, minAverage: -119.49\n",
      "Episode 113 done after 106 steps, reward Average: -118.55, up to now: minReward: -87.0, minAverage: -118.55\n",
      "Episode 114 done after 104 steps, reward Average: -117.66, up to now: minReward: -87.0, minAverage: -117.66\n",
      "Episode 115 done after 104 steps, reward Average: -116.7, up to now: minReward: -87.0, minAverage: -116.7\n",
      "Episode 116 done after 200 steps, reward Average: -116.74, up to now: minReward: -87.0, minAverage: -116.7\n",
      "Episode 117 done after 105 steps, reward Average: -115.79, up to now: minReward: -87.0, minAverage: -115.79\n",
      "Episode 118 done after 109 steps, reward Average: -114.95, up to now: minReward: -87.0, minAverage: -114.95\n",
      "Episode 119 done after 104 steps, reward Average: -114.93, up to now: minReward: -87.0, minAverage: -114.93\n",
      "Episode 120 done after 109 steps, reward Average: -114.1, up to now: minReward: -87.0, minAverage: -114.1\n",
      "Episode 121 done after 104 steps, reward Average: -113.93, up to now: minReward: -87.0, minAverage: -113.93\n",
      "Episode 122 done after 104 steps, reward Average: -113.87, up to now: minReward: -87.0, minAverage: -113.87\n",
      "Episode 123 done after 108 steps, reward Average: -113.84, up to now: minReward: -87.0, minAverage: -113.84\n",
      "Episode 124 done after 107 steps, reward Average: -113.81, up to now: minReward: -87.0, minAverage: -113.81\n",
      "Episode 125 done after 107 steps, reward Average: -113.7, up to now: minReward: -87.0, minAverage: -113.7\n",
      "Episode 126 done after 107 steps, reward Average: -113.7, up to now: minReward: -87.0, minAverage: -113.7\n",
      "Episode 127 done after 103 steps, reward Average: -113.52, up to now: minReward: -87.0, minAverage: -113.52\n",
      "Episode 128 done after 106 steps, reward Average: -113.51, up to now: minReward: -87.0, minAverage: -113.51\n",
      "Episode 129 done after 107 steps, reward Average: -113.52, up to now: minReward: -87.0, minAverage: -113.51\n",
      "Episode 130 done after 105 steps, reward Average: -113.51, up to now: minReward: -87.0, minAverage: -113.51\n",
      "Episode 131 done after 107 steps, reward Average: -113.51, up to now: minReward: -87.0, minAverage: -113.51\n",
      "Episode 132 done after 104 steps, reward Average: -113.45, up to now: minReward: -87.0, minAverage: -113.45\n",
      "Episode 133 done after 104 steps, reward Average: -113.41, up to now: minReward: -87.0, minAverage: -113.41\n",
      "Episode 134 done after 107 steps, reward Average: -113.4, up to now: minReward: -87.0, minAverage: -113.4\n",
      "Episode 135 done after 104 steps, reward Average: -112.44, up to now: minReward: -87.0, minAverage: -112.44\n",
      "Episode 136 done after 104 steps, reward Average: -111.48, up to now: minReward: -87.0, minAverage: -111.48\n",
      "Episode 137 done after 104 steps, reward Average: -110.52, up to now: minReward: -87.0, minAverage: -110.52\n",
      "Episode 138 done after 104 steps, reward Average: -110.49, up to now: minReward: -87.0, minAverage: -110.49\n",
      "Episode 139 done after 104 steps, reward Average: -110.42, up to now: minReward: -87.0, minAverage: -110.42\n",
      "Episode 140 done after 107 steps, reward Average: -110.38, up to now: minReward: -87.0, minAverage: -110.38\n",
      "Episode 141 done after 105 steps, reward Average: -109.43, up to now: minReward: -87.0, minAverage: -109.43\n",
      "Episode 142 done after 104 steps, reward Average: -109.41, up to now: minReward: -87.0, minAverage: -109.41\n",
      "Episode 143 done after 104 steps, reward Average: -109.4, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 144 done after 108 steps, reward Average: -109.41, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 145 done after 111 steps, reward Average: -109.45, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 146 done after 106 steps, reward Average: -109.47, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 147 done after 107 steps, reward Average: -109.49, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 148 done after 103 steps, reward Average: -109.45, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 149 done after 104 steps, reward Average: -109.42, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 150 done after 106 steps, reward Average: -109.44, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 151 done after 106 steps, reward Average: -109.46, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 152 done after 107 steps, reward Average: -109.49, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 153 done after 110 steps, reward Average: -109.55, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 154 done after 106 steps, reward Average: -109.56, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 155 done after 147 steps, reward Average: -110.0, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 156 done after 103 steps, reward Average: -109.96, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 157 done after 105 steps, reward Average: -109.97, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 158 done after 107 steps, reward Average: -109.99, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 159 done after 114 steps, reward Average: -110.07, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 160 done after 111 steps, reward Average: -110.14, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 161 done after 109 steps, reward Average: -110.16, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 162 done after 105 steps, reward Average: -110.15, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 163 done after 107 steps, reward Average: -110.18, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 164 done after 105 steps, reward Average: -110.16, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 165 done after 105 steps, reward Average: -110.17, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 166 done after 105 steps, reward Average: -110.18, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 167 done after 104 steps, reward Average: -110.18, up to now: minReward: -87.0, minAverage: -109.4\n",
      "Episode 168 done after 104 steps, reward Average: -109.36, up to now: minReward: -87.0, minAverage: -109.36\n",
      "Episode 169 done after 105 steps, reward Average: -109.36, up to now: minReward: -87.0, minAverage: -109.36\n",
      "Episode 170 done after 104 steps, reward Average: -109.34, up to now: minReward: -87.0, minAverage: -109.34\n",
      "Episode 171 done after 104 steps, reward Average: -109.3, up to now: minReward: -87.0, minAverage: -109.3\n",
      "Episode 172 done after 105 steps, reward Average: -109.31, up to now: minReward: -87.0, minAverage: -109.3\n",
      "Episode 173 done after 103 steps, reward Average: -109.28, up to now: minReward: -87.0, minAverage: -109.28\n",
      "Episode 174 done after 106 steps, reward Average: -109.27, up to now: minReward: -87.0, minAverage: -109.27\n",
      "Episode 175 done after 105 steps, reward Average: -109.28, up to now: minReward: -87.0, minAverage: -109.27\n",
      "Episode 176 done after 105 steps, reward Average: -108.41, up to now: minReward: -87.0, minAverage: -108.41\n",
      "Episode 177 done after 109 steps, reward Average: -108.42, up to now: minReward: -87.0, minAverage: -108.41\n",
      "Episode 178 done after 104 steps, reward Average: -108.38, up to now: minReward: -87.0, minAverage: -108.38\n",
      "Episode 179 done after 107 steps, reward Average: -108.36, up to now: minReward: -87.0, minAverage: -108.36\n",
      "Episode 180 done after 106 steps, reward Average: -107.42, up to now: minReward: -87.0, minAverage: -107.42\n",
      "Episode 181 done after 105 steps, reward Average: -107.43, up to now: minReward: -87.0, minAverage: -107.42\n",
      "Episode 182 done after 104 steps, reward Average: -107.4, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 183 done after 105 steps, reward Average: -107.41, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 184 done after 107 steps, reward Average: -107.4, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 185 done after 108 steps, reward Average: -107.43, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 186 done after 107 steps, reward Average: -107.42, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 187 done after 105 steps, reward Average: -107.42, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 188 done after 107 steps, reward Average: -107.42, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 189 done after 105 steps, reward Average: -107.42, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 190 done after 104 steps, reward Average: -107.42, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 191 done after 104 steps, reward Average: -107.42, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 192 done after 107 steps, reward Average: -107.47, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 193 done after 104 steps, reward Average: -107.64, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 194 done after 103 steps, reward Average: -107.7, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 195 done after 105 steps, reward Average: -107.71, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 196 done after 104 steps, reward Average: -107.86, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 197 done after 105 steps, reward Average: -107.82, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 198 done after 104 steps, reward Average: -107.81, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 199 done after 104 steps, reward Average: -107.79, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 200 done after 106 steps, reward Average: -107.8, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 201 done after 103 steps, reward Average: -107.75, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 202 done after 105 steps, reward Average: -107.76, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 203 done after 105 steps, reward Average: -107.76, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 204 done after 107 steps, reward Average: -107.76, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 205 done after 107 steps, reward Average: -107.78, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 206 done after 104 steps, reward Average: -107.78, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 207 done after 104 steps, reward Average: -107.77, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 208 done after 105 steps, reward Average: -107.78, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 209 done after 108 steps, reward Average: -107.82, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 210 done after 105 steps, reward Average: -107.82, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 211 done after 105 steps, reward Average: -107.8, up to now: minReward: -87.0, minAverage: -107.4\n",
      "Episode 212 done after 107 steps, reward Average: -106.97, up to now: minReward: -87.0, minAverage: -106.97\n",
      "Episode 213 done after 105 steps, reward Average: -106.96, up to now: minReward: -87.0, minAverage: -106.96\n",
      "Episode 214 done after 105 steps, reward Average: -106.97, up to now: minReward: -87.0, minAverage: -106.96\n",
      "Episode 215 done after 108 steps, reward Average: -107.01, up to now: minReward: -87.0, minAverage: -106.96\n",
      "Episode 216 done after 107 steps, reward Average: -106.08, up to now: minReward: -87.0, minAverage: -106.08\n",
      "Episode 217 done after 105 steps, reward Average: -106.08, up to now: minReward: -87.0, minAverage: -106.08\n",
      "Episode 218 done after 105 steps, reward Average: -106.04, up to now: minReward: -87.0, minAverage: -106.04\n",
      "Episode 219 done after 106 steps, reward Average: -106.06, up to now: minReward: -87.0, minAverage: -106.04\n",
      "Episode 220 done after 104 steps, reward Average: -106.01, up to now: minReward: -87.0, minAverage: -106.01\n",
      "Episode 221 done after 104 steps, reward Average: -106.01, up to now: minReward: -87.0, minAverage: -106.01\n",
      "Episode 222 done after 108 steps, reward Average: -106.05, up to now: minReward: -87.0, minAverage: -106.01\n",
      "Episode 223 done after 107 steps, reward Average: -106.04, up to now: minReward: -87.0, minAverage: -106.01\n",
      "Episode 224 done after 104 steps, reward Average: -106.01, up to now: minReward: -87.0, minAverage: -106.01\n",
      "Episode 225 done after 105 steps, reward Average: -105.99, up to now: minReward: -87.0, minAverage: -105.99\n",
      "Episode 226 done after 105 steps, reward Average: -105.97, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 227 done after 106 steps, reward Average: -106.0, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 228 done after 105 steps, reward Average: -105.99, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 229 done after 106 steps, reward Average: -105.98, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 230 done after 122 steps, reward Average: -106.15, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 231 done after 106 steps, reward Average: -106.14, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 232 done after 105 steps, reward Average: -106.15, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 233 done after 105 steps, reward Average: -106.16, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 234 done after 107 steps, reward Average: -106.16, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 235 done after 99 steps, reward Average: -106.11, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 236 done after 91 steps, reward Average: -105.98, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 237 done after 108 steps, reward Average: -106.02, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 238 done after 108 steps, reward Average: -106.06, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 239 done after 104 steps, reward Average: -106.06, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 240 done after 108 steps, reward Average: -106.07, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 241 done after 104 steps, reward Average: -106.06, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 242 done after 104 steps, reward Average: -106.06, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 243 done after 105 steps, reward Average: -106.07, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 244 done after 105 steps, reward Average: -106.04, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 245 done after 104 steps, reward Average: -105.97, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 246 done after 106 steps, reward Average: -105.97, up to now: minReward: -87.0, minAverage: -105.97\n",
      "Episode 247 done after 105 steps, reward Average: -105.95, up to now: minReward: -87.0, minAverage: -105.95\n",
      "Episode 248 done after 103 steps, reward Average: -105.95, up to now: minReward: -87.0, minAverage: -105.95\n",
      "Episode 249 done after 104 steps, reward Average: -105.95, up to now: minReward: -87.0, minAverage: -105.95\n",
      "Episode 250 done after 104 steps, reward Average: -105.93, up to now: minReward: -87.0, minAverage: -105.93\n",
      "Episode 251 done after 104 steps, reward Average: -105.91, up to now: minReward: -87.0, minAverage: -105.91\n",
      "Episode 252 done after 107 steps, reward Average: -105.91, up to now: minReward: -87.0, minAverage: -105.91\n",
      "Episode 253 done after 109 steps, reward Average: -105.9, up to now: minReward: -87.0, minAverage: -105.9\n",
      "Episode 254 done after 104 steps, reward Average: -105.88, up to now: minReward: -87.0, minAverage: -105.88\n",
      "Episode 255 done after 104 steps, reward Average: -105.45, up to now: minReward: -87.0, minAverage: -105.45\n",
      "Episode 256 done after 104 steps, reward Average: -105.46, up to now: minReward: -87.0, minAverage: -105.45\n",
      "Episode 257 done after 106 steps, reward Average: -105.47, up to now: minReward: -87.0, minAverage: -105.45\n",
      "Episode 258 done after 108 steps, reward Average: -105.48, up to now: minReward: -87.0, minAverage: -105.45\n",
      "Episode 259 done after 104 steps, reward Average: -105.38, up to now: minReward: -87.0, minAverage: -105.38\n",
      "Episode 260 done after 103 steps, reward Average: -105.3, up to now: minReward: -87.0, minAverage: -105.3\n",
      "Episode 261 done after 107 steps, reward Average: -105.28, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 262 done after 108 steps, reward Average: -105.31, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 263 done after 104 steps, reward Average: -105.28, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 264 done after 105 steps, reward Average: -105.28, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 265 done after 105 steps, reward Average: -105.28, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 266 done after 107 steps, reward Average: -105.3, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 267 done after 104 steps, reward Average: -105.3, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 268 done after 104 steps, reward Average: -105.3, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 269 done after 104 steps, reward Average: -105.29, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 270 done after 104 steps, reward Average: -105.29, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 271 done after 105 steps, reward Average: -105.3, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 272 done after 108 steps, reward Average: -105.33, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 273 done after 107 steps, reward Average: -105.37, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 274 done after 106 steps, reward Average: -105.37, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 275 done after 104 steps, reward Average: -105.36, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 276 done after 104 steps, reward Average: -105.35, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 277 done after 105 steps, reward Average: -105.31, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 278 done after 104 steps, reward Average: -105.31, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 279 done after 105 steps, reward Average: -105.29, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 280 done after 107 steps, reward Average: -105.3, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 281 done after 104 steps, reward Average: -105.29, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 282 done after 105 steps, reward Average: -105.3, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 283 done after 105 steps, reward Average: -105.3, up to now: minReward: -87.0, minAverage: -105.28\n",
      "Episode 284 done after 104 steps, reward Average: -105.27, up to now: minReward: -87.0, minAverage: -105.27\n",
      "Episode 285 done after 105 steps, reward Average: -105.24, up to now: minReward: -87.0, minAverage: -105.24\n",
      "Episode 286 done after 107 steps, reward Average: -105.24, up to now: minReward: -87.0, minAverage: -105.24\n",
      "Episode 287 done after 104 steps, reward Average: -105.23, up to now: minReward: -87.0, minAverage: -105.23\n",
      "Episode 288 done after 105 steps, reward Average: -105.21, up to now: minReward: -87.0, minAverage: -105.21\n",
      "Episode 289 done after 105 steps, reward Average: -105.21, up to now: minReward: -87.0, minAverage: -105.21\n",
      "Episode 290 done after 108 steps, reward Average: -105.25, up to now: minReward: -87.0, minAverage: -105.21\n",
      "Episode 291 done after 104 steps, reward Average: -105.25, up to now: minReward: -87.0, minAverage: -105.21\n",
      "Episode 292 done after 104 steps, reward Average: -105.22, up to now: minReward: -87.0, minAverage: -105.21\n",
      "Episode 293 done after 106 steps, reward Average: -105.24, up to now: minReward: -87.0, minAverage: -105.21\n",
      "Episode 294 done after 107 steps, reward Average: -105.28, up to now: minReward: -87.0, minAverage: -105.21\n",
      "Episode 295 done after 89 steps, reward Average: -105.12, up to now: minReward: -87.0, minAverage: -105.12\n",
      "Episode 296 done after 105 steps, reward Average: -105.13, up to now: minReward: -87.0, minAverage: -105.12\n",
      "Episode 297 done after 106 steps, reward Average: -105.14, up to now: minReward: -87.0, minAverage: -105.12\n",
      "Episode 298 done after 106 steps, reward Average: -105.16, up to now: minReward: -87.0, minAverage: -105.12\n",
      "Episode 299 done after 105 steps, reward Average: -105.17, up to now: minReward: -87.0, minAverage: -105.12\n",
      "final result: \n",
      "278 times arrived in 300 episodes, first time in episode 14\n",
      "problem solved?:True\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17a63ccf438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) +  (R +  Q(S, A)  Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
