{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile Model: #\n",
    "\n",
    "In this class there can be found both the tile coding, as well as the state model to learn.\n",
    "\n",
    "Tile coding is implemented straight forward:\n",
    "The observation, in out case position and velocity, is first shifted to avoid negative coordinated, and later scaled to the grid size. \n",
    "\n",
    "If we expect a grid of 8*8, after scaling and shifting, the range of the input is between [0,8] in both dimensions.\n",
    "\n",
    "For tiling the grids of each tiling level are shifted to its neighbour by 1/numberOfTilings. Before being copied into the coordinates grid (tiling\\*gridsize\\*gridsize)the observation is shifted accordingly and casted to an int.\n",
    "\n",
    "Now we have a lot of discrete points in the space (tiling\\*gridsize\\*gridsize). In a second step this 3D address of a point is mapped to point to a 1D-array to store the states. Mapping is done straigthforward: \n",
    "\n",
    "Tiling 0: mapped to 0...63\n",
    "Tiling 1: mapped to 64 ... 127 and so on.\n",
    "\n",
    "Inside one tiling level it is simiiar:\n",
    "(position,velocity) is mapped to position+8*velocity.\n",
    "\n",
    "As result we get an index vector, containing the index for each tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 07:14:25,810] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.01\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "### strategyAlpha=6\n",
    "\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 172 steps, reward Average: -197.84615384615384, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 13 done after 200 steps, reward Average: -198.0, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 14 done after 200 steps, reward Average: -198.13333333333333, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 15 done after 200 steps, reward Average: -198.25, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 16 done after 200 steps, reward Average: -198.35294117647058, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 17 done after 200 steps, reward Average: -198.44444444444446, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 18 done after 200 steps, reward Average: -198.52631578947367, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 19 done after 200 steps, reward Average: -198.6, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 20 done after 200 steps, reward Average: -198.66666666666666, up to now: minReward: -172.0, minAverage: -197.84615384615384\n",
      "Episode 21 done after 170 steps, reward Average: -197.36363636363637, up to now: minReward: -170.0, minAverage: -197.36363636363637\n",
      "Episode 22 done after 161 steps, reward Average: -195.7826086956522, up to now: minReward: -161.0, minAverage: -195.7826086956522\n",
      "Episode 23 done after 158 steps, reward Average: -194.20833333333334, up to now: minReward: -158.0, minAverage: -194.20833333333334\n",
      "Episode 24 done after 200 steps, reward Average: -194.44, up to now: minReward: -158.0, minAverage: -194.20833333333334\n",
      "Episode 25 done after 163 steps, reward Average: -193.23076923076923, up to now: minReward: -158.0, minAverage: -193.23076923076923\n",
      "Episode 26 done after 167 steps, reward Average: -192.25925925925927, up to now: minReward: -158.0, minAverage: -192.25925925925927\n",
      "Episode 27 done after 169 steps, reward Average: -191.42857142857142, up to now: minReward: -158.0, minAverage: -191.42857142857142\n",
      "Episode 28 done after 200 steps, reward Average: -191.72413793103448, up to now: minReward: -158.0, minAverage: -191.42857142857142\n",
      "Episode 29 done after 200 steps, reward Average: -192.0, up to now: minReward: -158.0, minAverage: -191.42857142857142\n",
      "Episode 30 done after 164 steps, reward Average: -191.09677419354838, up to now: minReward: -158.0, minAverage: -191.09677419354838\n",
      "Episode 31 done after 164 steps, reward Average: -190.25, up to now: minReward: -158.0, minAverage: -190.25\n",
      "Episode 32 done after 95 steps, reward Average: -187.36363636363637, up to now: minReward: -95.0, minAverage: -187.36363636363637\n",
      "Episode 33 done after 162 steps, reward Average: -186.61764705882354, up to now: minReward: -95.0, minAverage: -186.61764705882354\n",
      "Episode 34 done after 158 steps, reward Average: -185.8, up to now: minReward: -95.0, minAverage: -185.8\n",
      "Episode 35 done after 179 steps, reward Average: -185.61111111111111, up to now: minReward: -95.0, minAverage: -185.61111111111111\n",
      "Episode 36 done after 169 steps, reward Average: -185.16216216216216, up to now: minReward: -95.0, minAverage: -185.16216216216216\n",
      "Episode 37 done after 168 steps, reward Average: -184.71052631578948, up to now: minReward: -95.0, minAverage: -184.71052631578948\n",
      "Episode 38 done after 89 steps, reward Average: -182.25641025641025, up to now: minReward: -89.0, minAverage: -182.25641025641025\n",
      "Episode 39 done after 154 steps, reward Average: -181.55, up to now: minReward: -89.0, minAverage: -181.55\n",
      "Episode 40 done after 159 steps, reward Average: -181.0, up to now: minReward: -89.0, minAverage: -181.0\n",
      "Episode 41 done after 177 steps, reward Average: -180.9047619047619, up to now: minReward: -89.0, minAverage: -180.9047619047619\n",
      "Episode 42 done after 165 steps, reward Average: -180.53488372093022, up to now: minReward: -89.0, minAverage: -180.53488372093022\n",
      "Episode 43 done after 155 steps, reward Average: -179.95454545454547, up to now: minReward: -89.0, minAverage: -179.95454545454547\n",
      "Episode 44 done after 164 steps, reward Average: -179.6, up to now: minReward: -89.0, minAverage: -179.6\n",
      "Episode 45 done after 171 steps, reward Average: -179.41304347826087, up to now: minReward: -89.0, minAverage: -179.41304347826087\n",
      "Episode 46 done after 154 steps, reward Average: -178.87234042553192, up to now: minReward: -89.0, minAverage: -178.87234042553192\n",
      "Episode 47 done after 89 steps, reward Average: -177.0, up to now: minReward: -89.0, minAverage: -177.0\n",
      "Episode 48 done after 150 steps, reward Average: -176.44897959183675, up to now: minReward: -89.0, minAverage: -176.44897959183675\n",
      "Episode 49 done after 155 steps, reward Average: -176.02, up to now: minReward: -89.0, minAverage: -176.02\n",
      "Episode 50 done after 150 steps, reward Average: -175.50980392156862, up to now: minReward: -89.0, minAverage: -175.50980392156862\n",
      "Episode 51 done after 145 steps, reward Average: -174.92307692307693, up to now: minReward: -89.0, minAverage: -174.92307692307693\n",
      "Episode 52 done after 155 steps, reward Average: -174.54716981132074, up to now: minReward: -89.0, minAverage: -174.54716981132074\n",
      "Episode 53 done after 91 steps, reward Average: -173.0, up to now: minReward: -89.0, minAverage: -173.0\n",
      "Episode 54 done after 90 steps, reward Average: -171.4909090909091, up to now: minReward: -89.0, minAverage: -171.4909090909091\n",
      "Episode 55 done after 143 steps, reward Average: -170.98214285714286, up to now: minReward: -89.0, minAverage: -170.98214285714286\n",
      "Episode 56 done after 161 steps, reward Average: -170.80701754385964, up to now: minReward: -89.0, minAverage: -170.80701754385964\n",
      "Episode 57 done after 151 steps, reward Average: -170.4655172413793, up to now: minReward: -89.0, minAverage: -170.4655172413793\n",
      "Episode 58 done after 145 steps, reward Average: -170.03389830508473, up to now: minReward: -89.0, minAverage: -170.03389830508473\n",
      "Episode 59 done after 163 steps, reward Average: -169.91666666666666, up to now: minReward: -89.0, minAverage: -169.91666666666666\n",
      "Episode 60 done after 141 steps, reward Average: -169.44262295081967, up to now: minReward: -89.0, minAverage: -169.44262295081967\n",
      "Episode 61 done after 150 steps, reward Average: -169.1290322580645, up to now: minReward: -89.0, minAverage: -169.1290322580645\n",
      "Episode 62 done after 141 steps, reward Average: -168.68253968253967, up to now: minReward: -89.0, minAverage: -168.68253968253967\n",
      "Episode 63 done after 141 steps, reward Average: -168.25, up to now: minReward: -89.0, minAverage: -168.25\n",
      "Episode 64 done after 155 steps, reward Average: -168.04615384615386, up to now: minReward: -89.0, minAverage: -168.04615384615386\n",
      "Episode 65 done after 165 steps, reward Average: -168.0, up to now: minReward: -89.0, minAverage: -168.0\n",
      "Episode 66 done after 148 steps, reward Average: -167.70149253731344, up to now: minReward: -89.0, minAverage: -167.70149253731344\n",
      "Episode 67 done after 146 steps, reward Average: -167.38235294117646, up to now: minReward: -89.0, minAverage: -167.38235294117646\n",
      "Episode 68 done after 143 steps, reward Average: -167.02898550724638, up to now: minReward: -89.0, minAverage: -167.02898550724638\n",
      "Episode 69 done after 141 steps, reward Average: -166.65714285714284, up to now: minReward: -89.0, minAverage: -166.65714285714284\n",
      "Episode 70 done after 148 steps, reward Average: -166.3943661971831, up to now: minReward: -89.0, minAverage: -166.3943661971831\n",
      "Episode 71 done after 147 steps, reward Average: -166.125, up to now: minReward: -89.0, minAverage: -166.125\n",
      "Episode 72 done after 143 steps, reward Average: -165.8082191780822, up to now: minReward: -89.0, minAverage: -165.8082191780822\n",
      "Episode 73 done after 143 steps, reward Average: -165.5, up to now: minReward: -89.0, minAverage: -165.5\n",
      "Episode 74 done after 159 steps, reward Average: -165.41333333333333, up to now: minReward: -89.0, minAverage: -165.41333333333333\n",
      "Episode 75 done after 144 steps, reward Average: -165.1315789473684, up to now: minReward: -89.0, minAverage: -165.1315789473684\n",
      "Episode 76 done after 146 steps, reward Average: -164.88311688311688, up to now: minReward: -89.0, minAverage: -164.88311688311688\n",
      "Episode 77 done after 147 steps, reward Average: -164.65384615384616, up to now: minReward: -89.0, minAverage: -164.65384615384616\n",
      "Episode 78 done after 141 steps, reward Average: -164.35443037974684, up to now: minReward: -89.0, minAverage: -164.35443037974684\n",
      "Episode 79 done after 84 steps, reward Average: -163.35, up to now: minReward: -84.0, minAverage: -163.35\n",
      "Episode 80 done after 144 steps, reward Average: -163.11111111111111, up to now: minReward: -84.0, minAverage: -163.11111111111111\n",
      "Episode 81 done after 144 steps, reward Average: -162.8780487804878, up to now: minReward: -84.0, minAverage: -162.8780487804878\n",
      "Episode 82 done after 144 steps, reward Average: -162.65060240963857, up to now: minReward: -84.0, minAverage: -162.65060240963857\n",
      "Episode 83 done after 139 steps, reward Average: -162.36904761904762, up to now: minReward: -84.0, minAverage: -162.36904761904762\n",
      "Episode 84 done after 159 steps, reward Average: -162.3294117647059, up to now: minReward: -84.0, minAverage: -162.3294117647059\n",
      "Episode 85 done after 139 steps, reward Average: -162.0581395348837, up to now: minReward: -84.0, minAverage: -162.0581395348837\n",
      "Episode 86 done after 147 steps, reward Average: -161.88505747126436, up to now: minReward: -84.0, minAverage: -161.88505747126436\n",
      "Episode 87 done after 143 steps, reward Average: -161.67045454545453, up to now: minReward: -84.0, minAverage: -161.67045454545453\n",
      "Episode 88 done after 142 steps, reward Average: -161.4494382022472, up to now: minReward: -84.0, minAverage: -161.4494382022472\n",
      "Episode 89 done after 165 steps, reward Average: -161.48888888888888, up to now: minReward: -84.0, minAverage: -161.4494382022472\n",
      "Episode 90 done after 159 steps, reward Average: -161.46153846153845, up to now: minReward: -84.0, minAverage: -161.4494382022472\n",
      "Episode 91 done after 156 steps, reward Average: -161.40217391304347, up to now: minReward: -84.0, minAverage: -161.40217391304347\n",
      "Episode 92 done after 150 steps, reward Average: -161.27956989247312, up to now: minReward: -84.0, minAverage: -161.27956989247312\n",
      "Episode 93 done after 88 steps, reward Average: -160.5, up to now: minReward: -84.0, minAverage: -160.5\n",
      "Episode 94 done after 151 steps, reward Average: -160.4, up to now: minReward: -84.0, minAverage: -160.4\n",
      "Episode 95 done after 88 steps, reward Average: -159.64583333333334, up to now: minReward: -84.0, minAverage: -159.64583333333334\n",
      "Episode 96 done after 149 steps, reward Average: -159.53608247422682, up to now: minReward: -84.0, minAverage: -159.53608247422682\n",
      "Episode 97 done after 144 steps, reward Average: -159.37755102040816, up to now: minReward: -84.0, minAverage: -159.37755102040816\n",
      "Episode 98 done after 144 steps, reward Average: -159.22222222222223, up to now: minReward: -84.0, minAverage: -159.22222222222223\n",
      "Episode 99 done after 85 steps, reward Average: -158.48, up to now: minReward: -84.0, minAverage: -158.48\n",
      "Episode 100 done after 149 steps, reward Average: -157.97, up to now: minReward: -84.0, minAverage: -157.97\n",
      "Episode 101 done after 88 steps, reward Average: -156.85, up to now: minReward: -84.0, minAverage: -156.85\n",
      "Episode 102 done after 143 steps, reward Average: -156.28, up to now: minReward: -84.0, minAverage: -156.28\n",
      "Episode 103 done after 145 steps, reward Average: -155.73, up to now: minReward: -84.0, minAverage: -155.73\n",
      "Episode 104 done after 140 steps, reward Average: -155.13, up to now: minReward: -84.0, minAverage: -155.13\n",
      "Episode 105 done after 147 steps, reward Average: -154.6, up to now: minReward: -84.0, minAverage: -154.6\n",
      "Episode 106 done after 140 steps, reward Average: -154.0, up to now: minReward: -84.0, minAverage: -154.0\n",
      "Episode 107 done after 146 steps, reward Average: -153.46, up to now: minReward: -84.0, minAverage: -153.46\n",
      "Episode 108 done after 139 steps, reward Average: -152.85, up to now: minReward: -84.0, minAverage: -152.85\n",
      "Episode 109 done after 87 steps, reward Average: -151.72, up to now: minReward: -84.0, minAverage: -151.72\n",
      "Episode 110 done after 88 steps, reward Average: -150.6, up to now: minReward: -84.0, minAverage: -150.6\n",
      "Episode 111 done after 140 steps, reward Average: -150.0, up to now: minReward: -84.0, minAverage: -150.0\n",
      "Episode 112 done after 88 steps, reward Average: -149.16, up to now: minReward: -84.0, minAverage: -149.16\n",
      "Episode 113 done after 152 steps, reward Average: -148.68, up to now: minReward: -84.0, minAverage: -148.68\n",
      "Episode 114 done after 152 steps, reward Average: -148.2, up to now: minReward: -84.0, minAverage: -148.2\n",
      "Episode 115 done after 141 steps, reward Average: -147.61, up to now: minReward: -84.0, minAverage: -147.61\n",
      "Episode 116 done after 140 steps, reward Average: -147.01, up to now: minReward: -84.0, minAverage: -147.01\n",
      "Episode 117 done after 143 steps, reward Average: -146.44, up to now: minReward: -84.0, minAverage: -146.44\n",
      "Episode 118 done after 88 steps, reward Average: -145.32, up to now: minReward: -84.0, minAverage: -145.32\n",
      "Episode 119 done after 139 steps, reward Average: -144.71, up to now: minReward: -84.0, minAverage: -144.71\n",
      "Episode 120 done after 142 steps, reward Average: -144.13, up to now: minReward: -84.0, minAverage: -144.13\n",
      "Episode 121 done after 143 steps, reward Average: -143.86, up to now: minReward: -84.0, minAverage: -143.86\n",
      "Episode 122 done after 84 steps, reward Average: -143.09, up to now: minReward: -84.0, minAverage: -143.09\n",
      "Episode 123 done after 144 steps, reward Average: -142.95, up to now: minReward: -84.0, minAverage: -142.95\n",
      "Episode 124 done after 144 steps, reward Average: -142.39, up to now: minReward: -84.0, minAverage: -142.39\n",
      "Episode 125 done after 86 steps, reward Average: -141.62, up to now: minReward: -84.0, minAverage: -141.62\n",
      "Episode 126 done after 90 steps, reward Average: -140.85, up to now: minReward: -84.0, minAverage: -140.85\n",
      "Episode 127 done after 147 steps, reward Average: -140.63, up to now: minReward: -84.0, minAverage: -140.63\n",
      "Episode 128 done after 146 steps, reward Average: -140.09, up to now: minReward: -84.0, minAverage: -140.09\n",
      "Episode 129 done after 144 steps, reward Average: -139.53, up to now: minReward: -84.0, minAverage: -139.53\n",
      "Episode 130 done after 137 steps, reward Average: -139.26, up to now: minReward: -84.0, minAverage: -139.26\n",
      "Episode 131 done after 89 steps, reward Average: -138.51, up to now: minReward: -84.0, minAverage: -138.51\n",
      "Episode 132 done after 139 steps, reward Average: -138.95, up to now: minReward: -84.0, minAverage: -138.51\n",
      "Episode 133 done after 148 steps, reward Average: -138.81, up to now: minReward: -84.0, minAverage: -138.51\n",
      "Episode 134 done after 141 steps, reward Average: -138.64, up to now: minReward: -84.0, minAverage: -138.51\n",
      "Episode 135 done after 87 steps, reward Average: -137.72, up to now: minReward: -84.0, minAverage: -137.72\n",
      "Episode 136 done after 144 steps, reward Average: -137.47, up to now: minReward: -84.0, minAverage: -137.47\n",
      "Episode 137 done after 138 steps, reward Average: -137.17, up to now: minReward: -84.0, minAverage: -137.17\n",
      "Episode 138 done after 151 steps, reward Average: -137.79, up to now: minReward: -84.0, minAverage: -137.17\n",
      "Episode 139 done after 148 steps, reward Average: -137.73, up to now: minReward: -84.0, minAverage: -137.17\n",
      "Episode 140 done after 141 steps, reward Average: -137.55, up to now: minReward: -84.0, minAverage: -137.17\n",
      "Episode 141 done after 139 steps, reward Average: -137.17, up to now: minReward: -84.0, minAverage: -137.17\n",
      "Episode 142 done after 140 steps, reward Average: -136.92, up to now: minReward: -84.0, minAverage: -136.92\n",
      "Episode 143 done after 141 steps, reward Average: -136.78, up to now: minReward: -84.0, minAverage: -136.78\n",
      "Episode 144 done after 146 steps, reward Average: -136.6, up to now: minReward: -84.0, minAverage: -136.6\n",
      "Episode 145 done after 142 steps, reward Average: -136.31, up to now: minReward: -84.0, minAverage: -136.31\n",
      "Episode 146 done after 149 steps, reward Average: -136.26, up to now: minReward: -84.0, minAverage: -136.26\n",
      "Episode 147 done after 145 steps, reward Average: -136.82, up to now: minReward: -84.0, minAverage: -136.26\n",
      "Episode 148 done after 142 steps, reward Average: -136.74, up to now: minReward: -84.0, minAverage: -136.26\n",
      "Episode 149 done after 88 steps, reward Average: -136.07, up to now: minReward: -84.0, minAverage: -136.07\n",
      "Episode 150 done after 138 steps, reward Average: -135.95, up to now: minReward: -84.0, minAverage: -135.95\n",
      "Episode 151 done after 138 steps, reward Average: -135.88, up to now: minReward: -84.0, minAverage: -135.88\n",
      "Episode 152 done after 138 steps, reward Average: -135.71, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 153 done after 142 steps, reward Average: -136.22, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 154 done after 142 steps, reward Average: -136.74, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 155 done after 145 steps, reward Average: -136.76, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 156 done after 152 steps, reward Average: -136.67, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 157 done after 139 steps, reward Average: -136.55, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 158 done after 138 steps, reward Average: -136.48, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 159 done after 138 steps, reward Average: -136.23, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 160 done after 138 steps, reward Average: -136.2, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 161 done after 145 steps, reward Average: -136.15, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 162 done after 156 steps, reward Average: -136.3, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 163 done after 137 steps, reward Average: -136.26, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 164 done after 149 steps, reward Average: -136.2, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 165 done after 144 steps, reward Average: -135.99, up to now: minReward: -84.0, minAverage: -135.71\n",
      "Episode 166 done after 83 steps, reward Average: -135.34, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 167 done after 200 steps, reward Average: -135.88, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 168 done after 155 steps, reward Average: -136.0, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 169 done after 158 steps, reward Average: -136.17, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 170 done after 143 steps, reward Average: -136.12, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 171 done after 140 steps, reward Average: -136.05, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 172 done after 144 steps, reward Average: -136.06, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 173 done after 147 steps, reward Average: -136.1, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 174 done after 154 steps, reward Average: -136.05, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 175 done after 169 steps, reward Average: -136.3, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 176 done after 167 steps, reward Average: -136.51, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 177 done after 156 steps, reward Average: -136.6, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 178 done after 152 steps, reward Average: -136.71, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 179 done after 84 steps, reward Average: -136.71, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 180 done after 143 steps, reward Average: -136.7, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 181 done after 148 steps, reward Average: -136.74, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 182 done after 90 steps, reward Average: -136.2, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 183 done after 200 steps, reward Average: -136.81, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 184 done after 88 steps, reward Average: -136.1, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 185 done after 143 steps, reward Average: -136.14, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 186 done after 154 steps, reward Average: -136.21, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 187 done after 149 steps, reward Average: -136.27, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 188 done after 150 steps, reward Average: -136.35, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 189 done after 141 steps, reward Average: -136.11, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 190 done after 186 steps, reward Average: -136.38, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 191 done after 168 steps, reward Average: -136.5, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 192 done after 145 steps, reward Average: -136.45, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 193 done after 142 steps, reward Average: -136.99, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 194 done after 143 steps, reward Average: -136.91, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 195 done after 182 steps, reward Average: -137.85, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 196 done after 142 steps, reward Average: -137.78, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 197 done after 151 steps, reward Average: -137.85, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 198 done after 147 steps, reward Average: -137.88, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 199 done after 138 steps, reward Average: -138.41, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 200 done after 141 steps, reward Average: -138.33, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 201 done after 142 steps, reward Average: -138.87, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 202 done after 136 steps, reward Average: -138.8, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 203 done after 136 steps, reward Average: -138.71, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 204 done after 136 steps, reward Average: -138.67, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 205 done after 155 steps, reward Average: -138.75, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 206 done after 142 steps, reward Average: -138.77, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 207 done after 142 steps, reward Average: -138.73, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 208 done after 146 steps, reward Average: -138.8, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 209 done after 140 steps, reward Average: -139.33, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 210 done after 138 steps, reward Average: -139.83, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 211 done after 134 steps, reward Average: -139.77, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 212 done after 140 steps, reward Average: -140.29, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 213 done after 136 steps, reward Average: -140.13, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 214 done after 141 steps, reward Average: -140.02, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 215 done after 158 steps, reward Average: -140.19, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 216 done after 135 steps, reward Average: -140.14, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 217 done after 135 steps, reward Average: -140.06, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 218 done after 88 steps, reward Average: -140.06, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 219 done after 144 steps, reward Average: -140.11, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 220 done after 146 steps, reward Average: -140.15, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 221 done after 142 steps, reward Average: -140.14, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 222 done after 136 steps, reward Average: -140.66, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 223 done after 135 steps, reward Average: -140.57, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 224 done after 86 steps, reward Average: -139.99, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 225 done after 136 steps, reward Average: -140.49, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 226 done after 142 steps, reward Average: -141.01, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 227 done after 140 steps, reward Average: -140.94, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 228 done after 87 steps, reward Average: -140.35, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 229 done after 144 steps, reward Average: -140.35, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 230 done after 159 steps, reward Average: -140.57, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 231 done after 152 steps, reward Average: -141.2, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 232 done after 89 steps, reward Average: -140.7, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 233 done after 156 steps, reward Average: -140.78, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 234 done after 141 steps, reward Average: -140.78, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 235 done after 143 steps, reward Average: -141.34, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 236 done after 147 steps, reward Average: -141.37, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 237 done after 157 steps, reward Average: -141.56, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 238 done after 157 steps, reward Average: -141.62, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 239 done after 159 steps, reward Average: -141.73, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 240 done after 136 steps, reward Average: -141.68, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 241 done after 147 steps, reward Average: -141.76, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 242 done after 157 steps, reward Average: -141.93, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 243 done after 89 steps, reward Average: -141.41, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 244 done after 165 steps, reward Average: -141.6, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 245 done after 161 steps, reward Average: -141.79, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 246 done after 146 steps, reward Average: -141.76, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 247 done after 175 steps, reward Average: -142.06, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 248 done after 162 steps, reward Average: -142.26, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 249 done after 150 steps, reward Average: -142.88, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 250 done after 200 steps, reward Average: -143.5, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 251 done after 161 steps, reward Average: -143.73, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 252 done after 155 steps, reward Average: -143.9, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 253 done after 160 steps, reward Average: -144.08, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 254 done after 147 steps, reward Average: -144.13, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 255 done after 151 steps, reward Average: -144.19, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 256 done after 144 steps, reward Average: -144.11, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 257 done after 151 steps, reward Average: -144.23, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 258 done after 154 steps, reward Average: -144.39, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 259 done after 143 steps, reward Average: -144.44, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 260 done after 144 steps, reward Average: -144.5, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 261 done after 152 steps, reward Average: -144.57, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 262 done after 142 steps, reward Average: -144.43, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 263 done after 150 steps, reward Average: -144.56, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 264 done after 154 steps, reward Average: -144.61, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 265 done after 138 steps, reward Average: -144.55, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 266 done after 88 steps, reward Average: -144.6, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 267 done after 146 steps, reward Average: -144.06, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 268 done after 138 steps, reward Average: -143.89, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 269 done after 153 steps, reward Average: -143.84, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 270 done after 142 steps, reward Average: -143.83, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 271 done after 140 steps, reward Average: -143.83, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 272 done after 140 steps, reward Average: -143.79, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 273 done after 140 steps, reward Average: -143.72, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 274 done after 152 steps, reward Average: -143.7, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 275 done after 147 steps, reward Average: -143.48, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 276 done after 138 steps, reward Average: -143.19, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 277 done after 139 steps, reward Average: -143.02, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 278 done after 152 steps, reward Average: -143.02, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 279 done after 144 steps, reward Average: -143.62, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 280 done after 147 steps, reward Average: -143.66, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 281 done after 140 steps, reward Average: -143.58, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 282 done after 149 steps, reward Average: -144.17, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 283 done after 153 steps, reward Average: -143.7, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 284 done after 88 steps, reward Average: -143.7, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 285 done after 149 steps, reward Average: -143.76, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 286 done after 151 steps, reward Average: -143.73, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 287 done after 153 steps, reward Average: -143.77, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 288 done after 142 steps, reward Average: -143.69, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 289 done after 158 steps, reward Average: -143.86, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 290 done after 139 steps, reward Average: -143.39, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 291 done after 140 steps, reward Average: -143.11, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 292 done after 200 steps, reward Average: -143.66, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 293 done after 193 steps, reward Average: -144.17, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 294 done after 88 steps, reward Average: -143.62, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 295 done after 150 steps, reward Average: -143.3, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 296 done after 151 steps, reward Average: -143.39, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 297 done after 156 steps, reward Average: -143.44, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 298 done after 200 steps, reward Average: -143.97, up to now: minReward: -83.0, minAverage: -135.34\n",
      "Episode 299 done after 137 steps, reward Average: -143.96, up to now: minReward: -83.0, minAverage: -135.34\n",
      "final result: \n",
      "272 times arrived in 300 episodes, first time in episode 12\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiSgYiIoGQgIiIoGYiICEoGIiIC/H+N8yE5qyoA9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2824fd34d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        interval=nbEpisodes\n",
    "        if interval<decayInterval:\n",
    "            interval=decayInterval\n",
    "        result=decayStart-episode/interval\n",
    "        if result < decayBase:\n",
    "            result = decayBase\n",
    "        return result\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    ### epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "    ###                      decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    ### alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
