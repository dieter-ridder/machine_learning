{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tiling 888, Try 8 tilings, 16*16 - 50 Episodes #\n",
    "\n",
    "Best version was already fast, but not fast enough. Next try is with bigger alpha in the beginning\n",
    "\n",
    "parameters: \n",
    "\n",
    "alpha:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "gamma: \n",
    "        max(1.0-episode/200, 0.2)\n",
    "        \n",
    "epsilon:\n",
    "        max(0.1-episode/200, 0.001)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 22:34:36,381] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=4           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.005\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.2\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 126 steps, reward Average: -195.88888888888889, up to now: minReward: -126.0, minAverage: -195.88888888888889\n",
      "Episode 18 done after 115 steps, reward Average: -191.6315789473684, up to now: minReward: -115.0, minAverage: -191.6315789473684\n",
      "Episode 19 done after 120 steps, reward Average: -188.05, up to now: minReward: -115.0, minAverage: -188.05\n",
      "Episode 20 done after 117 steps, reward Average: -184.66666666666666, up to now: minReward: -115.0, minAverage: -184.66666666666666\n",
      "Episode 21 done after 118 steps, reward Average: -181.63636363636363, up to now: minReward: -115.0, minAverage: -181.63636363636363\n",
      "Episode 22 done after 115 steps, reward Average: -178.7391304347826, up to now: minReward: -115.0, minAverage: -178.7391304347826\n",
      "Episode 23 done after 113 steps, reward Average: -176.0, up to now: minReward: -113.0, minAverage: -176.0\n",
      "Episode 24 done after 117 steps, reward Average: -173.64, up to now: minReward: -113.0, minAverage: -173.64\n",
      "Episode 25 done after 113 steps, reward Average: -171.30769230769232, up to now: minReward: -113.0, minAverage: -171.30769230769232\n",
      "Episode 26 done after 112 steps, reward Average: -169.11111111111111, up to now: minReward: -112.0, minAverage: -169.11111111111111\n",
      "Episode 27 done after 108 steps, reward Average: -166.92857142857142, up to now: minReward: -108.0, minAverage: -166.92857142857142\n",
      "Episode 28 done after 108 steps, reward Average: -164.89655172413794, up to now: minReward: -108.0, minAverage: -164.89655172413794\n",
      "Episode 29 done after 108 steps, reward Average: -163.0, up to now: minReward: -108.0, minAverage: -163.0\n",
      "Episode 30 done after 108 steps, reward Average: -161.2258064516129, up to now: minReward: -108.0, minAverage: -161.2258064516129\n",
      "Episode 31 done after 107 steps, reward Average: -159.53125, up to now: minReward: -107.0, minAverage: -159.53125\n",
      "Episode 32 done after 107 steps, reward Average: -157.93939393939394, up to now: minReward: -107.0, minAverage: -157.93939393939394\n",
      "Episode 33 done after 107 steps, reward Average: -156.44117647058823, up to now: minReward: -107.0, minAverage: -156.44117647058823\n",
      "Episode 34 done after 119 steps, reward Average: -155.37142857142857, up to now: minReward: -107.0, minAverage: -155.37142857142857\n",
      "Episode 35 done after 107 steps, reward Average: -154.02777777777777, up to now: minReward: -107.0, minAverage: -154.02777777777777\n",
      "Episode 36 done after 110 steps, reward Average: -152.83783783783784, up to now: minReward: -107.0, minAverage: -152.83783783783784\n",
      "Episode 37 done after 108 steps, reward Average: -151.6578947368421, up to now: minReward: -107.0, minAverage: -151.6578947368421\n",
      "Episode 38 done after 107 steps, reward Average: -150.51282051282053, up to now: minReward: -107.0, minAverage: -150.51282051282053\n",
      "Episode 39 done after 93 steps, reward Average: -149.075, up to now: minReward: -93.0, minAverage: -149.075\n",
      "Episode 40 done after 108 steps, reward Average: -148.0731707317073, up to now: minReward: -93.0, minAverage: -148.0731707317073\n",
      "Episode 41 done after 107 steps, reward Average: -147.0952380952381, up to now: minReward: -93.0, minAverage: -147.0952380952381\n",
      "Episode 42 done after 113 steps, reward Average: -146.30232558139534, up to now: minReward: -93.0, minAverage: -146.30232558139534\n",
      "Episode 43 done after 118 steps, reward Average: -145.6590909090909, up to now: minReward: -93.0, minAverage: -145.6590909090909\n",
      "Episode 44 done after 108 steps, reward Average: -144.82222222222222, up to now: minReward: -93.0, minAverage: -144.82222222222222\n",
      "Episode 45 done after 108 steps, reward Average: -144.02173913043478, up to now: minReward: -93.0, minAverage: -144.02173913043478\n",
      "Episode 46 done after 118 steps, reward Average: -143.46808510638297, up to now: minReward: -93.0, minAverage: -143.46808510638297\n",
      "Episode 47 done after 107 steps, reward Average: -142.70833333333334, up to now: minReward: -93.0, minAverage: -142.70833333333334\n",
      "Episode 48 done after 112 steps, reward Average: -142.08163265306123, up to now: minReward: -93.0, minAverage: -142.08163265306123\n",
      "Episode 49 done after 107 steps, reward Average: -141.38, up to now: minReward: -93.0, minAverage: -141.38\n",
      "Episode 50 done after 107 steps, reward Average: -140.7058823529412, up to now: minReward: -93.0, minAverage: -140.7058823529412\n",
      "Episode 51 done after 112 steps, reward Average: -140.15384615384616, up to now: minReward: -93.0, minAverage: -140.15384615384616\n",
      "Episode 52 done after 108 steps, reward Average: -139.54716981132074, up to now: minReward: -93.0, minAverage: -139.54716981132074\n",
      "Episode 53 done after 107 steps, reward Average: -138.94444444444446, up to now: minReward: -93.0, minAverage: -138.94444444444446\n",
      "Episode 54 done after 107 steps, reward Average: -138.36363636363637, up to now: minReward: -93.0, minAverage: -138.36363636363637\n",
      "Episode 55 done after 108 steps, reward Average: -137.82142857142858, up to now: minReward: -93.0, minAverage: -137.82142857142858\n",
      "Episode 56 done after 108 steps, reward Average: -137.2982456140351, up to now: minReward: -93.0, minAverage: -137.2982456140351\n",
      "Episode 57 done after 108 steps, reward Average: -136.79310344827587, up to now: minReward: -93.0, minAverage: -136.79310344827587\n",
      "Episode 58 done after 107 steps, reward Average: -136.28813559322035, up to now: minReward: -93.0, minAverage: -136.28813559322035\n",
      "Episode 59 done after 108 steps, reward Average: -135.81666666666666, up to now: minReward: -93.0, minAverage: -135.81666666666666\n",
      "Episode 60 done after 107 steps, reward Average: -135.34426229508196, up to now: minReward: -93.0, minAverage: -135.34426229508196\n",
      "Episode 61 done after 119 steps, reward Average: -135.08064516129033, up to now: minReward: -93.0, minAverage: -135.08064516129033\n",
      "Episode 62 done after 107 steps, reward Average: -134.63492063492063, up to now: minReward: -93.0, minAverage: -134.63492063492063\n",
      "Episode 63 done after 116 steps, reward Average: -134.34375, up to now: minReward: -93.0, minAverage: -134.34375\n",
      "Episode 64 done after 108 steps, reward Average: -133.93846153846152, up to now: minReward: -93.0, minAverage: -133.93846153846152\n",
      "Episode 65 done after 107 steps, reward Average: -133.53030303030303, up to now: minReward: -93.0, minAverage: -133.53030303030303\n",
      "Episode 66 done after 108 steps, reward Average: -133.1492537313433, up to now: minReward: -93.0, minAverage: -133.1492537313433\n",
      "Episode 67 done after 107 steps, reward Average: -132.76470588235293, up to now: minReward: -93.0, minAverage: -132.76470588235293\n",
      "Episode 68 done after 108 steps, reward Average: -132.40579710144928, up to now: minReward: -93.0, minAverage: -132.40579710144928\n",
      "Episode 69 done after 108 steps, reward Average: -132.05714285714285, up to now: minReward: -93.0, minAverage: -132.05714285714285\n",
      "Episode 70 done after 118 steps, reward Average: -131.85915492957747, up to now: minReward: -93.0, minAverage: -131.85915492957747\n",
      "Episode 71 done after 113 steps, reward Average: -131.59722222222223, up to now: minReward: -93.0, minAverage: -131.59722222222223\n",
      "Episode 72 done after 115 steps, reward Average: -131.36986301369862, up to now: minReward: -93.0, minAverage: -131.36986301369862\n",
      "Episode 73 done after 109 steps, reward Average: -131.06756756756758, up to now: minReward: -93.0, minAverage: -131.06756756756758\n",
      "Episode 74 done after 107 steps, reward Average: -130.74666666666667, up to now: minReward: -93.0, minAverage: -130.74666666666667\n",
      "Episode 75 done after 107 steps, reward Average: -130.43421052631578, up to now: minReward: -93.0, minAverage: -130.43421052631578\n",
      "Episode 76 done after 107 steps, reward Average: -130.12987012987014, up to now: minReward: -93.0, minAverage: -130.12987012987014\n",
      "Episode 77 done after 108 steps, reward Average: -129.84615384615384, up to now: minReward: -93.0, minAverage: -129.84615384615384\n",
      "Episode 78 done after 107 steps, reward Average: -129.55696202531647, up to now: minReward: -93.0, minAverage: -129.55696202531647\n",
      "Episode 79 done after 104 steps, reward Average: -129.2375, up to now: minReward: -93.0, minAverage: -129.2375\n",
      "Episode 80 done after 107 steps, reward Average: -128.96296296296296, up to now: minReward: -93.0, minAverage: -128.96296296296296\n",
      "Episode 81 done after 109 steps, reward Average: -128.71951219512195, up to now: minReward: -93.0, minAverage: -128.71951219512195\n",
      "Episode 82 done after 107 steps, reward Average: -128.4578313253012, up to now: minReward: -93.0, minAverage: -128.4578313253012\n",
      "Episode 83 done after 107 steps, reward Average: -128.20238095238096, up to now: minReward: -93.0, minAverage: -128.20238095238096\n",
      "Episode 84 done after 108 steps, reward Average: -127.96470588235294, up to now: minReward: -93.0, minAverage: -127.96470588235294\n",
      "Episode 85 done after 84 steps, reward Average: -127.45348837209302, up to now: minReward: -84.0, minAverage: -127.45348837209302\n",
      "Episode 86 done after 107 steps, reward Average: -127.2183908045977, up to now: minReward: -84.0, minAverage: -127.2183908045977\n",
      "Episode 87 done after 104 steps, reward Average: -126.95454545454545, up to now: minReward: -84.0, minAverage: -126.95454545454545\n",
      "Episode 88 done after 106 steps, reward Average: -126.71910112359551, up to now: minReward: -84.0, minAverage: -126.71910112359551\n",
      "Episode 89 done after 108 steps, reward Average: -126.5111111111111, up to now: minReward: -84.0, minAverage: -126.5111111111111\n",
      "Episode 90 done after 113 steps, reward Average: -126.36263736263736, up to now: minReward: -84.0, minAverage: -126.36263736263736\n",
      "Episode 91 done after 85 steps, reward Average: -125.91304347826087, up to now: minReward: -84.0, minAverage: -125.91304347826087\n",
      "Episode 92 done after 110 steps, reward Average: -125.74193548387096, up to now: minReward: -84.0, minAverage: -125.74193548387096\n",
      "Episode 93 done after 88 steps, reward Average: -125.34042553191489, up to now: minReward: -84.0, minAverage: -125.34042553191489\n",
      "Episode 94 done after 107 steps, reward Average: -125.14736842105263, up to now: minReward: -84.0, minAverage: -125.14736842105263\n",
      "Episode 95 done after 106 steps, reward Average: -124.94791666666667, up to now: minReward: -84.0, minAverage: -124.94791666666667\n",
      "Episode 96 done after 105 steps, reward Average: -124.74226804123711, up to now: minReward: -84.0, minAverage: -124.74226804123711\n",
      "Episode 97 done after 107 steps, reward Average: -124.56122448979592, up to now: minReward: -84.0, minAverage: -124.56122448979592\n",
      "Episode 98 done after 102 steps, reward Average: -124.33333333333333, up to now: minReward: -84.0, minAverage: -124.33333333333333\n",
      "Episode 99 done after 108 steps, reward Average: -124.17, up to now: minReward: -84.0, minAverage: -124.17\n",
      "Episode 100 done after 116 steps, reward Average: -123.33, up to now: minReward: -84.0, minAverage: -123.33\n",
      "Episode 101 done after 108 steps, reward Average: -122.41, up to now: minReward: -84.0, minAverage: -122.41\n",
      "Episode 102 done after 116 steps, reward Average: -121.57, up to now: minReward: -84.0, minAverage: -121.57\n",
      "Episode 103 done after 109 steps, reward Average: -120.66, up to now: minReward: -84.0, minAverage: -120.66\n",
      "Episode 104 done after 107 steps, reward Average: -119.73, up to now: minReward: -84.0, minAverage: -119.73\n",
      "Episode 105 done after 107 steps, reward Average: -118.8, up to now: minReward: -84.0, minAverage: -118.8\n",
      "Episode 106 done after 107 steps, reward Average: -117.87, up to now: minReward: -84.0, minAverage: -117.87\n",
      "Episode 107 done after 106 steps, reward Average: -116.93, up to now: minReward: -84.0, minAverage: -116.93\n",
      "Episode 108 done after 117 steps, reward Average: -116.1, up to now: minReward: -84.0, minAverage: -116.1\n",
      "Episode 109 done after 110 steps, reward Average: -115.2, up to now: minReward: -84.0, minAverage: -115.2\n",
      "Episode 110 done after 116 steps, reward Average: -114.36, up to now: minReward: -84.0, minAverage: -114.36\n",
      "Episode 111 done after 106 steps, reward Average: -113.42, up to now: minReward: -84.0, minAverage: -113.42\n",
      "Episode 112 done after 110 steps, reward Average: -112.52, up to now: minReward: -84.0, minAverage: -112.52\n",
      "Episode 113 done after 108 steps, reward Average: -111.6, up to now: minReward: -84.0, minAverage: -111.6\n",
      "Episode 114 done after 108 steps, reward Average: -110.68, up to now: minReward: -84.0, minAverage: -110.68\n",
      "Episode 115 done after 107 steps, reward Average: -109.75, up to now: minReward: -84.0, minAverage: -109.75\n",
      "Episode 116 done after 105 steps, reward Average: -108.8, up to now: minReward: -84.0, minAverage: -108.8\n",
      "Episode 117 done after 107 steps, reward Average: -108.61, up to now: minReward: -84.0, minAverage: -108.61\n",
      "Episode 118 done after 103 steps, reward Average: -108.49, up to now: minReward: -84.0, minAverage: -108.49\n",
      "Episode 119 done after 113 steps, reward Average: -108.42, up to now: minReward: -84.0, minAverage: -108.42\n",
      "Episode 120 done after 110 steps, reward Average: -108.35, up to now: minReward: -84.0, minAverage: -108.35\n",
      "Episode 121 done after 94 steps, reward Average: -108.11, up to now: minReward: -84.0, minAverage: -108.11\n",
      "Episode 122 done after 109 steps, reward Average: -108.05, up to now: minReward: -84.0, minAverage: -108.05\n",
      "Episode 123 done after 107 steps, reward Average: -107.99, up to now: minReward: -84.0, minAverage: -107.99\n",
      "Episode 124 done after 99 steps, reward Average: -107.81, up to now: minReward: -84.0, minAverage: -107.81\n",
      "Episode 125 done after 104 steps, reward Average: -107.72, up to now: minReward: -84.0, minAverage: -107.72\n",
      "Episode 126 done after 104 steps, reward Average: -107.64, up to now: minReward: -84.0, minAverage: -107.64\n",
      "Episode 127 done after 112 steps, reward Average: -107.68, up to now: minReward: -84.0, minAverage: -107.64\n",
      "Episode 128 done after 116 steps, reward Average: -107.76, up to now: minReward: -84.0, minAverage: -107.64\n",
      "Episode 129 done after 111 steps, reward Average: -107.79, up to now: minReward: -84.0, minAverage: -107.64\n",
      "Episode 130 done after 106 steps, reward Average: -107.77, up to now: minReward: -84.0, minAverage: -107.64\n",
      "Episode 131 done after 91 steps, reward Average: -107.61, up to now: minReward: -84.0, minAverage: -107.61\n",
      "Episode 132 done after 106 steps, reward Average: -107.6, up to now: minReward: -84.0, minAverage: -107.6\n",
      "Episode 133 done after 104 steps, reward Average: -107.57, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 134 done after 124 steps, reward Average: -107.62, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 135 done after 107 steps, reward Average: -107.62, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 136 done after 118 steps, reward Average: -107.7, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 137 done after 107 steps, reward Average: -107.69, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 138 done after 108 steps, reward Average: -107.7, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 139 done after 172 steps, reward Average: -108.49, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 140 done after 193 steps, reward Average: -109.34, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 141 done after 109 steps, reward Average: -109.36, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 142 done after 106 steps, reward Average: -109.29, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 143 done after 108 steps, reward Average: -109.19, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 144 done after 108 steps, reward Average: -109.19, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 145 done after 105 steps, reward Average: -109.16, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 146 done after 106 steps, reward Average: -109.04, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 147 done after 105 steps, reward Average: -109.02, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 148 done after 107 steps, reward Average: -108.97, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 149 done after 103 steps, reward Average: -108.93, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 150 done after 114 steps, reward Average: -109.0, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 151 done after 107 steps, reward Average: -108.95, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 152 done after 105 steps, reward Average: -108.92, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 153 done after 109 steps, reward Average: -108.94, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 154 done after 107 steps, reward Average: -108.94, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 155 done after 106 steps, reward Average: -108.92, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 156 done after 107 steps, reward Average: -108.91, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 157 done after 118 steps, reward Average: -109.01, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 158 done after 105 steps, reward Average: -108.99, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 159 done after 105 steps, reward Average: -108.96, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 160 done after 121 steps, reward Average: -109.1, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 161 done after 105 steps, reward Average: -108.96, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 162 done after 106 steps, reward Average: -108.95, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 163 done after 107 steps, reward Average: -108.86, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 164 done after 110 steps, reward Average: -108.88, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 165 done after 107 steps, reward Average: -108.88, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 166 done after 119 steps, reward Average: -108.99, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 167 done after 102 steps, reward Average: -108.94, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 168 done after 102 steps, reward Average: -108.88, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 169 done after 107 steps, reward Average: -108.87, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 170 done after 106 steps, reward Average: -108.75, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 171 done after 104 steps, reward Average: -108.66, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 172 done after 105 steps, reward Average: -108.56, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 173 done after 118 steps, reward Average: -108.65, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 174 done after 103 steps, reward Average: -108.61, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 175 done after 107 steps, reward Average: -108.61, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 176 done after 104 steps, reward Average: -108.58, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 177 done after 106 steps, reward Average: -108.56, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 178 done after 107 steps, reward Average: -108.56, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 179 done after 105 steps, reward Average: -108.57, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 180 done after 106 steps, reward Average: -108.56, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 181 done after 102 steps, reward Average: -108.49, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 182 done after 105 steps, reward Average: -108.47, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 183 done after 102 steps, reward Average: -108.42, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 184 done after 106 steps, reward Average: -108.4, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 185 done after 106 steps, reward Average: -108.62, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 186 done after 102 steps, reward Average: -108.57, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 187 done after 106 steps, reward Average: -108.59, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 188 done after 107 steps, reward Average: -108.6, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 189 done after 107 steps, reward Average: -108.59, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 190 done after 105 steps, reward Average: -108.51, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 191 done after 108 steps, reward Average: -108.74, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 192 done after 106 steps, reward Average: -108.7, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 193 done after 107 steps, reward Average: -108.89, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 194 done after 107 steps, reward Average: -108.89, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 195 done after 107 steps, reward Average: -108.9, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 196 done after 107 steps, reward Average: -108.92, up to now: minReward: -84.0, minAverage: -107.57\n",
      "Episode 197 done after 83 steps, reward Average: -108.68, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 198 done after 107 steps, reward Average: -108.73, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 199 done after 106 steps, reward Average: -108.71, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 200 done after 108 steps, reward Average: -108.63, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 201 done after 112 steps, reward Average: -108.67, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 202 done after 105 steps, reward Average: -108.56, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 203 done after 106 steps, reward Average: -108.53, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 204 done after 103 steps, reward Average: -108.49, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 205 done after 106 steps, reward Average: -108.48, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 206 done after 107 steps, reward Average: -108.48, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 207 done after 106 steps, reward Average: -108.48, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 208 done after 106 steps, reward Average: -108.37, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 209 done after 107 steps, reward Average: -108.34, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 210 done after 114 steps, reward Average: -108.32, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 211 done after 106 steps, reward Average: -108.32, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 212 done after 108 steps, reward Average: -108.3, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 213 done after 116 steps, reward Average: -108.38, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 214 done after 104 steps, reward Average: -108.34, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 215 done after 107 steps, reward Average: -108.34, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 216 done after 107 steps, reward Average: -108.36, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 217 done after 107 steps, reward Average: -108.36, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 218 done after 107 steps, reward Average: -108.4, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 219 done after 99 steps, reward Average: -108.26, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 220 done after 200 steps, reward Average: -109.16, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 221 done after 190 steps, reward Average: -110.12, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 222 done after 106 steps, reward Average: -110.09, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 223 done after 107 steps, reward Average: -110.09, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 224 done after 96 steps, reward Average: -110.06, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 225 done after 108 steps, reward Average: -110.1, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 226 done after 85 steps, reward Average: -109.91, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 227 done after 89 steps, reward Average: -109.68, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 228 done after 104 steps, reward Average: -109.56, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 229 done after 108 steps, reward Average: -109.53, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 230 done after 107 steps, reward Average: -109.54, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 231 done after 107 steps, reward Average: -109.7, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 232 done after 107 steps, reward Average: -109.71, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 233 done after 200 steps, reward Average: -110.67, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 234 done after 188 steps, reward Average: -111.31, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 235 done after 137 steps, reward Average: -111.61, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 236 done after 88 steps, reward Average: -111.31, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 237 done after 105 steps, reward Average: -111.29, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 238 done after 123 steps, reward Average: -111.44, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 239 done after 93 steps, reward Average: -110.65, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 240 done after 144 steps, reward Average: -110.16, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 241 done after 107 steps, reward Average: -110.14, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 242 done after 107 steps, reward Average: -110.15, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 243 done after 87 steps, reward Average: -109.94, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 244 done after 107 steps, reward Average: -109.93, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 245 done after 106 steps, reward Average: -109.94, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 246 done after 84 steps, reward Average: -109.72, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 247 done after 138 steps, reward Average: -110.05, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 248 done after 107 steps, reward Average: -110.05, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 249 done after 139 steps, reward Average: -110.41, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 250 done after 107 steps, reward Average: -110.34, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 251 done after 107 steps, reward Average: -110.34, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 252 done after 118 steps, reward Average: -110.47, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 253 done after 106 steps, reward Average: -110.44, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 254 done after 107 steps, reward Average: -110.44, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 255 done after 107 steps, reward Average: -110.45, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 256 done after 105 steps, reward Average: -110.43, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 257 done after 97 steps, reward Average: -110.22, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 258 done after 129 steps, reward Average: -110.46, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 259 done after 105 steps, reward Average: -110.46, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 260 done after 112 steps, reward Average: -110.37, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 261 done after 107 steps, reward Average: -110.39, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 262 done after 115 steps, reward Average: -110.48, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 263 done after 116 steps, reward Average: -110.57, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 264 done after 106 steps, reward Average: -110.53, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 265 done after 100 steps, reward Average: -110.46, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 266 done after 104 steps, reward Average: -110.31, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 267 done after 107 steps, reward Average: -110.36, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 268 done after 99 steps, reward Average: -110.33, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 269 done after 98 steps, reward Average: -110.24, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 270 done after 115 steps, reward Average: -110.33, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 271 done after 108 steps, reward Average: -110.37, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 272 done after 119 steps, reward Average: -110.51, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 273 done after 106 steps, reward Average: -110.39, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 274 done after 109 steps, reward Average: -110.45, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 275 done after 109 steps, reward Average: -110.47, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 276 done after 121 steps, reward Average: -110.64, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 277 done after 106 steps, reward Average: -110.64, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 278 done after 106 steps, reward Average: -110.63, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 279 done after 107 steps, reward Average: -110.65, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 280 done after 119 steps, reward Average: -110.78, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 281 done after 107 steps, reward Average: -110.83, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 282 done after 132 steps, reward Average: -111.1, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 283 done after 121 steps, reward Average: -111.29, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 284 done after 106 steps, reward Average: -111.29, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 285 done after 119 steps, reward Average: -111.42, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 286 done after 126 steps, reward Average: -111.66, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 287 done after 107 steps, reward Average: -111.67, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 288 done after 106 steps, reward Average: -111.66, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 289 done after 106 steps, reward Average: -111.65, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 290 done after 112 steps, reward Average: -111.72, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 291 done after 87 steps, reward Average: -111.51, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 292 done after 84 steps, reward Average: -111.29, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 293 done after 109 steps, reward Average: -111.31, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 294 done after 87 steps, reward Average: -111.11, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 295 done after 108 steps, reward Average: -111.12, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 296 done after 84 steps, reward Average: -110.89, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 297 done after 87 steps, reward Average: -110.93, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 298 done after 106 steps, reward Average: -110.92, up to now: minReward: -83.0, minAverage: -107.57\n",
      "Episode 299 done after 88 steps, reward Average: -110.74, up to now: minReward: -83.0, minAverage: -107.57\n",
      "final result: \n",
      "281 times arrived in 300 episodes, first time in episode 17\n",
      "problem solved?:True\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b4dad5d550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        return max(decayStart-episode/decayInterval, decayBase)\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.65\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) +  (R +  Q(S, A)  Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
