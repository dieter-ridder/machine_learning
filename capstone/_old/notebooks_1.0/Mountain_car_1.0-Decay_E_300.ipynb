{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling 8*8*8, Decay Epsilon - 300 Episodes#\n",
    "\n",
    "Base parameters:\n",
    "alpha = 0.5\n",
    "gamma = 1.0\n",
    "epsilon = 1/(epsiode+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-31 11:21:14,458] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"DISPLAY\"] = \":0\"\n",
    "\n",
    "nbEpisodes=1\n",
    "nbTimesteps=50\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "class tileModel:\n",
    "    def __init__(self, nbTilings, gridSize):\n",
    "        # characteristica of observation\n",
    "        self.obsHigh = env.observation_space.high\n",
    "        self.obsLow = env.observation_space.low\n",
    "        self.obsDim = len(self.obsHigh)\n",
    "        \n",
    "        # characteristica of tile model\n",
    "        self.nbTilings = nbTilings\n",
    "        self.gridSize = gridSize\n",
    "        self.gridWidth = np.divide(np.subtract(self.obsHigh,self.obsLow), self.gridSize)\n",
    "        self.nbTiles = (self.gridSize**self.obsDim) * self.nbTilings\n",
    "        self.nbTilesExtra = (self.gridSize**self.obsDim) * (self.nbTilings+1)\n",
    "        \n",
    "        #state space\n",
    "        self.nbActions = env.action_space.n\n",
    "        self.resetStates()\n",
    "        \n",
    "    def resetStates(self):\n",
    "        ### funktioniert nicht, auch nicht mit -10 self.states = np.random.uniform(low=10.5, high=-11.0, size=(self.nbTilesExtra,self.nbActions))\n",
    "        self.states = np.random.uniform(low=0.0, high=0.0001, size=(self.nbTilesExtra,self.nbActions))\n",
    "        ### self.states = np.zeros([self.nbTiles,self.nbActions])\n",
    "        \n",
    "        \n",
    "    def displayM(self):\n",
    "        print('observation:\\thigh:', self.obsHigh, 'low:', self.obsLow, 'dim:',self.obsDim )\n",
    "        print('tile model:\\tnbTilings:', self.nbTilings, 'gridSize:',self.gridSize, 'gridWidth:',self.gridWidth,'nb tiles:', self.nbTiles )\n",
    "        print('state space:\\tnb of actions:', self.nbActions, 'size of state space:', self.nbTiles)\n",
    "        \n",
    "    def code (self, obsOrig):\n",
    "        #shift the original observation to range [0, obsHigh-obsLow]\n",
    "        #scale it to the external grid size: if grid is 8*8, each range is [0,8]\n",
    "        obsScaled = np.divide(np.subtract(obsOrig,self.obsLow),self.gridWidth)\n",
    "        ### print ('\\noriginal obs:',obsOrig,'shifted and scaled obs:', obsScaled )\n",
    "        \n",
    "        #compute the coordinates/tiling\n",
    "        #each tiling is shifted by tiling/gridSize, i.e. tiling*1/8 for grid 8*8\n",
    "        #and casted to integer\n",
    "        coordinates = np.zeros([self.nbTilings,self.obsDim])\n",
    "        tileIndices = np.zeros(self.nbTilings)\n",
    "        for tiling in range(self.nbTilings):\n",
    "            coordinates[tiling,:] = obsScaled + tiling/ self.nbTilings\n",
    "        coordinates=np.floor(coordinates)\n",
    "        \n",
    "        #this coordinates should be used to adress a 1-dimensional status array\n",
    "        #for 8 tilings of 8*8 grid we use:\n",
    "        #for tiling 0: 0-63, for tiling 1: 64-127,...\n",
    "        coordinatesOrg=np.array(coordinates, copy=True)        \n",
    "        ### print ('coordinates:', coordinates)\n",
    "        \n",
    "        for dim in range(1,self.obsDim):\n",
    "            coordinates[:,dim] *= self.gridSize**dim\n",
    "        ### print ('coordinates:', coordinates)\n",
    "        condTrace=False\n",
    "        for tiling in range(self.nbTilings):\n",
    "            tileIndices[tiling] = (tiling * (self.gridSize**self.obsDim) \\\n",
    "                                   + sum(coordinates[tiling,:])) \n",
    "            if tileIndices[tiling] >= self.nbTilesExtra:\n",
    "                condTrace=True\n",
    "                \n",
    "        if condTrace:\n",
    "            print(\"code: obsOrig:\",obsOrig, 'obsScaled:', obsScaled, \"coordinates w/o shift:\\n\", coordinatesOrg )\n",
    "            print (\"coordinates multiplied with base:\\n\", coordinates, \"\\ntileIndices:\", tileIndices)\n",
    "        ### print ('tileIndices:', tileIndices)\n",
    "        ### print ('coordinates-org:', coordinatesOrg)\n",
    "        return coordinatesOrg, tileIndices\n",
    "    \n",
    "    def getQ(self,state):\n",
    "        Q=np.zeros(self.nbActions)\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('getQ-in : state',state,'tileIndices:', tileIndices)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            Q=np.add(Q,self.states[index])\n",
    "        ### print ('getQ-out : Q',Q)\n",
    "        Q=np.divide(Q,self.nbTilings)\n",
    "        return Q\n",
    "    \n",
    "    def updateQ(self, state, action, deltaQA):\n",
    "        _,tileIndices=self.code(state)\n",
    "        ### print ('updateQ-in : state',state,'tileIndices:', tileIndices,'action:', action, 'deltaQA:', deltaQA)\n",
    "\n",
    "        for i in range(len(tileIndices)):\n",
    "            index=int(tileIndices[i])\n",
    "            self.states[index,action]+=deltaQA\n",
    "            ### print ('updateQ: index:', index, 'states[index]:',self.states[index])\n",
    "            \n",
    "    def preparePlot(self):\n",
    "        plotInput=np.zeros([self.gridSize*self.nbTilings, self.gridSize*self.nbTilings,self.nbActions])    #pos*velo*actions\n",
    "        iTiling=0\n",
    "        iDim1=0                 #velocity\n",
    "        iDim0=0                 #position\n",
    "        ### tileShift=1/self.nbTilings\n",
    "        \n",
    "        for i in range(self.nbTiles):\n",
    "            ### print ('i:',i,'iTiling:',iTiling,'iDim0:',iDim0, 'iDim1:', iDim1 ,'state:', self.states[i])\n",
    "            for jDim0 in range(iDim0*self.nbTilings-iTiling, (iDim0+1)*self.nbTilings-iTiling):\n",
    "                for jDim1 in range(iDim1*self.nbTilings-iTiling, (iDim1+1)*self.nbTilings-iTiling):\n",
    "                    ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state before:', plotInput[jDim0,jDim1] )\n",
    "                    if jDim0>0 and jDim1 >0:\n",
    "                        plotInput[jDim0,jDim1]+=self.states[i]\n",
    "                        ### print ('iTiling:',iTiling,'jDim0:', jDim0, 'jDim1:', jDim1, 'state after:', plotInput[jDim0,jDim1] )\n",
    "            iDim0+=1\n",
    "            if iDim0 >= self.gridSize:\n",
    "                iDim1 +=1\n",
    "                iDim0 =0\n",
    "            if iDim1 >= self.gridSize:\n",
    "                iTiling +=1\n",
    "                iDim0=0\n",
    "                iDim1=0\n",
    "        return plotInput\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation:\thigh: [ 0.6   0.07] low: [-1.2  -0.07] dim: 2\n",
      "tile model:\tnbTilings: 8 gridSize: 8 gridWidth: [ 0.225   0.0175] nb tiles: 512\n",
      "state space:\tnb of actions: 3 size of state space: 512\n"
     ]
    }
   ],
   "source": [
    "tileModel  = tileModel(8,8)                     #grid: 8*8, 8 tilings\n",
    "tileModel.displayM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbEpisodes = 300\n",
    "nbTimesteps = 200\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "EpisodesEvaluated=100\n",
    "rewardLimit=-110\n",
    "\n",
    "strategyEpsilon=1           #0: 1/episode**2, 1:1/episode, 2: (1/2)**episode 3: 0.01 4: linear 9:0.1\n",
    "IntervalEpsilon=200\n",
    "BaseEpsilon=0.001\n",
    "StartEpsilon=0.1\n",
    "\n",
    "strategyGamma=4\n",
    "IntervalGamma=200\n",
    "BaseGamma=0.5\n",
    "\n",
    "strategyAlpha=6\n",
    "\n",
    "policySARSA=True\n",
    "printEpisodeResult=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 1 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 2 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 3 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 4 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 5 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 6 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 7 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 8 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 9 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 10 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 11 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 12 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 13 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 14 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 15 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 16 done after 200 steps, reward Average: -200.0, up to now: minReward: -200, minAverage: -200\n",
      "Episode 17 done after 181 steps, reward Average: -198.94444444444446, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 18 done after 200 steps, reward Average: -199.0, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 19 done after 200 steps, reward Average: -199.05, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 20 done after 200 steps, reward Average: -199.0952380952381, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 21 done after 200 steps, reward Average: -199.13636363636363, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 22 done after 200 steps, reward Average: -199.17391304347825, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 23 done after 200 steps, reward Average: -199.20833333333334, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 24 done after 200 steps, reward Average: -199.24, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 25 done after 200 steps, reward Average: -199.26923076923077, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 26 done after 200 steps, reward Average: -199.2962962962963, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 27 done after 200 steps, reward Average: -199.32142857142858, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 28 done after 200 steps, reward Average: -199.3448275862069, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 29 done after 200 steps, reward Average: -199.36666666666667, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 30 done after 200 steps, reward Average: -199.38709677419354, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 31 done after 200 steps, reward Average: -199.40625, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 32 done after 200 steps, reward Average: -199.42424242424244, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 33 done after 200 steps, reward Average: -199.44117647058823, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 34 done after 200 steps, reward Average: -199.45714285714286, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 35 done after 200 steps, reward Average: -199.47222222222223, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 36 done after 200 steps, reward Average: -199.48648648648648, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 37 done after 200 steps, reward Average: -199.5, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 38 done after 200 steps, reward Average: -199.51282051282053, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 39 done after 200 steps, reward Average: -199.525, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 40 done after 200 steps, reward Average: -199.53658536585365, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 41 done after 200 steps, reward Average: -199.54761904761904, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 42 done after 200 steps, reward Average: -199.5581395348837, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 43 done after 200 steps, reward Average: -199.5681818181818, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 44 done after 200 steps, reward Average: -199.57777777777778, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 45 done after 200 steps, reward Average: -199.58695652173913, up to now: minReward: -181.0, minAverage: -198.94444444444446\n",
      "Episode 46 done after 157 steps, reward Average: -198.68085106382978, up to now: minReward: -157.0, minAverage: -198.68085106382978\n",
      "Episode 47 done after 150 steps, reward Average: -197.66666666666666, up to now: minReward: -150.0, minAverage: -197.66666666666666\n",
      "Episode 48 done after 146 steps, reward Average: -196.6122448979592, up to now: minReward: -146.0, minAverage: -196.6122448979592\n",
      "Episode 49 done after 200 steps, reward Average: -196.68, up to now: minReward: -146.0, minAverage: -196.6122448979592\n",
      "Episode 50 done after 140 steps, reward Average: -195.5686274509804, up to now: minReward: -140.0, minAverage: -195.5686274509804\n",
      "Episode 51 done after 200 steps, reward Average: -195.65384615384616, up to now: minReward: -140.0, minAverage: -195.5686274509804\n",
      "Episode 52 done after 156 steps, reward Average: -194.9056603773585, up to now: minReward: -140.0, minAverage: -194.9056603773585\n",
      "Episode 53 done after 159 steps, reward Average: -194.24074074074073, up to now: minReward: -140.0, minAverage: -194.24074074074073\n",
      "Episode 54 done after 199 steps, reward Average: -194.3272727272727, up to now: minReward: -140.0, minAverage: -194.24074074074073\n",
      "Episode 55 done after 149 steps, reward Average: -193.51785714285714, up to now: minReward: -140.0, minAverage: -193.51785714285714\n",
      "Episode 56 done after 142 steps, reward Average: -192.6140350877193, up to now: minReward: -140.0, minAverage: -192.6140350877193\n",
      "Episode 57 done after 155 steps, reward Average: -191.9655172413793, up to now: minReward: -140.0, minAverage: -191.9655172413793\n",
      "Episode 58 done after 190 steps, reward Average: -191.9322033898305, up to now: minReward: -140.0, minAverage: -191.9322033898305\n",
      "Episode 59 done after 200 steps, reward Average: -192.06666666666666, up to now: minReward: -140.0, minAverage: -191.9322033898305\n",
      "Episode 60 done after 145 steps, reward Average: -191.29508196721312, up to now: minReward: -140.0, minAverage: -191.29508196721312\n",
      "Episode 61 done after 146 steps, reward Average: -190.56451612903226, up to now: minReward: -140.0, minAverage: -190.56451612903226\n",
      "Episode 62 done after 200 steps, reward Average: -190.71428571428572, up to now: minReward: -140.0, minAverage: -190.56451612903226\n",
      "Episode 63 done after 200 steps, reward Average: -190.859375, up to now: minReward: -140.0, minAverage: -190.56451612903226\n",
      "Episode 64 done after 200 steps, reward Average: -191.0, up to now: minReward: -140.0, minAverage: -190.56451612903226\n",
      "Episode 65 done after 156 steps, reward Average: -190.46969696969697, up to now: minReward: -140.0, minAverage: -190.46969696969697\n",
      "Episode 66 done after 200 steps, reward Average: -190.61194029850745, up to now: minReward: -140.0, minAverage: -190.46969696969697\n",
      "Episode 67 done after 196 steps, reward Average: -190.69117647058823, up to now: minReward: -140.0, minAverage: -190.46969696969697\n",
      "Episode 68 done after 155 steps, reward Average: -190.17391304347825, up to now: minReward: -140.0, minAverage: -190.17391304347825\n",
      "Episode 69 done after 200 steps, reward Average: -190.31428571428572, up to now: minReward: -140.0, minAverage: -190.17391304347825\n",
      "Episode 70 done after 182 steps, reward Average: -190.19718309859155, up to now: minReward: -140.0, minAverage: -190.17391304347825\n",
      "Episode 71 done after 200 steps, reward Average: -190.33333333333334, up to now: minReward: -140.0, minAverage: -190.17391304347825\n",
      "Episode 72 done after 200 steps, reward Average: -190.46575342465752, up to now: minReward: -140.0, minAverage: -190.17391304347825\n",
      "Episode 73 done after 183 steps, reward Average: -190.36486486486487, up to now: minReward: -140.0, minAverage: -190.17391304347825\n",
      "Episode 74 done after 150 steps, reward Average: -189.82666666666665, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 75 done after 200 steps, reward Average: -189.96052631578948, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 76 done after 200 steps, reward Average: -190.0909090909091, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 77 done after 200 steps, reward Average: -190.21794871794873, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 78 done after 200 steps, reward Average: -190.34177215189874, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 79 done after 200 steps, reward Average: -190.4625, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 80 done after 185 steps, reward Average: -190.39506172839506, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 81 done after 153 steps, reward Average: -189.9390243902439, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 82 done after 200 steps, reward Average: -190.06024096385542, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 83 done after 200 steps, reward Average: -190.17857142857142, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 84 done after 200 steps, reward Average: -190.2941176470588, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 85 done after 200 steps, reward Average: -190.40697674418604, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 86 done after 200 steps, reward Average: -190.51724137931035, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 87 done after 200 steps, reward Average: -190.625, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 88 done after 200 steps, reward Average: -190.73033707865167, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 89 done after 200 steps, reward Average: -190.83333333333334, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 90 done after 200 steps, reward Average: -190.93406593406593, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 91 done after 200 steps, reward Average: -191.0326086956522, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 92 done after 147 steps, reward Average: -190.55913978494624, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 93 done after 200 steps, reward Average: -190.6595744680851, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 94 done after 200 steps, reward Average: -190.7578947368421, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 95 done after 200 steps, reward Average: -190.85416666666666, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 96 done after 144 steps, reward Average: -190.37113402061857, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 97 done after 150 steps, reward Average: -189.9591836734694, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 98 done after 200 steps, reward Average: -190.06060606060606, up to now: minReward: -140.0, minAverage: -189.82666666666665\n",
      "Episode 99 done after 161 steps, reward Average: -189.77, up to now: minReward: -140.0, minAverage: -189.77\n",
      "Episode 100 done after 200 steps, reward Average: -189.77, up to now: minReward: -140.0, minAverage: -189.77\n",
      "Episode 101 done after 200 steps, reward Average: -189.77, up to now: minReward: -140.0, minAverage: -189.77\n",
      "Episode 102 done after 200 steps, reward Average: -189.77, up to now: minReward: -140.0, minAverage: -189.77\n",
      "Episode 103 done after 148 steps, reward Average: -189.25, up to now: minReward: -140.0, minAverage: -189.25\n",
      "Episode 104 done after 200 steps, reward Average: -189.25, up to now: minReward: -140.0, minAverage: -189.25\n",
      "Episode 105 done after 200 steps, reward Average: -189.25, up to now: minReward: -140.0, minAverage: -189.25\n",
      "Episode 106 done after 200 steps, reward Average: -189.25, up to now: minReward: -140.0, minAverage: -189.25\n",
      "Episode 107 done after 170 steps, reward Average: -188.95, up to now: minReward: -140.0, minAverage: -188.95\n",
      "Episode 108 done after 200 steps, reward Average: -188.95, up to now: minReward: -140.0, minAverage: -188.95\n",
      "Episode 109 done after 155 steps, reward Average: -188.5, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 110 done after 200 steps, reward Average: -188.5, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 111 done after 200 steps, reward Average: -188.5, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 112 done after 200 steps, reward Average: -188.5, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 113 done after 200 steps, reward Average: -188.5, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 114 done after 200 steps, reward Average: -188.5, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 115 done after 200 steps, reward Average: -188.5, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 116 done after 200 steps, reward Average: -188.5, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 117 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 118 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 119 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 120 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 121 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 122 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 123 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 124 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 125 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 126 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 127 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 128 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 129 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 130 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 131 done after 200 steps, reward Average: -188.69, up to now: minReward: -140.0, minAverage: -188.5\n",
      "Episode 132 done after 152 steps, reward Average: -188.21, up to now: minReward: -140.0, minAverage: -188.21\n",
      "Episode 133 done after 200 steps, reward Average: -188.21, up to now: minReward: -140.0, minAverage: -188.21\n",
      "Episode 134 done after 157 steps, reward Average: -187.78, up to now: minReward: -140.0, minAverage: -187.78\n",
      "Episode 135 done after 200 steps, reward Average: -187.78, up to now: minReward: -140.0, minAverage: -187.78\n",
      "Episode 136 done after 200 steps, reward Average: -187.78, up to now: minReward: -140.0, minAverage: -187.78\n",
      "Episode 137 done after 200 steps, reward Average: -187.78, up to now: minReward: -140.0, minAverage: -187.78\n",
      "Episode 138 done after 200 steps, reward Average: -187.78, up to now: minReward: -140.0, minAverage: -187.78\n",
      "Episode 139 done after 162 steps, reward Average: -187.4, up to now: minReward: -140.0, minAverage: -187.4\n",
      "Episode 140 done after 200 steps, reward Average: -187.4, up to now: minReward: -140.0, minAverage: -187.4\n",
      "Episode 141 done after 154 steps, reward Average: -186.94, up to now: minReward: -140.0, minAverage: -186.94\n",
      "Episode 142 done after 200 steps, reward Average: -186.94, up to now: minReward: -140.0, minAverage: -186.94\n",
      "Episode 143 done after 200 steps, reward Average: -186.94, up to now: minReward: -140.0, minAverage: -186.94\n",
      "Episode 144 done after 200 steps, reward Average: -186.94, up to now: minReward: -140.0, minAverage: -186.94\n",
      "Episode 145 done after 200 steps, reward Average: -186.94, up to now: minReward: -140.0, minAverage: -186.94\n",
      "Episode 146 done after 156 steps, reward Average: -186.93, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 147 done after 200 steps, reward Average: -187.43, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 148 done after 159 steps, reward Average: -187.56, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 149 done after 200 steps, reward Average: -187.56, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 150 done after 200 steps, reward Average: -188.16, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 151 done after 200 steps, reward Average: -188.16, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 152 done after 200 steps, reward Average: -188.6, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 153 done after 200 steps, reward Average: -189.01, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 154 done after 200 steps, reward Average: -189.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 155 done after 200 steps, reward Average: -189.53, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 156 done after 200 steps, reward Average: -190.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 157 done after 200 steps, reward Average: -190.56, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 158 done after 200 steps, reward Average: -190.66, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 159 done after 200 steps, reward Average: -190.66, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 160 done after 200 steps, reward Average: -191.21, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 161 done after 200 steps, reward Average: -191.75, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 162 done after 200 steps, reward Average: -191.75, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 163 done after 200 steps, reward Average: -191.75, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 164 done after 200 steps, reward Average: -191.75, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 165 done after 200 steps, reward Average: -192.19, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 166 done after 200 steps, reward Average: -192.19, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 167 done after 200 steps, reward Average: -192.23, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 168 done after 200 steps, reward Average: -192.68, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 169 done after 200 steps, reward Average: -192.68, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 170 done after 200 steps, reward Average: -192.86, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 171 done after 200 steps, reward Average: -192.86, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 172 done after 200 steps, reward Average: -192.86, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 173 done after 200 steps, reward Average: -193.03, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 174 done after 158 steps, reward Average: -193.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 175 done after 200 steps, reward Average: -193.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 176 done after 200 steps, reward Average: -193.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 177 done after 200 steps, reward Average: -193.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 178 done after 200 steps, reward Average: -193.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 179 done after 200 steps, reward Average: -193.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 180 done after 200 steps, reward Average: -193.26, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 181 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 182 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 183 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 184 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 185 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 186 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 187 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 188 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 189 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 190 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 191 done after 200 steps, reward Average: -193.73, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 192 done after 200 steps, reward Average: -194.26, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 193 done after 200 steps, reward Average: -194.26, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 194 done after 153 steps, reward Average: -193.79, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 195 done after 200 steps, reward Average: -193.79, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 196 done after 200 steps, reward Average: -194.35, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 197 done after 200 steps, reward Average: -194.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 198 done after 200 steps, reward Average: -194.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 199 done after 200 steps, reward Average: -195.24, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 200 done after 200 steps, reward Average: -195.24, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 201 done after 156 steps, reward Average: -194.8, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 202 done after 200 steps, reward Average: -194.8, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 203 done after 200 steps, reward Average: -195.32, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 204 done after 200 steps, reward Average: -195.32, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 205 done after 200 steps, reward Average: -195.32, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 206 done after 200 steps, reward Average: -195.32, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 207 done after 200 steps, reward Average: -195.62, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 208 done after 159 steps, reward Average: -195.21, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 209 done after 200 steps, reward Average: -195.66, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 210 done after 200 steps, reward Average: -195.66, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 211 done after 200 steps, reward Average: -195.66, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 212 done after 150 steps, reward Average: -195.16, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 213 done after 158 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 214 done after 200 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 215 done after 200 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 216 done after 200 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 217 done after 200 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 218 done after 200 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 219 done after 200 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 220 done after 200 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 221 done after 200 steps, reward Average: -194.74, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 222 done after 158 steps, reward Average: -194.32, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 223 done after 200 steps, reward Average: -194.32, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 224 done after 200 steps, reward Average: -194.32, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 225 done after 153 steps, reward Average: -193.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 226 done after 200 steps, reward Average: -193.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 227 done after 200 steps, reward Average: -193.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 228 done after 200 steps, reward Average: -193.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 229 done after 200 steps, reward Average: -193.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 230 done after 200 steps, reward Average: -193.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 231 done after 200 steps, reward Average: -193.85, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 232 done after 200 steps, reward Average: -194.33, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 233 done after 200 steps, reward Average: -194.33, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 234 done after 200 steps, reward Average: -194.76, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 235 done after 200 steps, reward Average: -194.76, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 236 done after 200 steps, reward Average: -194.76, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 237 done after 200 steps, reward Average: -194.76, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 238 done after 200 steps, reward Average: -194.76, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 239 done after 200 steps, reward Average: -195.14, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 240 done after 200 steps, reward Average: -195.14, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 241 done after 200 steps, reward Average: -195.6, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 242 done after 200 steps, reward Average: -195.6, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 243 done after 200 steps, reward Average: -195.6, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 244 done after 200 steps, reward Average: -195.6, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 245 done after 200 steps, reward Average: -195.6, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 246 done after 200 steps, reward Average: -196.04, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 247 done after 157 steps, reward Average: -195.61, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 248 done after 200 steps, reward Average: -196.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 249 done after 200 steps, reward Average: -196.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 250 done after 200 steps, reward Average: -196.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 251 done after 200 steps, reward Average: -196.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 252 done after 149 steps, reward Average: -195.51, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 253 done after 151 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 254 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 255 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 256 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 257 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 258 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 259 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 260 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 261 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 262 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 263 done after 200 steps, reward Average: -195.02, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 264 done after 152 steps, reward Average: -194.54, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 265 done after 158 steps, reward Average: -194.12, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 266 done after 200 steps, reward Average: -194.12, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 267 done after 200 steps, reward Average: -194.12, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 268 done after 200 steps, reward Average: -194.12, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 269 done after 200 steps, reward Average: -194.12, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 270 done after 157 steps, reward Average: -193.69, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 271 done after 200 steps, reward Average: -193.69, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 272 done after 200 steps, reward Average: -193.69, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 273 done after 200 steps, reward Average: -193.69, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 274 done after 200 steps, reward Average: -194.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 275 done after 200 steps, reward Average: -194.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 276 done after 200 steps, reward Average: -194.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 277 done after 200 steps, reward Average: -194.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 278 done after 200 steps, reward Average: -194.11, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 279 done after 146 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 280 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 281 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 282 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 283 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 284 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 285 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 286 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 287 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 288 done after 200 steps, reward Average: -193.57, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 289 done after 146 steps, reward Average: -193.03, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 290 done after 147 steps, reward Average: -192.5, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 291 done after 200 steps, reward Average: -192.5, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 292 done after 200 steps, reward Average: -192.5, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 293 done after 200 steps, reward Average: -192.5, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 294 done after 200 steps, reward Average: -192.97, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 295 done after 200 steps, reward Average: -192.97, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 296 done after 200 steps, reward Average: -192.97, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 297 done after 200 steps, reward Average: -192.97, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 298 done after 200 steps, reward Average: -192.97, up to now: minReward: -140.0, minAverage: -186.93\n",
      "Episode 299 done after 200 steps, reward Average: -192.97, up to now: minReward: -140.0, minAverage: -186.93\n",
      "final result: \n",
      "52 times arrived in 300 episodes, first time in episode 17\n",
      "problem solved?:False\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMlBKKQX8fwmdHLa4/awkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e7f7fc2240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random as random\n",
    "### from matplotlib import colors as mcolors\n",
    "### colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "\n",
    "def policy(Q,epsilon):    \n",
    "    if epsilon>0.0 and random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else: \n",
    "        return np.argmax(Q) \n",
    "\n",
    "def decayFunction(episode, strategy=0, decayInterval=100, decayBase=0.5, decayStart=1.0):\n",
    "    global nbEpisodes\n",
    "    if strategy==0:\n",
    "        return 1/((episode+1)**2)\n",
    "    elif strategy ==1:\n",
    "        return 1/(episode+1)\n",
    "    elif strategy ==2:\n",
    "        return (0.5)**episode\n",
    "    elif strategy ==3:\n",
    "        return 0.05\n",
    "    elif strategy ==4:\n",
    "        interval=nbEpisodes\n",
    "        if interval<decayInterval:\n",
    "            interval=decayInterval\n",
    "        result=decayStart-episode/interval\n",
    "        if result < decayBase:\n",
    "            result = decayBase\n",
    "        return result\n",
    "    elif strategy ==5:\n",
    "        return 0.5*(0.5**episode)+0.5\n",
    "    elif strategy ==6:\n",
    "        if episode < 20:\n",
    "            return 0.75\n",
    "        else:\n",
    "            return 0.5\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lastDelta=-1000.0\n",
    "colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "tileModel.resetStates()\n",
    "arrivedNb=0\n",
    "arrivedFirst=0\n",
    "\n",
    "rewardTracker = np.zeros(EpisodesEvaluated)\n",
    "rewardAverages=[]\n",
    "problemSolved=False\n",
    "rewardMin=-200\n",
    "averageMin=-200\n",
    "\n",
    "\n",
    "for episode in range(nbEpisodes):                    \n",
    "    state = env.reset()\n",
    "    rewardAccumulated =0\n",
    "    epsilon=decayFunction(episode,strategy=strategyEpsilon,\\\n",
    "                          decayInterval=IntervalEpsilon, decayBase=BaseEpsilon,decayStart=StartEpsilon)\n",
    "    ##gamma=decayFunction(episode,strategy=strategyGamma,decayInterval=IntervalGamma, decayBase=BaseGamma)\n",
    "    ##alpha=decayFunction(episode,strategy=strategyAlpha)\n",
    "    \n",
    "\n",
    "    Q=tileModel.getQ(state)\n",
    "    action = policy(Q,epsilon)\n",
    "    ### print ('Q_all:', Q, 'action:', action, 'Q_action:',Q[action])\n",
    "\n",
    "    deltaQAs=[0.0]\n",
    "   \n",
    "    for t in range(nbTimesteps):\n",
    "        env.render()\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        rewardAccumulated+=reward\n",
    "        ### print('\\n--- step action:',action,'returns: reward:', reward, 'state_next:', state_next)\n",
    "        if done:\n",
    "            rewardTracker[episode%EpisodesEvaluated]=rewardAccumulated\n",
    "            if episode>=EpisodesEvaluated:\n",
    "                rewardAverage=np.mean(rewardTracker)\n",
    "                if rewardAverage>=rewardLimit:\n",
    "                    problemSolved=True\n",
    "            else:\n",
    "                rewardAverage=np.mean(rewardTracker[0:episode+1])\n",
    "            rewardAverages.append(rewardAverage)\n",
    "            \n",
    "            if rewardAccumulated>rewardMin:\n",
    "                rewardMin=rewardAccumulated\n",
    "            if rewardAverage>averageMin:\n",
    "                averageMin = rewardAverage\n",
    "                \n",
    "            print(\"Episode {} done after {} steps, reward Average: {}, up to now: minReward: {}, minAverage: {}\"\\\n",
    "                  .format(episode, t+1, rewardAverage, rewardMin, averageMin))\n",
    "            if t<nbTimesteps-1:\n",
    "                ### print (\"ARRIVED!!!!\")\n",
    "                arrivedNb+=1\n",
    "                if arrivedFirst==0:\n",
    "                    arrivedFirst=episode\n",
    "            break\n",
    "        #update Q(S,A)-Table according to:\n",
    "        #Q(S,A) <- Q(S,A) + α (R + γ Q(S’, A’) – Q(S,A))\n",
    "        \n",
    "        #start with the information from the old state:\n",
    "        #difference between Q(S,a) and actual reward\n",
    "        #problem: reward belongs to state as whole (-1 for mountain car), Q[action] is the sum over several features\n",
    "            \n",
    "        #now we choose the next state/action pair:\n",
    "        Q_next=tileModel.getQ(state_next)\n",
    "        action_next=policy(Q_next,epsilon)\n",
    "        action_QL=policy(Q_next,0.0)   \n",
    "\n",
    "        if policySARSA:\n",
    "            action_next_learning=action_next\n",
    "        else:\n",
    "            action_next_learning=action_QL\n",
    "\n",
    "        \n",
    "        # take into account the value of the next (Q(S',A') and update the tile model\n",
    "        deltaQA=alpha*(reward + gamma * Q_next[action_next_learning] - Q[action])\n",
    "        deltaQAs.append(deltaQA)\n",
    "        \n",
    "        ### print ('Q:', Q, 'action:', action, 'Q[action]:', Q[action])\n",
    "        ### print ('Q_next:', Q_next, 'action_next:', action_next, 'Q_next[action_next]:', Q_next[action_next])\n",
    "        ### print ('deltaQA:',deltaQA)\n",
    "        tileModel.updateQ(state, action, deltaQA)\n",
    "        ### print ('after update: Q:',tileModel.getQ(state))\n",
    "\n",
    "        \n",
    "        \n",
    "        ###if lastDelta * deltaQA < 0:           #vorzeichenwechsel\n",
    "        ###    print ('deltaQA in:alpha:', alpha,'reward:',reward,'gamma:',gamma,'action_next:', action_next,\\\n",
    "        ###           'Q_next[action_next]:',Q_next[action_next],'action:',action,'Q[action]:', Q[action],'deltaQA out:',deltaQA)\n",
    "        ###lastDelta=deltaQA\n",
    "        \n",
    "        # prepare next round:\n",
    "        state=state_next\n",
    "        action=action_next\n",
    "        Q=Q_next\n",
    "               \n",
    "    if printEpisodeResult:\n",
    "        #evaluation of episode: development of deltaQA\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(range(len(deltaQAs)),deltaQAs,'-')\n",
    "        ax1.set_title(\"after episode:  {} - development of deltaQA\".format(episode))\n",
    "        plt.show()\n",
    "\n",
    "        #evaluation of episode: states\n",
    "        plotInput=tileModel.preparePlot()\n",
    "        ### print ('states:', tileModel.states)\n",
    "        ### print ('plotInput:', plotInput)\n",
    "    \n",
    "        fig = plt.figure()\n",
    "        ax2 = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        x=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        y=range(tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=y*tileModel.gridWidth[0]+tileModel.obsLow[0]\n",
    "        xUnscaled=x*tileModel.gridWidth[1]+tileModel.obsLow[1]\n",
    "\n",
    "\n",
    "        colors=['r','b','g']\n",
    "        labels=['back','neutral','forward']\n",
    "        for i in range(tileModel.nbActions):\n",
    "            ax2.plot_wireframe(xUnscaled,yUnscaled,plotInput[x,y,i],color=colors[i],label=labels[i])\n",
    "\n",
    "        ax2.set_xlabel('velocity')\n",
    "        ax2.set_ylabel('position')\n",
    "        ax2.set_zlabel('action')\n",
    "        ax2.set_title(\"after episode:  {}\".format(episode))\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "        #colorplot=np.empty([tileModel.nbTilings*tileModel.gridSize,tileModel.nbTilings*tileModel.gridSize],dtype=str)\n",
    "        minVal=np.zeros(3)\n",
    "        minCoo=np.empty([3,2])\n",
    "        maxVal=np.zeros(3)\n",
    "        maxCoo=np.empty([3,2])\n",
    "        for ix in x:\n",
    "            for iy in y:\n",
    "                if 0.0 <= np.sum(plotInput[ix,iy,:]) and np.sum(plotInput[ix,iy,:])<=0.003:\n",
    "                    colorplot[ix,iy]='g'\n",
    "                elif colorplot[ix,iy]=='g':\n",
    "                    colorplot[ix,iy]='blue'\n",
    "                else:\n",
    "                    colorplot[ix,iy]='cyan'\n",
    "                \n",
    "\n",
    "        xUnscaled=np.linspace(tileModel.obsLow[0], tileModel.obsHigh[0], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "        yUnscaled=np.linspace(tileModel.obsLow[1], tileModel.obsHigh[1], num=tileModel.nbTilings*tileModel.gridSize)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax3 = fig.add_subplot(111)\n",
    "    \n",
    "        for i in x:\n",
    "            ax3.scatter([i]*len(x),y,c=colorplot[i,y],s=75, alpha=0.5)\n",
    "\n",
    "        #ax3.set_xticklabels(xUnscaled)\n",
    "        #ax3.set_yticklabels(yUnscaled)\n",
    "        ax3.set_xlabel('velocity')\n",
    "        ax3.set_ylabel('position')\n",
    "        ax3.set_title(\"after episode:  {} visited\".format(episode))\n",
    "        plt.show()\n",
    "        \n",
    "        if problemSolved:\n",
    "            break\n",
    "\n",
    "print('final result: \\n{} times arrived in {} episodes, first time in episode {}\\nproblem solved?:{}'.format(arrivedNb,nbEpisodes,arrivedFirst,problemSolved))\n",
    "#evaluation of episode: development of deltaQA\n",
    "fig = plt.figure()\n",
    "ax4 = fig.add_subplot(111)\n",
    "ax4.plot(range(len(rewardAverages)),rewardAverages,'-')\n",
    "ax4.set_title(\"development of rewardAverages\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
